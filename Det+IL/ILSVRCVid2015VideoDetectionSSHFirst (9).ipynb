{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jvvfb1riBioe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch \n",
    "from torch import nn, optim \n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.utils.data\n",
    "import time\n",
    "import itertools\n",
    "import glob \n",
    "from PIL import Image\n",
    "import csv \n",
    "import cv2\n",
    "import re\n",
    "import torchvision\n",
    "import random\n",
    "from xml.etree import ElementTree\n",
    "from torchvision.ops.boxes import box_iou\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sarthak/DataSets/ILSVRC2015/EfficientDet.Pytorch-Updated\n",
      "/home/sarthak/DataSets/ILSVRC2015\n"
     ]
    }
   ],
   "source": [
    "#Packages commonly needed to install pycocotools, tqdm, and requests.\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "%cd EfficientDet.Pytorch-Updated/\n",
    "import math\n",
    "from models.efficientnet import EfficientNet\n",
    "from models.bifpn import BIFPN\n",
    "from models.retinahead import RetinaHead\n",
    "from models.module import RegressionModel, ClassificationModel, Anchors, ClipBoxes, BBoxTransform\n",
    "from torchvision.ops import nms\n",
    "from models.losses import FocalLoss\n",
    "from models.efficientdet import EfficientDet\n",
    "from models.losses import FocalLoss\n",
    "from datasets import VOCDetection, CocoDataset, get_augumentation, detection_collate, Resizer, Normalizer, Augmenter, collater\n",
    "from utils import EFFICIENTDET, get_state_dict\n",
    "from eval import evaluate, evaluate_coco\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Ranger-Deep-Learning-Optimizer' already exists and is not an empty directory.\n",
      "/home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer\n",
      "Obtaining file:///home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer\n",
      "Requirement already satisfied: torch in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from ranger==0.1.dev0) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torch->ranger==0.1.dev0) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torch->ranger==0.1.dev0) (1.19.2)\n",
      "Installing collected packages: ranger\n",
      "  Attempting uninstall: ranger\n",
      "    Found existing installation: ranger 0.1.dev0\n",
      "    Uninstalling ranger-0.1.dev0:\n",
      "      Successfully uninstalled ranger-0.1.dev0\n",
      "  Running setup.py develop for ranger\n",
      "Successfully installed ranger\n",
      "/home/sarthak/DataSets/ILSVRC2015\n",
      "fatal: destination path 'sam' already exists and is not an empty directory.\n",
      "/home/sarthak/DataSets/ILSVRC2015/sam\n",
      "Imported SAM Successfully from github .py file\n",
      "/home/sarthak/DataSets/ILSVRC2015\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "%cd Ranger-Deep-Learning-Optimizer\n",
    "!pip install -e .\n",
    "from ranger import Ranger  \n",
    "%cd ..\n",
    "#https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1\n",
    "!git clone https://github.com/davda54/sam.git\n",
    "%cd sam\n",
    "import sam\n",
    "print(\"Imported SAM Successfully from github .py file\")\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPKm4C39fzGD"
   },
   "source": [
    "## To DO \n",
    "\n",
    "### Put saving model code\n",
    "\n",
    "### Remove printing seg len \n",
    "\n",
    "### Some Errors I get\n",
    "\n",
    "Value Error in Det dataset\n",
    "Model sometimes gets error while running on data.\n",
    "\n",
    "### Get better results\n",
    "\n",
    "* Start simple with Adam and basing training procedures, then iteratively add more stuff. Test on small dataset\n",
    "* Reduce dataset to 5000 train images and 500 valid images\n",
    "\n",
    "\n",
    "### Fix the metric creating of Effecient Det Model. \n",
    "\n",
    "* Push learning rate higher (current lr) * 10\n",
    "\n",
    "Way to make Effecient Det work: \n",
    "* Turn the model to train mode\n",
    "* Put some annotations and an image into the model and run it, which will get the output that will enter run metrics batch\n",
    "* Then try running model on that input with bigger batch sizes. \n",
    "* Figure out how to get the data for one image in calculate metrics \n",
    "* Generalize to see how a batch of predictions can be run to the metrics\n",
    "* Make sure mAP and missed images are the only things coming in and out.\n",
    "* Put code into function and run it.\n",
    "\n",
    "\n",
    "\n",
    "### Clean up bad xml files in DET dataset. \n",
    "List of the image file paths and xml file paths that is unflitered. \n",
    "Create an iteration code that I will run once that will go through every xml file path and parses it to find label and then it gets the \"n0006789\" looking label.text. \n",
    "Check if  \"n0006789\"  is in get_class_info. \n",
    "If not then remove the xml file path that I am parsing from the total xml file path and the corresponding image\n",
    "If yes then continue iterating. \n",
    "\n",
    "\n",
    "\n",
    "# Goal:\n",
    "\n",
    "Get acceptable resukts with a rcnn torchvision model\n",
    "Get acceptable results with effecient det\n",
    "\n",
    "### Advice\n",
    "\n",
    "Try to start with simplest (Adam basics) and keep on adding stuff to improve mAP\n",
    " How much data 57 k in Det dataset\n",
    " Full validation has 170,00 but use \n",
    " \n",
    " * (2,000 images in valid loader).\n",
    "* 10 - 15 images per folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "collapsed": true,
    "id": "G9vLpqYsehKW",
    "outputId": "5ae89189-d6ba-4cdd-d080-4a77f7c5a410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: PyYAML in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (5.4.1)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (0.18.1)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (0.4.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: scipy in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (1.19.2)\n",
      "Requirement already satisfied: Shapely in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
      "Requirement already satisfied: opencv-python in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: six in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (3.3.4)\n",
      "Requirement already satisfied: imageio in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (2.9.0)\n",
      "Requirement already satisfied: Pillow in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (8.2.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.5.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.4.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "juEv3bvw0yYp"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uXvczm7BMhQw"
   },
   "outputs": [],
   "source": [
    "def get_class_info(get_keys = False, smaller_mb_net = False):\n",
    "    obj_dict = {\n",
    "    \"n02691156\": \"airplane\", \n",
    "    \"n02419796\": \"antelope\", \n",
    "    \"n02131653\": \"bear\", \n",
    "    \"n02834778\": \"bicycle\", \n",
    "    \"n01503061\": \"bird\", \n",
    "    \"n02924116\": \"bus\", \n",
    "    \"n02958343\": \"car\", \n",
    "    \"n02402425\": \"cattle\", \n",
    "    \"n02084071\": \"dog\", \n",
    "    \"n02121808\": \"domestic_cat\", \n",
    "    \"n02503517\": \"elephant\", \n",
    "    \"n02118333\": \"fox\",\n",
    "    \"n02510455\": \"giant_panda\", \n",
    "    \"n02342885\": \"hamster\", \n",
    "    \"n02374451\": \"horse\", \n",
    "    \"n02129165\": \"lion\", \n",
    "    \"n01674464\": \"lizard\", \n",
    "    \"n02484322\": \"monkey\", \n",
    "    \"n03790512\": \"motorcycle\", \n",
    "    \"n02324045\": \"rabbit\",\n",
    "    \"n02509815\": \"red_panda\", \n",
    "    \"n02411705\": \"sheep\", \n",
    "    \"n01726692\": \"snake\", \n",
    "    \"n02355227\": \"squirrel\", \n",
    "    \"n02129604\": \"tiger\", \n",
    "    \"n04468005\": \"train\", \n",
    "    \"n01662784\": \"turtle\", \n",
    "    \"n04530566\": \"watercraft\", \n",
    "    \"n02062744\": \"whale\",\n",
    "    \"n02391049\": \"zebra\"\n",
    "    }\n",
    "    \n",
    "    if get_keys:\n",
    "        return list(obj_dict.keys())\n",
    "    \n",
    "    return obj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFrameDataset():\n",
    "    \n",
    "    def __init__(self, mode, vid_data_root_dir, vid_annotations_root_dir, transforms, seg_len = None, \n",
    "               make_valid_smaller_percent = None, effdet_data = None, rcnn_big = None, det_text_file_paths = None, \n",
    "               data_size = None):\n",
    "                 \n",
    "        \n",
    "    # If valid smaller is true cut the length of valid list to certain length with if statement. \n",
    "        if (mode == \"train\"):\n",
    "            if (seg_len):\n",
    "                ORIG_SEG_LEN = seg_len\n",
    "                #Subset images from every scene there are 3862 data folders.\n",
    "                print(\"There are 1122397 train images in total\")\n",
    "                globbed_image_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_data_root_dir)))\n",
    "                globbed_annotations_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_annotations_root_dir)))\n",
    "                \n",
    "                self.image_file_paths, self.annotations_file_paths = list(), list()\n",
    "                \n",
    "                for folder in list(zip(globbed_image_file_paths, globbed_annotations_file_paths)):\n",
    "                    scene_images = sorted(glob.glob(\"{}/*.JPEG\".format(folder[0])))\n",
    "                    scene_annotations = sorted(glob.glob(\"{}/*.xml\".format(folder[1])))\n",
    "                    \n",
    "                    image_annot = list(zip(scene_images, scene_annotations))\n",
    "\n",
    "                    if seg_len % 5 != 0:\n",
    "                        raise ValueError(\"Not allowed value for seg_len must be divisible by 5\")\n",
    "                    if seg_len >= len(image_annot):\n",
    "#                         print(\"Reduced seg len to 5 b/c folder size is {}\".format(len(image_annot)))\n",
    "                        seg_len = 5\n",
    "                    \n",
    "                    if len(image_annot) % seg_len != 0:\n",
    "                        image_annot = image_annot[:-(len(image_annot) % seg_len)]\n",
    "                    \n",
    "                    red_img_annot, start_index = list(), 0 \n",
    "                    \n",
    "                    for window in range(int(len(image_annot) / seg_len)):\n",
    "                        end_index = start_index + seg_len\n",
    "                        red_img_annot.append(random.sample(image_annot[start_index : end_index], 1)[0])\n",
    "                        start_index = end_index\n",
    "                    \n",
    "                   \n",
    "                    scene_images, scene_annotations = zip(*red_img_annot) \n",
    "                    \n",
    "                    self.annotations_file_paths.extend(scene_annotations)\n",
    "                    self.image_file_paths.extend(scene_images)\n",
    "                    \n",
    "                    seg_len = ORIG_SEG_LEN\n",
    "                    \n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "            \n",
    "        elif (mode == \"validation\"):\n",
    "            if make_valid_smaller_percent:\n",
    "                #Subset a percent of the valid data\n",
    "                valid_image_list = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir)))\n",
    "                valid_annotations_list = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "\n",
    "                subset = int(len(valid_image_list) * make_valid_smaller_percent)\n",
    "\n",
    "                #Shuffle both lists at once with same order\n",
    "                mapIndexPosition = list(zip(valid_image_list, valid_annotations_list))\n",
    "                random.shuffle(mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = zip(*mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = list(valid_image_list), list(valid_annotations_list)\n",
    "\n",
    "                self.image_file_paths = valid_image_list[:subset]\n",
    "                self.annotations_file_paths = valid_annotations_list[:subset]\n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "        else:\n",
    "            raise ValueError(\"Choose mode between train or validation only\")\n",
    "\n",
    "        self.labels_key = get_class_info(get_keys = True)\n",
    "\n",
    "        self.data_size = data_size\n",
    "        self.transforms = transforms\n",
    "        self.effdet_data = effdet_data\n",
    "        self.rcnn_big = rcnn_big\n",
    "        \n",
    "        if det_text_file_paths:\n",
    "            home_file_path_data = \"/data1/group/mlgroup/train_data/ILSVRC2015/Data/DET/\"\n",
    "            home_file_path_annot = \"/data1/group/mlgroup/train_data/ILSVRC2015/Annotations/DET/\"\n",
    "            \n",
    "            assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "            print(\"\\n\")\n",
    "            print(\"BEFORE DET: Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "            print(\"BEFORE DET: Amount of annotation files in Dataset {} \\n\".format(len(self.annotations_file_paths)))\n",
    "            det_txt = open(det_text_file_paths, \"r\").readlines()\n",
    "            np.random.shuffle(det_txt)\n",
    "            \n",
    "            #I am gonna sample about 10k images or around 20% of Det Dataset\n",
    "            det_txt = det_txt[:int(len(det_txt) * 0.2)]\n",
    "            print(\"Amount of images in Det Set (Approx.) {}\".format(len(det_txt)))\n",
    "            \n",
    "            for line in det_txt:\n",
    "                self.image_file_paths.append((home_file_path_data + line.split(\" \")[0] + \".JPEG\"))\n",
    "                self.annotations_file_paths.append((home_file_path_annot + line.split(\" \")[0] + \".xml\"))\n",
    "        \n",
    "        #Final sort to keep annotations and image file paths in same config\n",
    "        self.image_file_paths, self.annotations_file_paths = sorted(self.image_file_paths), sorted(self.annotations_file_paths)\n",
    "        \n",
    "        assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "        print(\"Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "        print(\"Amount of annotation files in Dataset {}\".format(len(self.annotations_file_paths)))\n",
    "        \n",
    "        \n",
    "        if self.effdet_data:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with Effecient Det Structure ... \\n\")\n",
    "        elif self.rcnn_big:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with bigger rcnn with ROI Structure ... \\n\")\n",
    "        else:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with mobilenet Faster R CNN Structure ... \\n\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path, xml_path = self.image_file_paths[idx], self.annotations_file_paths[idx]\n",
    "        \n",
    "        img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        # img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        xml_doc = ElementTree.parse(xml_path)\n",
    "\n",
    "        bounding_boxes_nodes = xml_doc.findall(\"object/bndbox\")\n",
    "        labels_nodes = xml_doc.findall(\"object/name\")\n",
    "\n",
    "        bbox, labels = [], []\n",
    "\n",
    "        for node in bounding_boxes_nodes:\n",
    "            xmax = node.find(\"xmax\").text\n",
    "            xmin = node.find(\"xmin\").text\n",
    "            ymax = node.find(\"ymax\").text\n",
    "            ymin = node.find(\"ymin\").text\n",
    "            bbox.append([int(xmin), int(ymin), int(xmax), int(ymax)])  \n",
    "        bbox = torch.as_tensor(bbox, dtype = torch.float32)\n",
    "\n",
    "        for node in labels_nodes:\n",
    "            if self.effdet_data or self.rcnn_big:\n",
    "                label = self.labels_key.index(node.text)\n",
    "            else:\n",
    "                try:\n",
    "                    label = self.labels_key.index(node.text) + 1\n",
    "                except:\n",
    "#                   # Unknown labels are put here\n",
    "                    label = 0\n",
    "            labels.append(label)\n",
    "        labels = torch.as_tensor(labels, dtype = torch.int64)\n",
    "\n",
    "        # labels = tf.cast(labels, dtype = tf.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        try:\n",
    "            if self.transforms:  \n",
    "                sample = {\n",
    "                    'image': img,\n",
    "                    'bboxes': bbox,\n",
    "                    'labels': labels\n",
    "                      }\n",
    "\n",
    "                sample = self.transforms(**sample)\n",
    "                img = sample['image']\n",
    "                try:\n",
    "                    bbox = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                except:\n",
    "                    bbox = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                    \n",
    "        except:\n",
    "            print(\"Caught error. Now trying to instill transforms using Pytorch transforms\")\n",
    "            \n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            emergency_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "            \n",
    "            \n",
    "            img = emergency_transforms(img)\n",
    "                \n",
    "                \n",
    "          # img = tf.cast(sample['image'], dtype = tf.float32) / 255.0\n",
    "          # bbox = tf.convert_to_tensor(torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0), dtype = tf.float32)\n",
    "\n",
    "        if self.effdet_data:\n",
    "            return {\"image\": img, \"bboxes\": bbox, \"category_id\": labels}\n",
    "        \n",
    "        target = dict()\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        return img, target  \n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.data_size:\n",
    "            return self.data_size\n",
    "        return len(self.image_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"n677899\" in get_class_info(get_keys = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFrameDataset():\n",
    "    \n",
    "    def __init__(self, mode, vid_data_root_dir, vid_annotations_root_dir, transforms, seg_len = None, \n",
    "               make_valid_smaller_percent = None, effdet_data = None, rcnn_big = None, det_text_file_paths = None, \n",
    "               data_size = None):\n",
    "                 \n",
    "        \n",
    "    # If valid smaller is true cut the length of valid list to certain length with if statement. \n",
    "        if (mode == \"train\"):\n",
    "            if (seg_len):\n",
    "                ORIG_SEG_LEN = seg_len\n",
    "                #Subset images from every scene there are 3862 data folders.\n",
    "                print(\"There are 1122397 train images in total\")\n",
    "                globbed_image_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_data_root_dir)))\n",
    "                globbed_annotations_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_annotations_root_dir)))\n",
    "                \n",
    "                self.image_file_paths, self.annotations_file_paths = list(), list()\n",
    "                \n",
    "                for folder in list(zip(globbed_image_file_paths, globbed_annotations_file_paths)):\n",
    "                    scene_images = sorted(glob.glob(\"{}/*.JPEG\".format(folder[0])))\n",
    "                    scene_annotations = sorted(glob.glob(\"{}/*.xml\".format(folder[1])))\n",
    "                    \n",
    "                    image_annot = list(zip(scene_images, scene_annotations))\n",
    "\n",
    "                    if seg_len % 5 != 0:\n",
    "                        raise ValueError(\"Not allowed value for seg_len must be divisible by 5\")\n",
    "                    if seg_len >= len(image_annot):\n",
    "                        print(\"Reduced seg len to 5 b/c folder size is {}\".format(len(image_annot)))\n",
    "                        seg_len = 5\n",
    "                    \n",
    "                    if len(image_annot) % seg_len != 0:\n",
    "                        image_annot = image_annot[:-(len(image_annot) % seg_len)]\n",
    "                    \n",
    "                    red_img_annot, start_index = list(), 0 \n",
    "                    \n",
    "                    for window in range(int(len(image_annot) / seg_len)):\n",
    "                        end_index = start_index + seg_len\n",
    "                        red_img_annot.append(random.sample(image_annot[start_index : end_index], 1)[0])\n",
    "                        start_index = end_index\n",
    "                    \n",
    "                   \n",
    "                    scene_images, scene_annotations = zip(*red_img_annot) \n",
    "                    \n",
    "                    self.annotations_file_paths.extend(scene_annotations)\n",
    "                    self.image_file_paths.extend(scene_images)\n",
    "                    \n",
    "                    seg_len = ORIG_SEG_LEN\n",
    "                    \n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "            \n",
    "        elif (mode == \"validation\"):\n",
    "            if make_valid_smaller_percent:\n",
    "                #Subset a percent of the valid data\n",
    "                valid_image_list = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir)))\n",
    "                valid_annotations_list = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "\n",
    "                subset = int(len(valid_image_list) * make_valid_smaller_percent)\n",
    "\n",
    "                #Shuffle both lists at once with same order\n",
    "                mapIndexPosition = list(zip(valid_image_list, valid_annotations_list))\n",
    "                random.shuffle(mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = zip(*mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = list(valid_image_list), list(valid_annotations_list)\n",
    "\n",
    "                self.image_file_paths = valid_image_list[:subset]\n",
    "                self.annotations_file_paths = valid_annotations_list[:subset]\n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "        else:\n",
    "            raise ValueError(\"Choose mode between train or validation only\")\n",
    "\n",
    "        self.labels_key = get_class_info(get_keys = True)\n",
    "\n",
    "        self.data_size = data_size\n",
    "        self.transforms = transforms\n",
    "        self.effdet_data = effdet_data\n",
    "        self.rcnn_big = rcnn_big\n",
    "        \n",
    "        if det_text_file_paths:\n",
    "            home_file_path_data = \"/data1/group/mlgroup/train_data/ILSVRC2015/Data/DET/\"\n",
    "            home_file_path_annot = \"/data1/group/mlgroup/train_data/ILSVRC2015/Annotations/DET/\"\n",
    "            \n",
    "            assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "            print(\"\\n\")\n",
    "            print(\"BEFORE DET: Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "            print(\"BEFORE DET: Amount of annotation files in Dataset {} \\n\".format(len(self.annotations_file_paths)))\n",
    "            det_txt = open(det_text_file_paths, \"r\").readlines()\n",
    "            np.random.shuffle(det_txt)\n",
    "            \n",
    "            #I am gonna sample about 10k images or around 20% of Det Dataset\n",
    "            det_txt = det_txt[:int(len(det_txt) * 0.2)]\n",
    "            print(\"Amount of images in Det Set (Approx.) {}\".format(len(det_txt)))\n",
    "            \n",
    "            for line in det_txt:\n",
    "                self.image_file_paths.append((home_file_path_data + line.split(\" \")[0] + \".JPEG\"))\n",
    "                self.annotations_file_paths.append((home_file_path_annot + line.split(\" \")[0] + \".xml\"))\n",
    "        \n",
    "        #Final sort to keep annotations and image file paths in same config\n",
    "        self.image_file_paths, self.annotations_file_paths = sorted(self.image_file_paths), sorted(self.annotations_file_paths)\n",
    "        \n",
    "        assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "        print(\"Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "        print(\"Amount of annotation files in Dataset {}\".format(len(self.annotations_file_paths)))\n",
    "        \n",
    "        \n",
    "        if self.effdet_data:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with Effecient Det Structure ... \\n\")\n",
    "        elif self.rcnn_big:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with bigger rcnn with ROI Structure ... \\n\")\n",
    "        else:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with mobilenet Faster R CNN Structure ... \\n\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path, xml_path = self.image_file_paths[idx], self.annotations_file_paths[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        # img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        marking = False\n",
    "        xml_doc = ElementTree.parse(xml_path)\n",
    "\n",
    "        bounding_boxes_nodes = xml_doc.findall(\"object/bndbox\")\n",
    "        labels_nodes = xml_doc.findall(\"object/name\")\n",
    "\n",
    "        bbox, labels = [], []\n",
    "\n",
    "        for node in bounding_boxes_nodes:\n",
    "            xmax = node.find(\"xmax\").text\n",
    "            xmin = node.find(\"xmin\").text\n",
    "            ymax = node.find(\"ymax\").text\n",
    "            ymin = node.find(\"ymin\").text\n",
    "            bbox.append([int(xmin), int(ymin), int(xmax), int(ymax)])  \n",
    "        bbox = torch.as_tensor(bbox, dtype = torch.float32)\n",
    "\n",
    "        for node in labels_nodes:\n",
    "            if node.text in self.labels_key:\n",
    "                label = self.labels_key.index(node.text)    \n",
    "            else:\n",
    "                label = \"DNE\"\n",
    "                marking = True\n",
    "            labels.append(label)\n",
    "        \n",
    "        if (marking):\n",
    "            for ii in range(len(labels)):\n",
    "                if (labels[ii] == \"DNE\"):\n",
    "                    #pop the labels\n",
    "                    #pop the bounding box at corresponding index.\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if self.effdet_data or self.rcnn_big:\n",
    "                label = self.labels_key.index(node.text)\n",
    "            else:\n",
    "                try:\n",
    "                    label = self.labels_key.index(node.text) + 1\n",
    "                except:\n",
    "#                   # Unknown labels are put here\n",
    "                    label = 0\n",
    "            labels.append(label)\n",
    "        labels = torch.as_tensor(labels, dtype = torch.int64)\n",
    "\n",
    "        # labels = tf.cast(labels, dtype = tf.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        try:\n",
    "            if self.transforms:  \n",
    "                sample = {\n",
    "                    'image': img,\n",
    "                    'bboxes': bbox,\n",
    "                    'labels': labels\n",
    "                      }\n",
    "\n",
    "                sample = self.transforms(**sample)\n",
    "                img = sample['image']\n",
    "                try:\n",
    "                    bbox = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                except:\n",
    "                    bbox = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                    \n",
    "        except:\n",
    "            print(\"Caught error. Now trying to instill transforms using Pytorch transforms\")\n",
    "            \n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            emergency_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "            \n",
    "            \n",
    "            img = emergency_transforms(img)\n",
    "                \n",
    "                \n",
    "          # img = tf.cast(sample['image'], dtype = tf.float32) / 255.0\n",
    "          # bbox = tf.convert_to_tensor(torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0), dtype = tf.float32)\n",
    "\n",
    "        if self.effdet_data:\n",
    "            return {\"image\": img, \"bboxes\": bbox, \"category_id\": labels}\n",
    "        \n",
    "        target = dict()\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        return img, target  \n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.data_size:\n",
    "            return self.data_size\n",
    "        return len(self.image_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change image size and try and except in dataclass before transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(mode):\n",
    "    if (mode == \"train\"):\n",
    "        return A.Compose([\n",
    "                          A.Resize(512, 512), \n",
    "                          A.OneOf([\n",
    "                          A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                         val_shift_limit=0.2, p=0.9),\n",
    "                          A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                               contrast_limit=0.2, p=0.9)],p=0.9),\n",
    "                          A.Cutout(num_holes=8, max_h_size=8, max_w_size=8, p=0.5),\n",
    "                          A.HorizontalFlip(),\n",
    "                          A.VerticalFlip(), \n",
    "                          ToTensorV2()\n",
    "                          ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    elif (mode == \"test\"):\n",
    "        return A.Compose([\n",
    "                          A.Resize(512, 512), \n",
    "                          ToTensorV2()\n",
    "                          ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    elif (mode == \"effdet_train\"):\n",
    "        return A.Compose([\n",
    "                          A.OneOf([\n",
    "                          A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                         val_shift_limit=0.2, p=0.9),\n",
    "                          A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                               contrast_limit=0.2, p=0.9)],p=0.9),\n",
    "                          A.Cutout(num_holes=8, max_h_size=4, max_w_size=4, p=0.5),\n",
    "                          A.HorizontalFlip(),\n",
    "                          A.VerticalFlip(), \n",
    "                          A.Resize(height = 512, width=512), \n",
    "                          ToTensorV2()\n",
    "                          ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    elif (mode == \"effdet_test\"):\n",
    "        return A.Compose([\n",
    "                          A.Resize(height = 512, width = 512), \n",
    "                          ToTensorV2()])\n",
    "    else:\n",
    "        raise ValueError(\"mode is wrong value can either be train or test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IwHLAJZQjrt6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1122397 train images in total\n",
      "Amount of image files in Dataset 54647\n",
      "Amount of annotation files in Dataset 54647\n",
      "\n",
      "\n",
      "Loading with mobilenet Faster R CNN Structure ... \n",
      "\n",
      "Amount of image files in Dataset 2201\n",
      "Amount of annotation files in Dataset 2201\n",
      "\n",
      "\n",
      "Loading with mobilenet Faster R CNN Structure ... \n",
      "\n",
      " \n",
      " ... Seperate from Data Loader \n",
      "\n",
      "Length of train_dataset 500\n",
      "Length of valid_dataset 100\n"
     ]
    }
   ],
   "source": [
    "# 1122397 Files in train set total\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "#The amount of scenes to load in one go. # 2 and 2 are the best values\n",
    "train_batch_size = 2\n",
    "valid_batch_size = 2\n",
    "det_text_file = \"/data1/group/mlgroup/train_data/ILSVRC2015/DET_train_30classes.txt\"\n",
    "\n",
    "\n",
    "train_dataset = VideoFrameDataset(\"train\", os.path.join(\"Data/VID\", \"train\"), os.path.join(\"Annotations/VID\", \"train\"), get_transforms(mode = \"train\"), \n",
    "                                  seg_len = 20, det_text_file_paths = None, data_size = 500)\n",
    "valid_dataset = VideoFrameDataset(\"validation\", os.path.join(\"Data/VID\", \"val\"), os.path.join(\"Annotations/VID\", \"val\"), get_transforms(mode = \"test\"),\n",
    "                                  make_valid_smaller_percent = 0.0125, data_size = 100)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= collate_fn)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = valid_batch_size, shuffle = True, collate_fn = collate_fn)\n",
    "\n",
    "print(\" \\n ... Seperate from Data Loader \\n\")\n",
    "print(\"Length of train_dataset {}\".format(len(train_dataset)))\n",
    "print(\"Length of valid_dataset {}\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3iCwxnxIcRsr"
   },
   "outputs": [],
   "source": [
    "COLORS = [(0, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0), (255, 0, 0)]\n",
    "\n",
    "def draw_boxes(boxes, labels, image, infer = False, put_text = True):\n",
    "    classes = get_class_info()\n",
    "    keys = list(classes.keys())\n",
    "\n",
    "    # read the image with OpenCV\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    if infer:\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    for i, box in enumerate(boxes):\n",
    "        color = COLORS[1]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 5\n",
    "        )\n",
    "        if put_text:\n",
    "          cv2.putText(image, classes[keys[labels[i] - 1]], (int(box[0]), int(box[1]-5)),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 2, color, 3, \n",
    "                      lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBIf0ODU0Aod"
   },
   "source": [
    "### Create a draw function that works with tensorflow. If cant make it work with tensorflow then just comment out code and put it under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "YMd58kVzZd2L",
    "outputId": "3c70eca3-8a99-4b35-8df9-99b0b895efc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Passing non-integers as three-element position specification is deprecated since 3.3 and will be removed two minor releases later.\n",
      "  \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6c2b8dd2ea90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    image = draw_boxes(labels[idx][\"boxes\"], labels[idx][\"labels\"], images[idx], put_text = True)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5ytYP752Zd7S"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(target_box,predictions_box,scores, device):\n",
    "\n",
    "    #Get most confident boxes first and least confident last\n",
    "    predictions_box = predictions_box[scores.argsort().flip(-1)]\n",
    "    iou_mat = box_iou(target_box,predictions_box)\n",
    "    #return a one by one matrix that is form (target_box, prediction_box) or (1, 1)\n",
    "    target_boxes_count, prediction_boxes_count = iou_mat.shape\n",
    "    \n",
    "    mAP_Matrix = torch.zeros_like(iou_mat)\n",
    "    # if not matrix coordinates that relate to nothing.\n",
    "    if not iou_mat[:,0].eq(0.).all():\n",
    "      index_of_biggest_iou = iou_mat[:,0].argsort()[-1]\n",
    "      mAP_Matrix[index_of_biggest_iou,0] = 1\n",
    "\n",
    "    for pr_idx in range(1,prediction_boxes_count):\n",
    "        not_assigned = torch.logical_not(mAP_Matrix[:,:pr_idx].sum(1)).long()\n",
    "        targets = not_assigned * iou_mat[:,pr_idx]\n",
    "\n",
    "        if targets.eq(0).all():\n",
    "            continue\n",
    "\n",
    "        pivot = targets.argsort()[-1]\n",
    "        mAP_Matrix[pivot,pr_idx] = 1\n",
    "\n",
    "    # mAP calculation\n",
    "    tp = mAP_Matrix.sum()\n",
    "    fp = mAP_Matrix.sum(0).eq(0).sum()\n",
    "    fn = mAP_Matrix.sum(1).eq(0).sum()\n",
    "\n",
    "    mAP = tp / (tp+fp)\n",
    "\n",
    "    return mAP\n",
    "\n",
    "def run_metrics_for_batch(output, targets, mAP, missed_images, device):\n",
    "  for pos_in_batch, image_pred in enumerate(output):\n",
    "    assert (len(image_pred[\"boxes\"]) == len(image_pred[\"labels\"]) == len(image_pred[\"scores\"]))\n",
    "    if len(image_pred[\"boxes\"]) != 0:\n",
    "      mAP += calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
    "    else:\n",
    "      missed_images += 1\n",
    "  \n",
    "  return mAP, missed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "IN1fBHzJZd92"
   },
   "outputs": [],
   "source": [
    "def train(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch,\n",
    "          lo_valid_dataset = len(valid_dataset), lo_train_dataset = len(train_dataset), saving_path = False):\n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print_every = len(train_dataset) / train_batch_size // print_times_per_epoch\n",
    "    print(\"Print Every: {}\".format(print_every))\n",
    "\n",
    "    #Check which parameters can calculate gradients. \n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "\n",
    "#     base_optimizer = Ranger\n",
    "#     optimizer = sam.SAM(net.parameters(), base_optimizer, lr = lr, weight_decay = weight_decay)\n",
    "    optimizer = optim.Adam(params, lr = lr, weight_decay = weight_decay)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
    "\n",
    "    #Might be some problems with the Data Parallel code\n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#         net = nn.DistrubutedParallel(net)\n",
    "    net.to(device)\n",
    "    \n",
    "    print(\"Device: {}\".format(device))\n",
    "    print(\"Optimizer: {}\".format(optimizer))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        \n",
    "        train_loss = train_mAP = steps = train_missed_images = 0\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            \n",
    "            \n",
    "            net.train()\n",
    "            steps += 1\n",
    "\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss_dict = net(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            net.eval()\n",
    "            try:\n",
    "                train_mAP, train_missed_images = run_metrics_for_batch(net(images), targets, train_mAP, train_missed_images, device)\n",
    "            except:\n",
    "                print(images[0].size(), targets)\n",
    "                print(\"Caught an exception in an image could not predict metric for it\")\n",
    "            net.train()\n",
    "\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "#             optimizer.first_step(zero_grad = True)\n",
    "\n",
    "#             loss_dict = net(images, targets)\n",
    "\n",
    "#             losses = sum(loss for loss in loss_dict.values())\n",
    "#             losses.backward()\n",
    "#             optimizer.second_step(zero_grad = True)\n",
    "\n",
    "            train_loss +=  losses.item()\n",
    "            scheduler.step()\n",
    "\n",
    "            if (steps % print_every) == 0:  \n",
    "                with torch.no_grad():\n",
    "                \n",
    "                    valid_mAP = valid_loss = valid_missed_images = 0\n",
    "\n",
    "                    for images, targets in valid_loader:\n",
    "                        net.eval()\n",
    "                        if device == torch.device(\"cuda\"):\n",
    "                            images = [image.to(device) for image in images]\n",
    "                            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
    "                        \n",
    "                        try:\n",
    "                            output = net(images)\n",
    "                            valid_mAP, valid_missed_images = run_metrics_for_batch(output, targets, valid_mAP, valid_missed_images, device)\n",
    "                        except:\n",
    "                            print(targets, images[0].size())\n",
    "                            print(\"Caught exception with running metrics for one valid image (skipped)\")\n",
    "\n",
    "                        net.train()\n",
    "                        valid_loss_dict = net(images, targets)\n",
    "                        valid_losses = sum(loss for loss in valid_loss_dict.values())\n",
    "                        valid_loss += valid_losses.item()\n",
    "\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        learning_rate_extract = param_group[\"lr\"]\n",
    "                    print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Valid_loss: {:0.2f} | Valid mAP: {:0.2f}% | Valid Missed Images {} / {}\".format(\n",
    "                        epoch + 1, epochs, steps, learning_rate_extract, train_loss, valid_loss,  \n",
    "                        (valid_mAP / float(lo_valid_dataset)) * 100., valid_missed_images, lo_valid_dataset))\n",
    "\n",
    "                assert (steps % print_every) == 0\n",
    "                train_loss = 0\n",
    "                 \n",
    "        print(\"\\n Epoch {} | Final Train mAP: {:0.2f}% | Final Train Missed Images {} / {} \\n\".format(\n",
    "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., train_missed_images, lo_train_dataset\n",
    "        ))\n",
    "        if saving_path:\n",
    "            #\n",
    "            print(\"Saving Model ...\")\n",
    "            torch.save(net.state_dicts(), saving_path)\n",
    "    \n",
    "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))\n",
    "    \n",
    "        # Example File path: /saved_models/epoch10small.pth \n",
    "        \n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mish helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU to Mish conversion for models \n",
    "#Option to switch any activation function for another.\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Mish activation loaded...\")\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        x = x *( torch.tanh(F.softplus(x)))\n",
    "\n",
    "        return x\n",
    "    \n",
    "def convert_it(model, new, replaced_act):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, replaced_act):\n",
    "            setattr(model, child_name, new)\n",
    "        else:\n",
    "            convert_it(child, new, replaced_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuNTqGHFaAyU"
   },
   "source": [
    "## Faster R CNN with mobile net backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "z-R_qJ3haFf-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mish activation loaded...\n"
     ]
    }
   ],
   "source": [
    "mob_net = torchvision.models.detection.faster_rcnn.fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "# mob_net = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
    "mob_net.roi_heads.box_predictor.cls_score.out_features = len(get_class_info())\n",
    "mob_net.roi_heads.box_predictor.bbox_pred.out_features = len(get_class_info()) * 4\n",
    "convert_it(mob_net, Mish(), nn.ReLU6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0f08bc118bac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "a = a.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing LR to 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Every: 250.0\n",
      "Device: cuda\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Epoch 1/10 | Batch Number: 250 | LR: 0.00098 | Train_loss: 116.71 | Valid_loss: 39.59 | Valid mAP: 9.50% | Valid Missed Images 0 / 100\n",
      "\n",
      " Epoch 1 | Final Train mAP: 5.57% | Final Train Missed Images 57 / 500 \n",
      "\n",
      "Epoch 2/10 | Batch Number: 250 | LR: 0.00090 | Train_loss: 60.85 | Valid_loss: 52.22 | Valid mAP: 13.74% | Valid Missed Images 0 / 100\n",
      "\n",
      " Epoch 2 | Final Train mAP: 5.96% | Final Train Missed Images 0 / 500 \n",
      "\n",
      "Epoch 3/10 | Batch Number: 250 | LR: 0.00079 | Train_loss: 59.66 | Valid_loss: 49.57 | Valid mAP: 14.23% | Valid Missed Images 0 / 100\n",
      "\n",
      " Epoch 3 | Final Train mAP: 7.06% | Final Train Missed Images 0 / 500 \n",
      "\n",
      "Epoch 4/10 | Batch Number: 250 | LR: 0.00065 | Train_loss: 60.16 | Valid_loss: 40.35 | Valid mAP: 12.28% | Valid Missed Images 0 / 100\n",
      "\n",
      " Epoch 4 | Final Train mAP: 6.98% | Final Train Missed Images 0 / 500 \n",
      "\n",
      "Epoch 5/10 | Batch Number: 250 | LR: 0.00050 | Train_loss: 58.94 | Valid_loss: 68.38 | Valid mAP: 17.17% | Valid Missed Images 0 / 100\n",
      "\n",
      " Epoch 5 | Final Train mAP: 7.24% | Final Train Missed Images 0 / 500 \n",
      "\n",
      "Epoch 6/10 | Batch Number: 250 | LR: 0.00035 | Train_loss: 58.66 | Valid_loss: 57.95 | Valid mAP: 18.24% | Valid Missed Images 0 / 100\n",
      "\n",
      " Epoch 6 | Final Train mAP: 7.01% | Final Train Missed Images 0 / 500 \n",
      "\n",
      "Epoch 7/10 | Batch Number: 250 | LR: 0.00021 | Train_loss: 58.73 | Valid_loss: 44.56 | Valid mAP: 11.28% | Valid Missed Images 0 / 100\n",
      "\n",
      " Epoch 7 | Final Train mAP: 7.72% | Final Train Missed Images 0 / 500 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-12877eb95a51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmob_net_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmob_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-0ccc82e99e6e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch, lo_valid_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m#             optimizer.second_step(zero_grad = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mob_net_trained = train(mob_net, 10, train_loader, valid_loader, 0.001, weight_decay = 1e-5, print_times_per_epoch = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset) / 2// 20 #(10 epochs when final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Every: 110.0\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Device: cuda\n",
      "Optimizer: SAM (\n",
      "Parameter Group 0\n",
      "    N_sma_threshhold: 5\n",
      "    alpha: 0.5\n",
      "    betas: (0.95, 0.999)\n",
      "    eps: 1e-05\n",
      "    initial_lr: 0.0001\n",
      "    k: 6\n",
      "    lr: 0.0001\n",
      "    rho: 0.05\n",
      "    step_counter: 0\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer/ranger/ranger.py:138: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370128159/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "/home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch Number: 110 | LR: 0.00010 | Train_loss: 85.81 | Valid_loss: 433.96 | Valid mAP: 13.20% | Valid Missed Images 27 / 1761\n",
      "Epoch 1/1 | Batch Number: 220 | LR: 0.00010 | Train_loss: 54.37 | Valid_loss: 371.03 | Valid mAP: 20.87% | Valid Missed Images 101 / 1761\n",
      "Epoch 1/1 | Batch Number: 330 | LR: 0.00010 | Train_loss: 50.03 | Valid_loss: 341.60 | Valid mAP: 22.12% | Valid Missed Images 160 / 1761\n",
      "Epoch 1/1 | Batch Number: 440 | LR: 0.00010 | Train_loss: 52.77 | Valid_loss: 343.54 | Valid mAP: 19.24% | Valid Missed Images 162 / 1761\n",
      "Epoch 1/1 | Batch Number: 550 | LR: 0.00010 | Train_loss: 45.13 | Valid_loss: 335.35 | Valid mAP: 21.67% | Valid Missed Images 132 / 1761\n",
      "Epoch 1/1 | Batch Number: 660 | LR: 0.00010 | Train_loss: 46.84 | Valid_loss: 332.24 | Valid mAP: 22.64% | Valid Missed Images 154 / 1761\n",
      "Epoch 1/1 | Batch Number: 770 | LR: 0.00010 | Train_loss: 48.45 | Valid_loss: 330.31 | Valid mAP: 22.28% | Valid Missed Images 122 / 1761\n",
      "Epoch 1/1 | Batch Number: 880 | LR: 0.00010 | Train_loss: 44.98 | Valid_loss: 332.81 | Valid mAP: 23.56% | Valid Missed Images 184 / 1761\n",
      "Epoch 1/1 | Batch Number: 990 | LR: 0.00010 | Train_loss: 53.62 | Valid_loss: 337.11 | Valid mAP: 13.43% | Valid Missed Images 49 / 1761\n",
      "Epoch 1/1 | Batch Number: 1100 | LR: 0.00010 | Train_loss: 45.89 | Valid_loss: 324.72 | Valid mAP: 18.14% | Valid Missed Images 81 / 1761\n",
      "Epoch 1/1 | Batch Number: 1210 | LR: 0.00010 | Train_loss: 43.40 | Valid_loss: 322.55 | Valid mAP: 14.52% | Valid Missed Images 60 / 1761\n",
      "Epoch 1/1 | Batch Number: 1320 | LR: 0.00010 | Train_loss: 48.04 | Valid_loss: 324.35 | Valid mAP: 19.32% | Valid Missed Images 103 / 1761\n",
      "Epoch 1/1 | Batch Number: 1430 | LR: 0.00010 | Train_loss: 45.03 | Valid_loss: 314.75 | Valid mAP: 14.18% | Valid Missed Images 54 / 1761\n",
      "Epoch 1/1 | Batch Number: 1540 | LR: 0.00010 | Train_loss: 48.96 | Valid_loss: 309.00 | Valid mAP: 15.39% | Valid Missed Images 57 / 1761\n",
      "Epoch 1/1 | Batch Number: 1650 | LR: 0.00010 | Train_loss: 45.77 | Valid_loss: 314.81 | Valid mAP: 14.68% | Valid Missed Images 59 / 1761\n",
      "Epoch 1/1 | Batch Number: 1760 | LR: 0.00010 | Train_loss: 43.62 | Valid_loss: 308.76 | Valid mAP: 15.52% | Valid Missed Images 70 / 1761\n",
      "Epoch 1/1 | Batch Number: 1870 | LR: 0.00010 | Train_loss: 48.13 | Valid_loss: 301.21 | Valid mAP: 12.94% | Valid Missed Images 56 / 1761\n",
      "Epoch 1/1 | Batch Number: 1980 | LR: 0.00010 | Train_loss: 46.32 | Valid_loss: 303.85 | Valid mAP: 17.49% | Valid Missed Images 98 / 1761\n",
      "Epoch 1/1 | Batch Number: 2090 | LR: 0.00010 | Train_loss: 43.78 | Valid_loss: 301.89 | Valid mAP: 15.95% | Valid Missed Images 84 / 1761\n",
      "Epoch 1/1 | Batch Number: 2200 | LR: 0.00010 | Train_loss: 42.29 | Valid_loss: 294.12 | Valid mAP: 13.51% | Valid Missed Images 76 / 1761\n",
      "Epoch 1/1 | Batch Number: 2310 | LR: 0.00010 | Train_loss: 44.86 | Valid_loss: 298.63 | Valid mAP: 12.43% | Valid Missed Images 31 / 1761\n",
      "Epoch 1/1 | Batch Number: 2420 | LR: 0.00010 | Train_loss: 41.92 | Valid_loss: 285.92 | Valid mAP: 11.96% | Valid Missed Images 32 / 1761\n",
      "Epoch 1/1 | Batch Number: 2530 | LR: 0.00010 | Train_loss: 45.75 | Valid_loss: 292.88 | Valid mAP: 11.62% | Valid Missed Images 48 / 1761\n",
      "Epoch 1/1 | Batch Number: 2640 | LR: 0.00010 | Train_loss: 44.44 | Valid_loss: 294.20 | Valid mAP: 12.25% | Valid Missed Images 28 / 1761\n",
      "Epoch 1/1 | Batch Number: 2750 | LR: 0.00010 | Train_loss: 44.20 | Valid_loss: 290.85 | Valid mAP: 12.62% | Valid Missed Images 24 / 1761\n",
      "Epoch 1/1 | Batch Number: 2860 | LR: 0.00010 | Train_loss: 40.84 | Valid_loss: 297.80 | Valid mAP: 11.39% | Valid Missed Images 43 / 1761\n",
      "Epoch 1/1 | Batch Number: 2970 | LR: 0.00010 | Train_loss: 42.18 | Valid_loss: 284.70 | Valid mAP: 11.28% | Valid Missed Images 37 / 1761\n",
      "Epoch 1/1 | Batch Number: 3080 | LR: 0.00010 | Train_loss: 44.47 | Valid_loss: 292.18 | Valid mAP: 11.70% | Valid Missed Images 33 / 1761\n",
      "Caught exception with running metrics for one valid image (skipped)\n",
      "Caught exception with running metrics for one valid image (skipped)\n",
      "Caught exception with running metrics for one valid image (skipped)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-478f5cfa93e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmob_net_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmob_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-100-d8d2705a4d2b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch, lo_valid_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                         \u001b[0mvalid_loss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                         \u001b[0mvalid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_loss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 147\u001b[0;31m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_grid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 147\u001b[0;31m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcached_grid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mob_net_trained = train(mob_net, 1, train_loader, valid_loader, 0.0001, weight_decay = 1e-5, print_times_per_epoch = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: Effecient Det Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1122397 train images in total\n",
      "Amount of image files in Dataset 54647\n",
      "Amount of annotation files in Dataset 54647\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n",
      "Amount of image files in Dataset 2201\n",
      "Amount of annotation files in Dataset 2201\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n",
      " \n",
      " ... Seperate from Data Loader \n",
      "\n",
      "Length of train_dataset 500\n",
      "Length of valid_dataset 100\n"
     ]
    }
   ],
   "source": [
    "def detection_collate(batch):\n",
    "    imgs = [s['image'] for s in batch]\n",
    "    annots = [s['bboxes'] for s in batch]\n",
    "    labels = [s['category_id'] for s in batch]\n",
    "\n",
    "    max_num_annots = max(len(annot) for annot in annots)\n",
    "    annot_padded = np.ones((len(annots), max_num_annots, 5))*-1\n",
    "\n",
    "    if max_num_annots > 0:\n",
    "        for idx, (annot, lab) in enumerate(zip(annots, labels)):\n",
    "            if len(annot) > 0:\n",
    "                annot_padded[idx, :len(annot), :4] = annot\n",
    "                annot_padded[idx, :len(annot), 4] = lab\n",
    "    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded))\n",
    "\n",
    "eff_train_batch = 1\n",
    "eff_test_batch = 1\n",
    "\n",
    "eff_train_dataset = VideoFrameDataset(\"train\", os.path.join(\"Data/VID\", \"train\"), os.path.join(\"Annotations/VID\", \"train\"), get_transforms(mode = \"effdet_train\"), \n",
    "                                  seg_len = 20, det_text_file_paths = None, effdet_data = True, data_size = 500)\n",
    "eff_valid_dataset = VideoFrameDataset(\"validation\", os.path.join(\"Data/VID\", \"val\"), os.path.join(\"Annotations/VID\", \"val\"), get_transforms(mode = \"effdet_test\"),\n",
    "                                  make_valid_smaller_percent = 0.0125, effdet_data = True, data_size = 100)\n",
    "\n",
    "eff_train_loader = torch.utils.data.DataLoader(eff_train_dataset, batch_size = eff_train_batch, shuffle = True, collate_fn= detection_collate)\n",
    "eff_valid_loader = torch.utils.data.DataLoader(eff_valid_dataset, batch_size = eff_test_batch, shuffle = True, collate_fn = detection_collate)\n",
    "\n",
    "print(\" \\n ... Seperate from Data Loader \\n\")\n",
    "print(\"Length of train_dataset {}\".format(len(eff_train_dataset)))\n",
    "print(\"Length of valid_dataset {}\".format(len(eff_valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6224fb74576a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Run with DataParallel ....'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "MODEL_MAP = {\n",
    "    'efficientdet-d0': 'efficientnet-b0',\n",
    "    'efficientdet-d1': 'efficientnet-b1',\n",
    "    'efficientdet-d2': 'efficientnet-b2',\n",
    "    'efficientdet-d3': 'efficientnet-b3',\n",
    "    'efficientdet-d4': 'efficientnet-b4',\n",
    "    'efficientdet-d5': 'efficientnet-b5',\n",
    "    'efficientdet-d6': 'efficientnet-b6',\n",
    "    'efficientdet-d7': 'efficientnet-b6',\n",
    "}\n",
    "class EfficientDet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 network='efficientdet-d0',\n",
    "                 D_bifpn=3,\n",
    "                 W_bifpn=88,\n",
    "                 D_class=3,\n",
    "                 is_training=True,\n",
    "                 threshold=0.001, #can change this value 0.01\n",
    "                 iou_threshold=1): # can change this value 0.5\n",
    "        super(EfficientDet, self).__init__()\n",
    "        \n",
    "        self.backbone = EfficientNet.from_pretrained(MODEL_MAP[network])\n",
    "        self.is_training = is_training\n",
    "        self.neck = BIFPN(in_channels=self.backbone.get_list_features()[-5:],\n",
    "                          out_channels=W_bifpn,\n",
    "                          stack=D_bifpn,\n",
    "                          num_outs=5)\n",
    "        self.bbox_head = RetinaHead(num_classes=num_classes,\n",
    "                                    in_channels=W_bifpn)\n",
    "\n",
    "        self.anchors = Anchors()\n",
    "        self.regressBoxes = BBoxTransform()\n",
    "        self.clipBoxes = ClipBoxes()\n",
    "        self.threshold = threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        self.freeze_bn()\n",
    "        self.criterion = FocalLoss()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.is_training:\n",
    "            inputs, annotations = inputs\n",
    "        else:\n",
    "            inputs = inputs\n",
    "        x = self.extract_feat(inputs)\n",
    "        outs = self.bbox_head(x)\n",
    "        classification = torch.cat([out for out in outs[0]], dim=1)\n",
    "        regression = torch.cat([out for out in outs[1]], dim=1)\n",
    "        anchors = self.anchors(inputs)\n",
    "        if self.is_training:\n",
    "            return self.criterion(classification, regression, anchors, annotations)\n",
    "        else:\n",
    "            transformed_anchors = self.regressBoxes(anchors, regression)\n",
    "            transformed_anchors = self.clipBoxes(transformed_anchors, inputs)\n",
    "            scores = torch.max(classification, dim=2, keepdim=True)[0]\n",
    "            scores_over_thresh = (scores > self.threshold)[0, :, 0]\n",
    "\n",
    "            if scores_over_thresh.sum() == 0:\n",
    "                # print('No boxes to NMS')\n",
    "                # no boxes to NMS, just return\n",
    "                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n",
    "            classification = classification[:, scores_over_thresh, :]\n",
    "            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
    "            scores = scores[:, scores_over_thresh, :]\n",
    "            anchors_nms_idx = nms(\n",
    "                transformed_anchors[0, :, :], scores[0, :, 0], iou_threshold=self.iou_threshold)\n",
    "            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(\n",
    "                dim=1)\n",
    "            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        '''Freeze BatchNorm layers.'''\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.eval()\n",
    "\n",
    "    def extract_feat(self, img):\n",
    "        \"\"\"\n",
    "            Directly extract features from the backbone+neck\n",
    "        \"\"\"\n",
    "        x = self.backbone(img)\n",
    "        x = self.neck(x[-5:])\n",
    "        return x\n",
    "\n",
    "model= EfficientDet(num_classes=len(get_class_info()),is_training=True)\n",
    "model.train()\n",
    "\n",
    "model.freeze_bn()\n",
    "\n",
    "model = model.cuda()\n",
    "print('Run with DataParallel ....')\n",
    "\n",
    "## Make sure that you add this line, even though you are not using more than one \n",
    "# GPU DataParallel adds \"module\" to the start of the model structure \n",
    "# allowing for the syntax to be correct when calling \"model.module.freeze_bn()\" for example\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "# I am doing this here an example, you do not have to call the lines below here\n",
    "model.module.is_training = True\n",
    "model.module.freeze_bn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mish activation loaded...\n"
     ]
    }
   ],
   "source": [
    "convert_it(model, Mish(), nn.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1122397 train images in total\n",
      "Amount of image files in Dataset 54647\n",
      "Amount of annotation files in Dataset 54647\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n",
      "Amount of image files in Dataset 88\n",
      "Amount of annotation files in Dataset 88\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
    "    assert (len(scores) == len(classification) == len(transformed_anchors))\n",
    "    if len(transformed_anchors) != 0:\n",
    "      print(targets[0][:, :4], \"\\n\", transformed_anchors, scores)\n",
    "      curr_mAP, curr_mAR = calculate_metrics(targets[0][:, :4], transformed_anchors, scores, device)\n",
    "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
    "    else:\n",
    "      missed_images += 1 \n",
    "    \n",
    "# def run_metrics_for_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
    "#   for pos_in_batch, image_pred in enumerate(output):\n",
    "#     assert (len(scores) == len(classification) == len(transformed_anchors))\n",
    "#     if len(image_pred[\"boxes\"]) != 0:\n",
    "#       mAP += calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
    "#     else:\n",
    "#       missed_images += 1\n",
    "  \n",
    "#   return mAP, missed_images\n",
    "      \n",
    "    return mAP, mAR, missed_images\n",
    "def detection_collate(batch):\n",
    "    imgs = [s['image'] for s in batch]\n",
    "    annots = [s['bboxes'] for s in batch]\n",
    "    labels = [s['category_id'] for s in batch]\n",
    "\n",
    "    max_num_annots = max(len(annot) for annot in annots)\n",
    "    annot_padded = np.ones((len(annots), max_num_annots, 5))*-1\n",
    "\n",
    "    if max_num_annots > 0:\n",
    "        for idx, (annot, lab) in enumerate(zip(annots, labels)):\n",
    "            if len(annot) > 0:\n",
    "                annot_padded[idx, :len(annot), :4] = annot\n",
    "                annot_padded[idx, :len(annot), 4] = lab\n",
    "    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded))\n",
    "\n",
    "train_batch_size = 8\n",
    "valid_batch_size = 1\n",
    "\n",
    "eff_train_dataset = VideoFrameDataset(\"train\", os.path.join(\"Data/VID\", \"train\"), os.path.join(\"Annotations/VID\", \"train\"), get_transforms(mode = \"effdet_train\"), \n",
    "                                  seg_len = 20, effdet_data = True)\n",
    "eff_train_loader = torch.utils.data.DataLoader(eff_train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= detection_collate)\n",
    "\n",
    "\n",
    "eff_valid_dataset = VideoFrameDataset(\"validation\", os.path.join(\"Data/VID\", \"val\"), os.path.join(\"Annotations/VID\", \"val\"), get_transforms(mode = \"effdet_test\"),\n",
    "                                  make_valid_smaller_percent = 0.01, effdet_data = True)\n",
    "eff_valid_loader = torch.utils.data.DataLoader(eff_valid_dataset, batch_size = valid_batch_size, shuffle = True, collate_fn = detection_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_effdet(net, epochs, train_loader, test_loader, lr, weight_decay, \n",
    "          print_times_per_epoch, lo_test_dataset = len(eff_valid_dataset), lo_train_dataset = len(eff_train_dataset)):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device: {}\".format(device))\n",
    "    print(\"Note: Train Accuracies are only run through one train image per batch\")\n",
    "    \n",
    "    print_every = len(train_dataset) / train_batch_size // print_times_per_epoch\n",
    "\n",
    "    if device == torch.device(\"cpu\"):\n",
    "      warnings.warn(\"Code does not support running on CPU but only GPU\")\n",
    "\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
    "\n",
    "    start_time = time.time()\n",
    "    net.module.freeze_bn()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        net.module.is_training = True\n",
    "        \n",
    "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "\n",
    "            net.train()\n",
    "            net.module.is_training = True\n",
    "\n",
    "            steps += 1\n",
    "            \n",
    "            images = images.cuda().float()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            classification_loss, regression_loss = model([images, targets])\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "            loss = classification_loss + regression_loss\n",
    "            if bool(loss == 0):\n",
    "              print('loss equal zero(0)')\n",
    "              continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            net.eval()\n",
    "            net.module.is_training = False\n",
    "            scores, classification, transformed_anchors = net(images[0].unsqueeze(0))\n",
    "#             train_mAP, train_mAR, missed_train_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, train_mAP, \n",
    "#                                                                                      train_mAR, missed_train_images, device)\n",
    "            net.train()\n",
    "            net.module.is_training = True\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if (steps % print_every) == 0:\n",
    "\n",
    "              with torch.no_grad():\n",
    "                test_mAP = test_mAR = missed_test_images = test_loss = 0\n",
    "\n",
    "                for images, targets in test_loader:\n",
    "\n",
    "                  if images.size(0) != 1:\n",
    "                    warning.warn(\"Only can validate fully with batch size of 1, \\\n",
    "                    bigger batch sizes risk Errors or Incomplete Validation\")\n",
    "                  \n",
    "                  net.eval()\n",
    "                  net.module.is_training = False\n",
    "\n",
    "                  if device == torch.device(\"cuda\"):\n",
    "                    images = images.cuda().float()\n",
    "                    targets = targets.cuda()\n",
    "\n",
    "                  scores, classification, transformed_anchors = net(images)\n",
    "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, \n",
    "                                                                                 test_mAP, test_mAR, missed_test_images, device)\n",
    "\n",
    "                  net.train()\n",
    "                  net.module.is_training = True\n",
    "\n",
    "                  classification_loss, regression_loss = model([images, targets])\n",
    "                  classification_loss = classification_loss.mean()\n",
    "                  regression_loss = regression_loss.mean()\n",
    "                  loss = classification_loss + regression_loss\n",
    "\n",
    "                  test_loss += loss.item()\n",
    "\n",
    "                for param_group in optimizer.param_groups:\n",
    "                  learning_rate_extract = param_group[\"lr\"]\n",
    "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Test mAR: {:0.2f}% | Missed Valid Images: {}\".format(\n",
    "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
    "                    (test_mAP / float(lo_test_dataset)) * 100., (test_mAR / float(lo_test_dataset)) * 100.,missed_test_images))\n",
    "\n",
    "              assert (steps % print_every) == 0\n",
    "              train_loss = 0\n",
    "              # scheduler.step(test_loss / float(lo_test_dataset))\n",
    "             \n",
    "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Train mAR: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
    "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
    "            epoch + 1, (train_mAR / float(lo_train_dataset)) * 100., \n",
    "            epoch + 1, missed_train_images, lo_train_dataset\n",
    "        ))\n",
    "    \n",
    "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Note: Train Accuracies are only run through one train image per batch\n",
      "tensor([[143.,  96., 292., 235.]], device='cuda:0') \n",
      " tensor([[  0.0000,   0.0000, 512.0000, 512.0000],\n",
      "        [  0.0000,   0.0000, 512.0000, 512.0000],\n",
      "        [  0.0000,   0.0000, 512.0000, 512.0000],\n",
      "        ...,\n",
      "        [161.3834,  33.3957, 279.5280,  35.1457],\n",
      "        [  0.0000,  61.7551, 150.5256,  63.4067],\n",
      "        [  0.0000, 102.2922, 512.0000, 103.9337]], device='cuda:0') tensor([1.0000, 1.0000, 1.0000,  ..., 0.0010, 0.0010, 0.0010], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c8faeeaf9f49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_effdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meff_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meff_valid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-51c91b5f1eed>\u001b[0m in \u001b[0;36mtrain_effdet\u001b[0;34m(net, epochs, train_loader, test_loader, lr, weight_decay, print_times_per_epoch, lo_test_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     79\u001b[0m                   \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                   test_mAP, test_mAR, missed_test_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, \n\u001b[0;32m---> 81\u001b[0;31m                                                                                  test_mAP, test_mAR, missed_test_images, device)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                   \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f903546c6ec3>\u001b[0m in \u001b[0;36mrun_metrics_for_effdet_batch\u001b[0;34m(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mcurr_mAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_mAR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mmAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmAR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmAP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr_mAP\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmAR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr_mAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration over a 0-d tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "train_effdet(model, 3, eff_train_loader, eff_valid_loader, 0.001, 1e-4, print_times_per_epoch = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0cu33Jwf_K4"
   },
   "source": [
    "# Write inference code for this pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aShMihw2d4tB"
   },
   "source": [
    "### Using Recurrent Neural Networks with Faster R CNNs\n",
    "\n",
    "https://arxiv.org/pdf/2010.15740.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH2vWjX4IG_U"
   },
   "source": [
    "####  Get all images file paths in a folder and all xml file paths in a folder. Then put in tuple [(image file path1, xml file path1), (image2, xml 2)]\n",
    "\n",
    "Pair the image file paths and xml file paths into a particular scene. \n",
    "So all_scenes dict(): {folder name of one scene: list[(imf_path1, xml1), (img_path2, xml2), (img_path3, xml3)], foler name of second scene}\n",
    "\n",
    "key_path: [scene 1, scene 2, scene 3, scene 4, scene 5]\n",
    "\n",
    "class will get certain_scene = key_path[index] and then all_scenes[certain_scene] -> get access to list of images and labels and then load into image file path. \n",
    "\n",
    "create a tensor called single_scene\n",
    "\n",
    "For one indexed scene\n",
    "for tuple in list we get a tupe like this (img file path, annot file path)\n",
    "open image file path of (tup[0])\n",
    "open xml file and parse to get bounding boxes and other info (tup[1])\n",
    "\n",
    "use torch.stack()\n",
    "\n",
    "Now we have image tensor and target tensor. We can append to the single_scene.\n",
    "\n",
    "return single_scene which is [(image 1 tensor, target tensor of 1), (image 2 tensor, target tensor of 2)] also known as all the images and targets of one scene\n",
    "\n",
    "return this data to dataloader.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFgKvZ_k19LQ"
   },
   "outputs": [],
   "source": [
    "def get_per_scene_dict(img_root_path, annotations_root_path):\n",
    "  scene_names = glob.glob(\"{}/*\".format(img_root_path))\n",
    "  all_scenes = dict()\n",
    "  for scene in scene_names:\n",
    "  \n",
    "    image_file_paths = glob.glob(\"{}{}/*.JPEG\".format(img_root_path, scene.split(\"/\")[-1]))\n",
    "    xml_file_paths = glob.glob(\"{}{}/*.xml\".format(annotations_root_path, scene.split(\"/\")[-1]))\n",
    "    image_file_paths, xml_file_paths = sorted(image_file_paths), sorted(xml_file_paths)\n",
    "\n",
    "    assert len(image_file_paths) == len(xml_file_paths)\n",
    "\n",
    "    scene_list = [(image_file_path, xml_file_paths[ii]) for ii, image_file_path in enumerate(image_file_paths)]\n",
    "    all_scenes[scene] = scene_list\n",
    "  \n",
    "  return all_scenes\n",
    "\n",
    "class VideoFrameDataset():\n",
    "    \n",
    "    def __init__(self, all_scenes_dict, transforms, seg_len = 5):\n",
    "\n",
    "      self.all_scenes_dict = all_scenes_dict\n",
    "      self.seg_len = seg_len\n",
    "      self.transforms = transforms\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "      \n",
    "      current_scene = list(self.all_scenes_dict.keys())[idx]\n",
    "      list_of_fp_per_scene = self.all_scenes_dict[current_scene]\n",
    "\n",
    "      #Random Sampling. \n",
    "      if seg_len % 5 != 0:\n",
    "        raise ValueError(\"Not allowed value for seg_len must be divisible by 5\")\n",
    "      if seg_len >= len(list_of_fp_per_scene):\n",
    "        raise ValueError(\"Segments are bigger than the amount of frames available for a scene\")\n",
    "      \n",
    "      list_of_fp_per_scene = list_of_fp_per_scene[:-(len(list_of_fp_per_scene) % seg_len)]\n",
    "      reduced_fp_per_scene, start_index = list(), 0\n",
    "      for window in range(int(len(list_of_fp_per_scene) / self.seg_len))\n",
    "        end_index = start_index + self.seg_len\n",
    "        reduced_fp_per_scene.append(random.sample(list_of_fp_per_scene[start_index: end_index], 1)[0])\n",
    "        start_index = end_index\n",
    "      \n",
    "      scene_images, scene_bboxes = [], []\n",
    "      for data in reduced_fp_per_scene:\n",
    "        img_path, xml_path = data\n",
    "\n",
    "        img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        xml_doc = ElementTree.parse(xml_path)\n",
    "\n",
    "        bounding_boxes = xml_doc.findall(\"object/bndbox\")\n",
    "\n",
    "        bbox = []\n",
    "        for node in bounding_boxes:\n",
    "          xmax = node.find(\"xmax\").text\n",
    "          xmin = node.find(\"xmin\").text\n",
    "          ymax = node.find(\"ymax\").text\n",
    "          ymin = node.find(\"ymin\").text\n",
    "\n",
    "          bbox.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
    "        \n",
    "        bbox = torch.as_tensor(bbox, dtype = torch.float32)\n",
    "        \n",
    "        if self.transforms: \n",
    "          sample = {\n",
    "              'image': img,\n",
    "              'bboxes': bbox,\n",
    "              'labels': labels\n",
    "              }\n",
    "\n",
    "        sample = self.transforms(**sample)\n",
    "        augmented_img = sample['image']\n",
    "        augmented_bbox = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        scene_bboxes.append(augmented_bbox)\n",
    "        scene_images.append(augmented_img)\n",
    "      \n",
    "      scene_images, scene_bboxes = torch.stack(scene_images), torch.stack(scene_bboxes)\n",
    "\n",
    "      scene_target = dict() \n",
    "      scene_target[\"boxes\"] = scene_bboxes\n",
    "\n",
    "      scene_target[\"image_id\"] = ###############################################\n",
    "      scene_target[\"labels\"] = #################################################\n",
    "\n",
    "      return scene_images, scene_target\n",
    "      \n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nChPPsoeUE2M"
   },
   "outputs": [],
   "source": [
    "def test(net, test_loader, ####):\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    test_mAP = test_loss = test_missed_images = 0\n",
    "    for images, targets in test_loader:\n",
    "\n",
    "        if device == torch.device(\"cuda\"):\n",
    "          images = [image.to(device) for image in images]\n",
    "          targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
    "\n",
    "        net.eval()\n",
    "        output = net(images)\n",
    "        test_mAP, test_missed_images = run_metrics_for_batch(output, targets, test_mAP, test_missed_images, device)\n",
    "\n",
    "        net.train()\n",
    "        test_loss_dict = net(images, targets)\n",
    "        test_losses = sum(loss for loss in test_loss_dict.values())\n",
    "        test_loss += test_losses\n",
    "\n",
    "    print(\"Test mAP {:0.2f}% | Test Loss {:0.2f} | Test Missed Images {} / {}\".format(test_mAP, test_loss, test_missed_images, #####))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links that I used to get notebook running with correct 3rd party packages\n",
    "\n",
    "* https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "* https://www.digitalocean.com/community/tutorials/how-to-set-up-jupyter-notebook-with-python-3-on-ubuntu-18-04\n",
    "* Make sure after running packages update on conda environment to shut down and reopen notebook for update.\n",
    "* Just git clone in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "NHV44UYGVab0"
   },
   "outputs": [],
   "source": [
    "class VideoFrameDataset():\n",
    "    \n",
    "    def __init__(self, mode, vid_data_root_dir, vid_annotations_root_dir, transforms, amount_per_folder = None, \n",
    "               make_valid_smaller_percent = None, effdet_data = None, rcnn_big = None, det_text_file_paths = None):\n",
    "        \n",
    "    # If valid smaller is true cut the length of valid list to certain length with if statement. \n",
    "        if (mode == \"train\"):\n",
    "            if (amount_per_folder):\n",
    "                #Subset images from every scene there are 3862 data folders.\n",
    "                print(\"There are 3862 train data folders in total\")\n",
    "                globbed_image_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_data_root_dir)))\n",
    "                globbed_annotations_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_annotations_root_dir)))\n",
    "                \n",
    "                self.image_file_paths, self.annotations_file_paths = list(), list()\n",
    "                \n",
    "                for folder in list(zip(globbed_image_file_paths, globbed_annotations_file_paths)):\n",
    "                    scene_images = sorted(glob.glob(\"{}/*.JPEG\".format(folder[0])))\n",
    "                    scene_annotations = sorted(glob.glob(\"{}/*.xml\".format(folder[1])))\n",
    "                    \n",
    "                    image_annot = list(zip(scene_images, scene_annotations))\n",
    "                    #Use span evenly distributed \n",
    "                    \n",
    "                    image_annot = random.sample(image_annot, amount_per_folder)\n",
    "                    \n",
    "                    scene_images, scene_annotations = zip(*image_annot) \n",
    "                    \n",
    "                    self.annotations_file_paths.extend(scene_annotations)\n",
    "                    self.image_file_paths.extend(scene_images)\n",
    "                    \n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "            \n",
    "        elif (mode == \"validation\"):\n",
    "            if make_valid_smaller_percent:\n",
    "                #Subset a percent of the valid data\n",
    "                valid_image_list = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir)))\n",
    "                valid_annotations_list = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "\n",
    "                subset = int(len(valid_image_list) * make_valid_smaller_percent)\n",
    "\n",
    "                #Shuffle both lists at once with same order\n",
    "                mapIndexPosition = list(zip(valid_image_list, valid_annotations_list))\n",
    "                random.shuffle(mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = zip(*mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = list(valid_image_list), list(valid_annotations_list)\n",
    "\n",
    "                self.image_file_paths = valid_image_list[:subset]\n",
    "                self.annotations_file_paths = valid_annotations_list[:subset]\n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "        else:\n",
    "            raise ValueError(\"Choose mode between train or validation only\")\n",
    "\n",
    "        self.labels_key = get_class_info(get_keys = True)\n",
    "\n",
    "        assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "        print(\"Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "        print(\"Amount of annotation files in Dataset {}\".format(len(self.annotations_file_paths)))\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.effdet_data = effdet_data\n",
    "        self.rcnn_big = rcnn_big\n",
    "        \n",
    "#         if det_text_file_paths:\n",
    "#             det_txt = open(det_text_file_paths, \"r\").readlines()\n",
    "#             for line in det_txt:\n",
    "#                 if len(line) == 56:\n",
    "#                     self.image_file_paths.append((line.split(\" \")[0] + \".JPEG\"))\n",
    "#                     self.annotations_file_paths.append((line.split(\" \")[0] + \".xml\"))\n",
    "        \n",
    "        \n",
    "        if self.effdet_data:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with Effecient Det Structure ... \\n\")\n",
    "        elif self.rcnn_big:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with bigger rcnn with ROI Structure ... \\n\")\n",
    "        else:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with mobilenet Faster R CNN Structure ... \\n\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path, xml_path = self.image_file_paths[idx], self.annotations_file_paths[idx]\n",
    "\n",
    "        img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        # img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        xml_doc = ElementTree.parse(xml_path)\n",
    "\n",
    "        bounding_boxes_nodes = xml_doc.findall(\"object/bndbox\")\n",
    "        labels_nodes = xml_doc.findall(\"object/name\")\n",
    "\n",
    "        bbox, labels = [], []\n",
    "\n",
    "        for node in bounding_boxes_nodes:\n",
    "            xmax = node.find(\"xmax\").text\n",
    "            xmin = node.find(\"xmin\").text\n",
    "            ymax = node.find(\"ymax\").text\n",
    "            ymin = node.find(\"ymin\").text\n",
    "            bbox.append([int(xmin), int(ymin), int(xmax), int(ymax)])  \n",
    "        bbox = torch.as_tensor(bbox, dtype = torch.float32)\n",
    "\n",
    "        for node in labels_nodes:\n",
    "            if self.effdet_data or self.rcnn_big:\n",
    "                label = self.labels_key.index(node.text)\n",
    "            else:\n",
    "                label = self.labels_key.index(node.text) + 1\n",
    "            labels.append(label)\n",
    "        labels = torch.as_tensor(labels, dtype = torch.int64)\n",
    "\n",
    "        # labels = tf.cast(labels, dtype = tf.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        try:\n",
    "            if self.transforms:  \n",
    "                sample = {\n",
    "                    'image': img,\n",
    "                    'bboxes': bbox,\n",
    "                    'labels': labels\n",
    "                      }\n",
    "\n",
    "                sample = self.transforms(**sample)\n",
    "                img = sample['image']\n",
    "                try:\n",
    "                    bbox = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                except:\n",
    "                    bbox = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        except:\n",
    "            print(\"Caught error. Now trying to instill transforms using Pytorch transforms\")\n",
    "            \n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = transforms.ToTensor()(img) \n",
    "                \n",
    "          # img = tf.cast(sample['image'], dtype = tf.float32) / 255.0\n",
    "          # bbox = tf.convert_to_tensor(torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0), dtype = tf.float32)\n",
    "\n",
    "        if self.effdet_data:\n",
    "            return {\"image\": img, \"bboxes\": bbox, \"category_id\": labels}\n",
    "        \n",
    "        target = dict()\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        return img, target  \n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.image_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wO5yLFymZOSZ",
    "outputId": "c4684fb1-e61e-452d-cad6-5c833149b1ec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Device: cuda\n",
      "Optimizer: SAM (\n",
      "Parameter Group 0\n",
      "    N_sma_threshhold: 5\n",
      "    alpha: 0.5\n",
      "    betas: (0.95, 0.999)\n",
      "    eps: 1e-05\n",
      "    initial_lr: 0.001\n",
      "    k: 6\n",
      "    lr: 0.001\n",
      "    rho: 0.05\n",
      "    step_counter: 0\n",
      "    weight_decay: 0.0001\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer/ranger/ranger.py:138: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370128159/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "/home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch Number: 28 | LR: 0.00100 | Train_loss: 94.32 | Valid_loss: 76.92 | Valid mAP: 1.16% | Valid Missed Images 75 / 88\n",
      "Epoch 1/1 | Batch Number: 56 | LR: 0.00100 | Train_loss: 27.41 | Valid_loss: 62.92 | Valid mAP: 3.97% | Valid Missed Images 76 / 88\n",
      "Epoch 1/1 | Batch Number: 84 | LR: 0.00100 | Train_loss: 25.47 | Valid_loss: 50.36 | Valid mAP: 11.10% | Valid Missed Images 57 / 88\n",
      "Epoch 1/1 | Batch Number: 112 | LR: 0.00100 | Train_loss: 18.71 | Valid_loss: 40.04 | Valid mAP: 7.95% | Valid Missed Images 69 / 88\n",
      "Epoch 1/1 | Batch Number: 140 | LR: 0.00100 | Train_loss: 21.55 | Valid_loss: 39.10 | Valid mAP: 0.32% | Valid Missed Images 66 / 88\n",
      "Epoch 1/1 | Batch Number: 168 | LR: 0.00100 | Train_loss: 17.03 | Valid_loss: 32.78 | Valid mAP: 3.97% | Valid Missed Images 77 / 88\n",
      "Epoch 1/1 | Batch Number: 196 | LR: 0.00100 | Train_loss: 18.85 | Valid_loss: 36.41 | Valid mAP: 24.06% | Valid Missed Images 32 / 88\n",
      "Epoch 1/1 | Batch Number: 224 | LR: 0.00100 | Train_loss: 14.55 | Valid_loss: 35.09 | Valid mAP: 2.53% | Valid Missed Images 78 / 88\n",
      "Epoch 1/1 | Batch Number: 252 | LR: 0.00100 | Train_loss: 15.28 | Valid_loss: 35.01 | Valid mAP: 1.14% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 280 | LR: 0.00100 | Train_loss: 12.37 | Valid_loss: 33.99 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 308 | LR: 0.00100 | Train_loss: 14.67 | Valid_loss: 32.56 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 336 | LR: 0.00100 | Train_loss: 13.46 | Valid_loss: 31.55 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 364 | LR: 0.00100 | Train_loss: 15.14 | Valid_loss: 34.07 | Valid mAP: 11.36% | Valid Missed Images 76 / 88\n",
      "Epoch 1/1 | Batch Number: 392 | LR: 0.00100 | Train_loss: 12.74 | Valid_loss: 32.65 | Valid mAP: 0.00% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 420 | LR: 0.00100 | Train_loss: 16.99 | Valid_loss: 35.44 | Valid mAP: 0.57% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 448 | LR: 0.00100 | Train_loss: 15.36 | Valid_loss: 31.74 | Valid mAP: 3.41% | Valid Missed Images 79 / 88\n",
      "Epoch 1/1 | Batch Number: 476 | LR: 0.00100 | Train_loss: 12.81 | Valid_loss: 32.34 | Valid mAP: 2.27% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 504 | LR: 0.00100 | Train_loss: 19.45 | Valid_loss: 43.06 | Valid mAP: 0.19% | Valid Missed Images 86 / 88\n",
      "Epoch 1/1 | Batch Number: 532 | LR: 0.00099 | Train_loss: 15.30 | Valid_loss: 33.95 | Valid mAP: 4.84% | Valid Missed Images 81 / 88\n",
      "Epoch 1/1 | Batch Number: 560 | LR: 0.00099 | Train_loss: 11.59 | Valid_loss: 36.38 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 588 | LR: 0.00099 | Train_loss: 16.96 | Valid_loss: 39.23 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 616 | LR: 0.00099 | Train_loss: 71.33 | Valid_loss: 50.94 | Valid mAP: 34.25% | Valid Missed Images 41 / 88\n",
      "Epoch 1/1 | Batch Number: 644 | LR: 0.00099 | Train_loss: 20.04 | Valid_loss: 50.85 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 672 | LR: 0.00099 | Train_loss: 46.87 | Valid_loss: 44.66 | Valid mAP: 1.14% | Valid Missed Images 86 / 88\n",
      "Epoch 1/1 | Batch Number: 700 | LR: 0.00099 | Train_loss: 22.35 | Valid_loss: 36.40 | Valid mAP: 2.27% | Valid Missed Images 86 / 88\n",
      "Epoch 1/1 | Batch Number: 728 | LR: 0.00099 | Train_loss: 13.44 | Valid_loss: 37.08 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 756 | LR: 0.00099 | Train_loss: 11.84 | Valid_loss: 37.66 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 784 | LR: 0.00099 | Train_loss: 14.93 | Valid_loss: 39.79 | Valid mAP: 0.57% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 812 | LR: 0.00099 | Train_loss: 13.54 | Valid_loss: 36.03 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 840 | LR: 0.00099 | Train_loss: 15.20 | Valid_loss: 34.60 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 868 | LR: 0.00099 | Train_loss: 12.57 | Valid_loss: 34.46 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 896 | LR: 0.00099 | Train_loss: 12.98 | Valid_loss: 39.05 | Valid mAP: 0.23% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 924 | LR: 0.00098 | Train_loss: 19.98 | Valid_loss: 40.42 | Valid mAP: 0.57% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 952 | LR: 0.00098 | Train_loss: 13.05 | Valid_loss: 41.11 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 980 | LR: 0.00098 | Train_loss: 16.54 | Valid_loss: 33.79 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c125db021ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr_cnn_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-158e2967f03d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch, lo_valid_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Check for degenerate boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 raise ValueError(\"images is expected to be a list of 3d tensors \"\n\u001b[1;32m    103\u001b[0m                                  \"of shape [C, H, W], got {}\".format(image.shape))\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r_cnn_trained = train(model, 1, train_loader, valid_loader, 0.001, weight_decay = 1e-4, print_times_per_epoch = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Device: cuda\n",
      "Optimizer: SAM (\n",
      "Parameter Group 0\n",
      "    N_sma_threshhold: 5\n",
      "    alpha: 0.5\n",
      "    betas: (0.95, 0.999)\n",
      "    eps: 1e-05\n",
      "    initial_lr: 0.0001\n",
      "    k: 6\n",
      "    lr: 0.0001\n",
      "    rho: 0.05\n",
      "    step_counter: 0\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer/ranger/ranger.py:138: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370128159/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "/home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch Number: 22447 | LR: 0.00010 | Train_loss: 112178093.03 | Valid_loss: 1046.19 | Valid mAP: 6.47% | Valid Missed Images 0 / 1761\n",
      "Epoch 1/1 | Batch Number: 44894 | LR: 0.00010 | Train_loss: 3822367.73 | Valid_loss: 1813196.61 | Valid mAP: 20.79% | Valid Missed Images 0 / 1761\n",
      "Epoch 1/1 | Batch Number: 67341 | LR: 0.00010 | Train_loss: 68994806.74 | Valid_loss: 666.07 | Valid mAP: 1.79% | Valid Missed Images 1709 / 1761\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected x_max for bbox (tensor(0.9219), tensor(0.3847), tensor(1.0063), tensor(0.5528), tensor(7)) to be in the range [0.0, 1.0], got 1.006250023841858.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b2c9aa3dc8d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmob_net_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmob_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-a4be57bd6387>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch, lo_valid_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_missed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e12314bbb96e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   }\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdual_start_end\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdual_start_end\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/core/utils.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_and_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_and_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/core/utils.py\u001b[0m in \u001b[0;36mcheck_and_convert\u001b[0;34m(self, data, rows, cols, direction)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_from_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_to_albumentations\u001b[0;34m(self, data, rows, cols)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_bboxes_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_bboxes_to_albumentations\u001b[0;34m(bboxes, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\n\u001b[1;32m    302\u001b[0m     \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert_bbox_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\n\u001b[1;32m    302\u001b[0m     \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert_bbox_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_bbox_to_albumentations\u001b[0;34m(bbox, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mcheck_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mcheck_bbox\u001b[0;34m(bbox)\u001b[0m\n\u001b[1;32m    328\u001b[0m             raise ValueError(\n\u001b[1;32m    329\u001b[0m                 \u001b[0;34m\"Expected {name} for bbox {bbox} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0;34m\"to be in the range [0.0, 1.0], got {value}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             )\n\u001b[1;32m    332\u001b[0m     \u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected x_max for bbox (tensor(0.9219), tensor(0.3847), tensor(1.0063), tensor(0.5528), tensor(7)) to be in the range [0.0, 1.0], got 1.006250023841858."
     ]
    }
   ],
   "source": [
    "mob_net_trained = train(mob_net, 1, train_loader, valid_loader, 0.0001, weight_decay = 1e-5, print_times_per_epoch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resizer(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=512):\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, annots = sample['img'], sample['annot']\n",
    "        height, width, _ = image.shape\n",
    "        if height > width:\n",
    "            scale = self.img_size / height\n",
    "            resized_height = self.img_size\n",
    "            resized_width = int(width * scale)\n",
    "        else:\n",
    "            scale = self.img_size / width\n",
    "            resized_height = int(height * scale)\n",
    "            resized_width = self.img_size\n",
    "\n",
    "        image = cv2.resize(image, (resized_width, resized_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        new_image = np.zeros((self.img_size, self.img_size, 3))\n",
    "        new_image[0:resized_height, 0:resized_width] = image\n",
    "\n",
    "        annots[:, :4] *= scale\n",
    "\n",
    "        return {'img': torch.from_numpy(new_image).to(torch.float32), 'annot': torch.from_numpy(annots), 'scale': scale}\n",
    "\n",
    "\n",
    "class Augmenter(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample, flip_x=0.5):\n",
    "        if np.random.rand() < flip_x:\n",
    "            image, annots = sample['img'], sample['annot']\n",
    "            image = image[:, ::-1, :]\n",
    "\n",
    "            rows, cols, channels = image.shape\n",
    "\n",
    "            x1 = annots[:, 0].copy()\n",
    "            x2 = annots[:, 2].copy()\n",
    "\n",
    "            x_tmp = x1.copy()\n",
    "\n",
    "            annots[:, 0] = cols - x2\n",
    "            annots[:, 2] = cols - x_tmp\n",
    "\n",
    "            sample = {'img': image, 'annot': annots}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "\n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        self.mean = np.array([[mean]])\n",
    "        self.std = np.array([[std]])\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, annots = sample['img'], sample['annot']\n",
    "\n",
    "        return {'img': ((image.astype(np.float32) - self.mean) / self.std), 'annot': annots}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ILSVRCVid2015VideoDetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "169081d5b0d94288a2b0ebb020b790d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1c3baca68a4e42afa680e581e92a7e82": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28ba486f593a4109bd152e94c871919d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c3baca68a4e42afa680e581e92a7e82",
      "max": 14212972,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_169081d5b0d94288a2b0ebb020b790d3",
      "value": 14212972
     }
    },
    "4839a59e79b6413bab12980f433870dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc398d009abc4ff7872a46b6e7bd497e",
      "placeholder": "​",
      "style": "IPY_MODEL_cb9ef1bcc2334f7190b70c08a16090f7",
      "value": " 13.6M/13.6M [00:16&lt;00:00, 875kB/s]"
     }
    },
    "afba5dfb1b1a4811b1b0c02f03cb3c8d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbfa8ade6ff548aab08c392fe4bb20f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_28ba486f593a4109bd152e94c871919d",
       "IPY_MODEL_4839a59e79b6413bab12980f433870dc"
      ],
      "layout": "IPY_MODEL_afba5dfb1b1a4811b1b0c02f03cb3c8d"
     }
    },
    "cb9ef1bcc2334f7190b70c08a16090f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc398d009abc4ff7872a46b6e7bd497e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
