{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Jvvfb1riBioe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch \n",
    "from torch import nn, optim \n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.utils.data\n",
    "import time\n",
    "import itertools\n",
    "import glob \n",
    "from PIL import Image\n",
    "import csv \n",
    "import cv2\n",
    "import re\n",
    "import torchvision\n",
    "import random\n",
    "from xml.etree import ElementTree\n",
    "from torchvision.ops.boxes import box_iou\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sarthak/DataSets/ILSVRC2015/EfficientDet.Pytorch-Updated\n",
      "/home/sarthak/DataSets/ILSVRC2015\n"
     ]
    }
   ],
   "source": [
    "#Packages commonly needed to install pycocotools, tqdm, and requests.\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "%cd EfficientDet.Pytorch-Updated/\n",
    "import math\n",
    "from models.efficientnet import EfficientNet\n",
    "from models.bifpn import BIFPN\n",
    "from models.retinahead import RetinaHead\n",
    "from models.module import RegressionModel, ClassificationModel, Anchors, ClipBoxes, BBoxTransform\n",
    "from torchvision.ops import nms\n",
    "from models.losses import FocalLoss\n",
    "from models.efficientdet import EfficientDet\n",
    "from models.losses import FocalLoss\n",
    "from datasets import VOCDetection, CocoDataset, get_augumentation, detection_collate, Resizer, Normalizer, Augmenter, collater\n",
    "from utils import EFFICIENTDET, get_state_dict\n",
    "from eval import evaluate, evaluate_coco\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Ranger-Deep-Learning-Optimizer' already exists and is not an empty directory.\n",
      "/home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer\n",
      "Obtaining file:///home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer\n",
      "Requirement already satisfied: torch in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from ranger==0.1.dev0) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torch->ranger==0.1.dev0) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torch->ranger==0.1.dev0) (1.19.2)\n",
      "Installing collected packages: ranger\n",
      "  Attempting uninstall: ranger\n",
      "    Found existing installation: ranger 0.1.dev0\n",
      "    Uninstalling ranger-0.1.dev0:\n",
      "      Successfully uninstalled ranger-0.1.dev0\n",
      "  Running setup.py develop for ranger\n",
      "Successfully installed ranger\n",
      "/home/sarthak/DataSets/ILSVRC2015\n",
      "fatal: destination path 'sam' already exists and is not an empty directory.\n",
      "/home/sarthak/DataSets/ILSVRC2015/sam\n",
      "Imported SAM Successfully from github .py file\n",
      "/home/sarthak/DataSets/ILSVRC2015\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "%cd Ranger-Deep-Learning-Optimizer\n",
    "!pip install -e .\n",
    "from ranger import Ranger  \n",
    "%cd ..\n",
    "#https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1\n",
    "!git clone https://github.com/davda54/sam.git\n",
    "%cd sam\n",
    "import sam\n",
    "print(\"Imported SAM Successfully from github .py file\")\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPKm4C39fzGD"
   },
   "source": [
    "## To DO \n",
    "\n",
    "### Put saving model code\n",
    "\n",
    "\n",
    "\n",
    "### Get better results\n",
    "\n",
    "* Reduce dataset to 5000 train images and 500 valid images\n",
    "\n",
    "\n",
    "### Fix the metric creating of Effecient Det Model. \n",
    "\n",
    "* Push learning rate higher (current lr) * 10\n",
    "\n",
    "Way to make Effecient Det work: \n",
    "* Turn the model to train mode\n",
    "* Put some annotations and an image into the model and run it, which will get the output that will enter run metrics batch\n",
    "* Then try running model on that input with bigger batch sizes. \n",
    "* Figure out how to get the data for one image in calculate metrics \n",
    "* Generalize to see how a batch of predictions can be run to the metrics\n",
    "* Make sure mAP and missed images are the only things coming in and out.\n",
    "* Put code into function and run it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Goal:\n",
    "\n",
    "Get acceptable resukts with a rcnn torchvision model\n",
    "Get acceptable results with effecient det\n",
    "\n",
    "### Some Errors I get\n",
    "\n",
    "Value Error in Det dataset\n",
    "Model sometimes gets error while running on data.\n",
    "\n",
    "### Advice\n",
    "\n",
    "Try to start with simplest (Adam basics) and keep on adding stuff to improve mAP\n",
    " How much data 57 k in Det dataset\n",
    " Full validation has 170,00 but use \n",
    " \n",
    " * (2,000 images in valid loader).\n",
    "* 10 - 15 images per folder.\n",
    "\n",
    "Great Proportions is that around 35 - 40% should be the Det Dataset and 60 - 65 % should be the sampled VID Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "collapsed": true,
    "id": "G9vLpqYsehKW",
    "outputId": "5ae89189-d6ba-4cdd-d080-4a77f7c5a410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: PyYAML in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (5.4.1)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (0.18.1)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (0.4.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: scipy in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (1.19.2)\n",
      "Requirement already satisfied: Shapely in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
      "Requirement already satisfied: opencv-python in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: six in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (3.3.4)\n",
      "Requirement already satisfied: imageio in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (2.9.0)\n",
      "Requirement already satisfied: Pillow in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (8.2.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.5.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.4.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "juEv3bvw0yYp"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uXvczm7BMhQw"
   },
   "outputs": [],
   "source": [
    "def get_class_info(get_keys = False, smaller_mb_net = False):\n",
    "    obj_dict = {\n",
    "    \"n02691156\": \"airplane\", \n",
    "    \"n02419796\": \"antelope\", \n",
    "    \"n02131653\": \"bear\", \n",
    "    \"n02834778\": \"bicycle\", \n",
    "    \"n01503061\": \"bird\", \n",
    "    \"n02924116\": \"bus\", \n",
    "    \"n02958343\": \"car\", \n",
    "    \"n02402425\": \"cattle\", \n",
    "    \"n02084071\": \"dog\", \n",
    "    \"n02121808\": \"domestic_cat\", \n",
    "    \"n02503517\": \"elephant\", \n",
    "    \"n02118333\": \"fox\",\n",
    "    \"n02510455\": \"giant_panda\", \n",
    "    \"n02342885\": \"hamster\", \n",
    "    \"n02374451\": \"horse\", \n",
    "    \"n02129165\": \"lion\", \n",
    "    \"n01674464\": \"lizard\", \n",
    "    \"n02484322\": \"monkey\", \n",
    "    \"n03790512\": \"motorcycle\", \n",
    "    \"n02324045\": \"rabbit\",\n",
    "    \"n02509815\": \"red_panda\", \n",
    "    \"n02411705\": \"sheep\", \n",
    "    \"n01726692\": \"snake\", \n",
    "    \"n02355227\": \"squirrel\", \n",
    "    \"n02129604\": \"tiger\", \n",
    "    \"n04468005\": \"train\", \n",
    "    \"n01662784\": \"turtle\", \n",
    "    \"n04530566\": \"watercraft\", \n",
    "    \"n02062744\": \"whale\",\n",
    "    \"n02391049\": \"zebra\"\n",
    "    }\n",
    "    \n",
    "    if get_keys:\n",
    "        return list(obj_dict.keys())\n",
    "    \n",
    "    return obj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFrameDataset():\n",
    "    \n",
    "    def __init__(self, mode, vid_data_root_dir, vid_annotations_root_dir, transforms, seg_len = None, \n",
    "               make_valid_smaller_percent = None, effdet_data = None, rcnn_big = None, det_text_file_paths = None, \n",
    "               data_size = None):\n",
    "                 \n",
    "        \n",
    "    # If valid smaller is true cut the length of valid list to certain length with if statement. \n",
    "        if (mode == \"train\"):\n",
    "            if (seg_len):\n",
    "                ORIG_SEG_LEN = seg_len\n",
    "                #Subset images from every scene there are 3862 data folders.\n",
    "                print(\"There are 1122397 train images in total\")\n",
    "                globbed_image_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_data_root_dir)))\n",
    "                globbed_annotations_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_annotations_root_dir)))\n",
    "                \n",
    "                self.image_file_paths, self.annotations_file_paths = list(), list()\n",
    "                \n",
    "                for folder in list(zip(globbed_image_file_paths, globbed_annotations_file_paths)):\n",
    "                    scene_images = sorted(glob.glob(\"{}/*.JPEG\".format(folder[0])))\n",
    "                    scene_annotations = sorted(glob.glob(\"{}/*.xml\".format(folder[1])))\n",
    "                    \n",
    "                    image_annot = list(zip(scene_images, scene_annotations))\n",
    "\n",
    "                    if seg_len % 5 != 0:\n",
    "                        raise ValueError(\"Not allowed value for seg_len must be divisible by 5\")\n",
    "                    if seg_len >= len(image_annot):\n",
    "                        seg_len = 5\n",
    "                    \n",
    "                    if len(image_annot) % seg_len != 0:\n",
    "                        image_annot = image_annot[:-(len(image_annot) % seg_len)]\n",
    "                    \n",
    "                    red_img_annot, start_index = list(), 0 \n",
    "                    \n",
    "                    for window in range(int(len(image_annot) / seg_len)):\n",
    "                        end_index = start_index + seg_len\n",
    "                        red_img_annot.append(random.sample(image_annot[start_index : end_index], 1)[0])\n",
    "                        start_index = end_index\n",
    "                    \n",
    "                   \n",
    "                    scene_images, scene_annotations = zip(*red_img_annot) \n",
    "                    \n",
    "                    self.annotations_file_paths.extend(scene_annotations)\n",
    "                    self.image_file_paths.extend(scene_images)\n",
    "                    \n",
    "                    seg_len = ORIG_SEG_LEN\n",
    "                    \n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "            \n",
    "        elif (mode == \"validation\"):\n",
    "            if make_valid_smaller_percent:\n",
    "                #Subset a percent of the valid data\n",
    "                valid_image_list = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir)))\n",
    "                valid_annotations_list = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "\n",
    "                subset = int(len(valid_image_list) * make_valid_smaller_percent)\n",
    "\n",
    "                #Shuffle both lists at once with same order\n",
    "                mapIndexPosition = list(zip(valid_image_list, valid_annotations_list))\n",
    "                random.shuffle(mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = zip(*mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = list(valid_image_list), list(valid_annotations_list)\n",
    "\n",
    "                self.image_file_paths = valid_image_list[:subset]\n",
    "                self.annotations_file_paths = valid_annotations_list[:subset]\n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "        else:\n",
    "            raise ValueError(\"Choose mode between train or validation only\")\n",
    "\n",
    "        self.labels_key = get_class_info(get_keys = True)\n",
    "\n",
    "        self.data_size = data_size\n",
    "        self.transforms = transforms\n",
    "        self.effdet_data = effdet_data\n",
    "        self.rcnn_big = rcnn_big\n",
    "        \n",
    "        if det_text_file_paths:\n",
    "            home_file_path_data = \"/data1/group/mlgroup/train_data/ILSVRC2015/Data/DET/\"\n",
    "            home_file_path_annot = \"/data1/group/mlgroup/train_data/ILSVRC2015/Annotations/DET/\"\n",
    "            \n",
    "            assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "            print(\"\\n\")\n",
    "            print(\"BEFORE DET: Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "            print(\"BEFORE DET: Amount of annotation files in Dataset {} \\n\".format(len(self.annotations_file_paths)))\n",
    "            det_txt = open(det_text_file_paths, \"r\").readlines()\n",
    "            np.random.shuffle(det_txt)\n",
    "            \n",
    "            #I am gonna sample about 10k images or around 20% of Det Dataset\n",
    "            det_txt = det_txt[:int(len(det_txt) * 0.6)]\n",
    "            print(\"Amount of images in Det Set (Approx.) {}\".format(len(det_txt)))\n",
    "            \n",
    "            for line in det_txt:\n",
    "                self.image_file_paths.append((home_file_path_data + line.split(\" \")[0] + \".JPEG\"))\n",
    "                self.annotations_file_paths.append((home_file_path_annot + line.split(\" \")[0] + \".xml\"))\n",
    "        \n",
    "        #Final sort to keep annotations and image file paths in same config\n",
    "        self.image_file_paths, self.annotations_file_paths = sorted(self.image_file_paths), sorted(self.annotations_file_paths)\n",
    "        \n",
    "        assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "        print(\"Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "        print(\"Amount of annotation files in Dataset {}\".format(len(self.annotations_file_paths)))\n",
    "        \n",
    "        \n",
    "        if self.effdet_data:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with Effecient Det Structure ... \\n\")\n",
    "        elif self.rcnn_big:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with bigger rcnn with ROI Structure ... \\n\")\n",
    "        else:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with mobilenet Faster R CNN Structure ... \\n\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path, xml_path = self.image_file_paths[idx], self.annotations_file_paths[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        # img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        marking = False\n",
    "        xml_doc = ElementTree.parse(xml_path)\n",
    "\n",
    "        bounding_boxes_nodes = xml_doc.findall(\"object/bndbox\")\n",
    "        labels_nodes = xml_doc.findall(\"object/name\")\n",
    "\n",
    "        bbox, labels = [], []\n",
    "\n",
    "        for node in bounding_boxes_nodes:\n",
    "            xmax = node.find(\"xmax\").text\n",
    "            xmin = node.find(\"xmin\").text\n",
    "            ymax = node.find(\"ymax\").text\n",
    "            ymin = node.find(\"ymin\").text\n",
    "            bbox.append([int(xmin), int(ymin), int(xmax), int(ymax)])  \n",
    "            \n",
    "\n",
    "        for node in labels_nodes:\n",
    "            if node.text in self.labels_key:\n",
    "                label = self.labels_key.index(node.text)    \n",
    "            else:\n",
    "                label = \"DNE\"\n",
    "                marking = True\n",
    "            labels.append(label)\n",
    "        \n",
    "        if (marking):\n",
    "            removed_indices = list()\n",
    "            \n",
    "            for ii in range(len(labels)):\n",
    "                if (labels[ii] == \"DNE\"):\n",
    "                    removed_indices.append(ii)\n",
    "            labels = [i for j, i in enumerate(labels) if j not in removed_indices]\n",
    "            bbox = [i for j, i in enumerate(bbox) if j not in removed_indices]\n",
    "                \n",
    "        if not(self.effdet_data or self.rcnn_big):\n",
    "            #Need to add one to labels\n",
    "            labels = [label + 1 for label in labels]\n",
    "        \n",
    "        bbox = torch.as_tensor(bbox, dtype = torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype = torch.int64)\n",
    "\n",
    "        # labels = tf.cast(labels, dtype = tf.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        try:\n",
    "            if self.transforms:  \n",
    "                sample = {\n",
    "                    'image': img,\n",
    "                    'bboxes': bbox,\n",
    "                    'labels': labels\n",
    "                      }\n",
    "\n",
    "                sample = self.transforms(**sample)\n",
    "                img = sample['image']\n",
    "                try:\n",
    "                    bbox = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                except:\n",
    "                    bbox = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                    \n",
    "        except:\n",
    "            print(\"Caught error. Now trying to instill transforms using Pytorch transforms\")\n",
    "            \n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            emergency_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "            \n",
    "            \n",
    "            img = emergency_transforms(img)\n",
    "                \n",
    "                \n",
    "          # img = tf.cast(sample['image'], dtype = tf.float32) / 255.0\n",
    "          # bbox = tf.convert_to_tensor(torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0), dtype = tf.float32)\n",
    "\n",
    "        if self.effdet_data:\n",
    "            return {\"image\": img, \"bboxes\": bbox, \"category_id\": labels}\n",
    "        \n",
    "        target = dict()\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        return img, target  \n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.data_size:\n",
    "            return self.data_size\n",
    "        return len(self.image_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change image size and try and except in dataclass before transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(mode):\n",
    "    if (mode == \"train\"):\n",
    "        return A.Compose([\n",
    "                          A.Resize(512, 512), \n",
    "                          A.OneOf([\n",
    "                          A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                         val_shift_limit=0.2, p=0.9),\n",
    "                          A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                               contrast_limit=0.2, p=0.9)],p=0.9),\n",
    "                          A.Cutout(num_holes=8, max_h_size=8, max_w_size=8, p=0.5),\n",
    "                          A.HorizontalFlip(),\n",
    "                          A.VerticalFlip(), \n",
    "                          ToTensorV2()\n",
    "                          ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    elif (mode == \"test\"):\n",
    "        return A.Compose([\n",
    "                          A.Resize(512, 512), \n",
    "                          ToTensorV2()\n",
    "                          ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    elif (mode == \"effdet_train\"):\n",
    "        return A.Compose([\n",
    "                          A.OneOf([\n",
    "                          A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                         val_shift_limit=0.2, p=0.9),\n",
    "                          A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                               contrast_limit=0.2, p=0.9)],p=0.9),\n",
    "                          A.Cutout(num_holes=8, max_h_size=4, max_w_size=4, p=0.5),\n",
    "                          A.HorizontalFlip(),\n",
    "                          A.VerticalFlip(), \n",
    "                          A.Resize(height = 512, width=512), \n",
    "                          ToTensorV2()\n",
    "                          ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    elif (mode == \"effdet_test\"):\n",
    "        return A.Compose([\n",
    "                          A.Resize(height = 512, width = 512), \n",
    "                          ToTensorV2()])\n",
    "    else:\n",
    "        raise ValueError(\"mode is wrong value can either be train or test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "IwHLAJZQjrt6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1122397 train images in total\n",
      "\n",
      "\n",
      "BEFORE DET: Amount of image files in Dataset 54647\n",
      "BEFORE DET: Amount of annotation files in Dataset 54647 \n",
      "\n",
      "Amount of images in Det Set (Approx.) 32183\n",
      "Amount of image files in Dataset 86830\n",
      "Amount of annotation files in Dataset 86830\n",
      "\n",
      "\n",
      "Loading with mobilenet Faster R CNN Structure ... \n",
      "\n",
      "Amount of image files in Dataset 2201\n",
      "Amount of annotation files in Dataset 2201\n",
      "\n",
      "\n",
      "Loading with mobilenet Faster R CNN Structure ... \n",
      "\n",
      " \n",
      " ... Seperate from Data Loader \n",
      "\n",
      "Length of train_dataset 86830\n",
      "Length of valid_dataset 2201\n"
     ]
    }
   ],
   "source": [
    "# 1122397 Files in train set total\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "#The amount of scenes to load in one go. # 2 and 2 are the best values\n",
    "train_batch_size = 2\n",
    "valid_batch_size = 2\n",
    "det_text_file = \"/data1/group/mlgroup/train_data/ILSVRC2015/DET_train_30classes.txt\"\n",
    "\n",
    "\n",
    "train_dataset = VideoFrameDataset(\"train\", os.path.join(\"Data/VID\", \"train\"), os.path.join(\"Annotations/VID\", \"train\"), get_transforms(mode = \"train\"), \n",
    "                                  seg_len = 20, det_text_file_paths = det_text_file, data_size = None)\n",
    "valid_dataset = VideoFrameDataset(\"validation\", os.path.join(\"Data/VID\", \"val\"), os.path.join(\"Annotations/VID\", \"val\"), get_transforms(mode = \"test\"),\n",
    "                                  make_valid_smaller_percent = 0.0125, data_size = None)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= collate_fn)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = valid_batch_size, shuffle = True, collate_fn = collate_fn)\n",
    "\n",
    "print(\" \\n ... Seperate from Data Loader \\n\")\n",
    "print(\"Length of train_dataset {}\".format(len(train_dataset)))\n",
    "print(\"Length of valid_dataset {}\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "3iCwxnxIcRsr"
   },
   "outputs": [],
   "source": [
    "COLORS = [(0, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0), (255, 0, 0)]\n",
    "\n",
    "def draw_boxes(boxes, labels, image, infer = False, put_text = True):\n",
    "    classes = get_class_info()\n",
    "    keys = list(classes.keys())\n",
    "\n",
    "    # read the image with OpenCV\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    if infer:\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    for i, box in enumerate(boxes):\n",
    "        color = COLORS[1]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 5\n",
    "        )\n",
    "        if put_text:\n",
    "          cv2.putText(image, classes[keys[labels[i] - 1]], (int(box[0]), int(box[1]-5)),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 2, color, 3, \n",
    "                      lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBIf0ODU0Aod"
   },
   "source": [
    "### Create a draw function to visualize some data (Will give index error if batch size < 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "YMd58kVzZd2L",
    "outputId": "3c70eca3-8a99-4b35-8df9-99b0b895efc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Passing non-integers as three-element position specification is deprecated since 3.3 and will be removed two minor releases later.\n",
      "  \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6c2b8dd2ea90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    image = draw_boxes(labels[idx][\"boxes\"], labels[idx][\"labels\"], images[idx], put_text = True)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "5ytYP752Zd7S"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(target_box,predictions_box,scores, device):\n",
    "\n",
    "    #Get most confident boxes first and least confident last\n",
    "    predictions_box = predictions_box[scores.argsort().flip(-1)]\n",
    "    iou_mat = box_iou(target_box,predictions_box)\n",
    "    #return a one by one matrix that is form (target_box, prediction_box) or (1, 1)\n",
    "    target_boxes_count, prediction_boxes_count = iou_mat.shape\n",
    "    \n",
    "    mAP_Matrix = torch.zeros_like(iou_mat)\n",
    "    # if not matrix coordinates that relate to nothing.\n",
    "    if not iou_mat[:,0].eq(0.).all():\n",
    "      index_of_biggest_iou = iou_mat[:,0].argsort()[-1]\n",
    "      mAP_Matrix[index_of_biggest_iou,0] = 1\n",
    "\n",
    "    for pr_idx in range(1,prediction_boxes_count):\n",
    "        not_assigned = torch.logical_not(mAP_Matrix[:,:pr_idx].sum(1)).long()\n",
    "        targets = not_assigned * iou_mat[:,pr_idx]\n",
    "\n",
    "        if targets.eq(0).all():\n",
    "            continue\n",
    "\n",
    "        pivot = targets.argsort()[-1]\n",
    "        mAP_Matrix[pivot,pr_idx] = 1\n",
    "\n",
    "    # mAP calculation\n",
    "    tp = mAP_Matrix.sum()\n",
    "    fp = mAP_Matrix.sum(0).eq(0).sum()\n",
    "    fn = mAP_Matrix.sum(1).eq(0).sum()\n",
    "\n",
    "    mAP = tp / (tp+fp)\n",
    "\n",
    "    return mAP\n",
    "\n",
    "def run_metrics_for_batch(output, targets, mAP, missed_images, device):\n",
    "  for pos_in_batch, image_pred in enumerate(output):\n",
    "    assert (len(image_pred[\"boxes\"]) == len(image_pred[\"labels\"]) == len(image_pred[\"scores\"]))\n",
    "    if len(image_pred[\"boxes\"]) != 0:\n",
    "      mAP += calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
    "    else:\n",
    "      missed_images += 1\n",
    "  \n",
    "  return mAP, missed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "IN1fBHzJZd92"
   },
   "outputs": [],
   "source": [
    "def train(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch,\n",
    "          lo_valid_dataset = len(valid_dataset), lo_train_dataset = len(train_dataset), saving_directory = None,\n",
    "          unique_char_for_saving = None):\n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print_every = len(train_dataset) / train_batch_size // print_times_per_epoch\n",
    "    print(\"Print Every: {}\".format(print_every))\n",
    "\n",
    "    #Check which parameters can calculate gradients. \n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "\n",
    "    base_optimizer = Ranger\n",
    "    optimizer = sam.SAM(net.parameters(), base_optimizer, lr = lr, weight_decay = weight_decay)\n",
    "#     optimizer = optim.Adam(params, lr = lr, weight_decay = weight_decay)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
    "\n",
    "    #Might be some problems with the Data Parallel code\n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#         net = nn.DistrubutedParallel(net)\n",
    "    net.to(device)\n",
    "    \n",
    "    print(\"Device: {}\".format(device))\n",
    "    print(\"Optimizer: {}\".format(optimizer))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        \n",
    "        train_loss = train_mAP = steps = train_missed_images = 0\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            \n",
    "            \n",
    "            net.train()\n",
    "            steps += 1\n",
    "\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss_dict = net(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            net.eval()\n",
    "            try:\n",
    "                train_mAP, train_missed_images = run_metrics_for_batch(net(images), targets, train_mAP, train_missed_images, device)\n",
    "            except:\n",
    "                print(images[0].size(), targets)\n",
    "                print(\"Caught an exception in an image could not predict metric for it\")\n",
    "            net.train()\n",
    "\n",
    "            losses.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            optimizer.first_step(zero_grad = True)\n",
    "\n",
    "            loss_dict = net(images, targets)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            losses.backward()\n",
    "            optimizer.second_step(zero_grad = True)\n",
    "\n",
    "            train_loss +=  losses.item()\n",
    "            scheduler.step()\n",
    "\n",
    "            if (steps % print_every) == 0:  \n",
    "                with torch.no_grad():\n",
    "                \n",
    "                    valid_mAP = valid_loss = valid_missed_images = 0\n",
    "\n",
    "                    for images, targets in valid_loader:\n",
    "                        net.eval()\n",
    "                        if device == torch.device(\"cuda\"):\n",
    "                            images = [image.to(device) for image in images]\n",
    "                            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
    "                        \n",
    "                        try:\n",
    "                            output = net(images)\n",
    "                            valid_mAP, valid_missed_images = run_metrics_for_batch(output, targets, valid_mAP, valid_missed_images, device)\n",
    "                        except:\n",
    "                            print(targets, images[0].size())\n",
    "                            print(\"Caught exception with running metrics for one valid image (skipped)\")\n",
    "\n",
    "                        net.train()\n",
    "                        valid_loss_dict = net(images, targets)\n",
    "                        valid_losses = sum(loss for loss in valid_loss_dict.values())\n",
    "                        valid_loss += valid_losses.item()\n",
    "\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        learning_rate_extract = param_group[\"lr\"]\n",
    "                    print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Valid_loss: {:0.2f} | Valid mAP: {:0.2f}% | Valid Missed Images {} / {}\".format(\n",
    "                        epoch + 1, epochs, steps, learning_rate_extract, train_loss, valid_loss,  \n",
    "                        (valid_mAP / float(lo_valid_dataset)) * 100., valid_missed_images, lo_valid_dataset))\n",
    "\n",
    "                assert (steps % print_every) == 0\n",
    "                train_loss = 0\n",
    "                 \n",
    "        print(\"\\n Epoch {} | Final Train mAP: {:0.2f}% | Final Train Missed Images {} / {} \\n\".format(\n",
    "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., train_missed_images, lo_train_dataset\n",
    "        ))\n",
    "        if saving_directory:\n",
    "            if os.path.isdir(saving_directory):\n",
    "                print(\"Saving Model path to directory {} ...  \".format(saving_directory))\n",
    "                torch.save(net.state_dicts(), saving_path)\n",
    "            else:\n",
    "                print(\"Directory Provided does not exist, hence will skip saving the model\")\n",
    "    \n",
    "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))\n",
    "    \n",
    "        # Example File path: /saved_models/epoch10small.pth \n",
    "        \n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir(\"Fved_Models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = \"Faster_rcnn_Saved_Models\"\n",
    "unqiu_char = \"Version1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Faster_rcnn_Saved_Models/Epoch1Version1.pth'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(saving_path, \"Epoch1\" + unqiu_char + \".pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mish helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU to Mish conversion for models \n",
    "#Option to switch any activation function for another.\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Mish activation loaded...\")\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        x = x *( torch.tanh(F.softplus(x)))\n",
    "\n",
    "        return x\n",
    "    \n",
    "def convert_it(model, new, replaced_act):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, replaced_act):\n",
    "            setattr(model, child_name, new)\n",
    "        else:\n",
    "            convert_it(child, new, replaced_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuNTqGHFaAyU"
   },
   "source": [
    "## Faster R CNN with mobile net backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "z-R_qJ3haFf-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mish activation loaded...\n"
     ]
    }
   ],
   "source": [
    "mob_net = torchvision.models.detection.faster_rcnn.fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "# mob_net = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
    "mob_net.roi_heads.box_predictor.cls_score.out_features = len(get_class_info())\n",
    "mob_net.roi_heads.box_predictor.bbox_pred.out_features = len(get_class_info()) * 4\n",
    "convert_it(mob_net, Mish(), nn.ReLU6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing LR to 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144.0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset) / train_batch_size // 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Every: 144.0\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Device: cuda\n",
      "Optimizer: SAM (\n",
      "Parameter Group 0\n",
      "    N_sma_threshhold: 5\n",
      "    alpha: 0.5\n",
      "    betas: (0.95, 0.999)\n",
      "    eps: 1e-05\n",
      "    initial_lr: 0.0005\n",
      "    k: 6\n",
      "    lr: 0.0005\n",
      "    rho: 0.05\n",
      "    step_counter: 0\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Epoch 1/1 | Batch Number: 144 | LR: 0.00050 | Train_loss: 75.36 | Valid_loss: 449.45 | Valid mAP: 18.10% | Valid Missed Images 35 / 2201\n",
      "Epoch 1/1 | Batch Number: 288 | LR: 0.00050 | Train_loss: 64.39 | Valid_loss: 445.75 | Valid mAP: 32.34% | Valid Missed Images 308 / 2201\n",
      "Epoch 1/1 | Batch Number: 432 | LR: 0.00050 | Train_loss: 57.59 | Valid_loss: 423.22 | Valid mAP: 24.08% | Valid Missed Images 125 / 2201\n",
      "Epoch 1/1 | Batch Number: 576 | LR: 0.00050 | Train_loss: 60.70 | Valid_loss: 450.54 | Valid mAP: 28.29% | Valid Missed Images 323 / 2201\n",
      "Epoch 1/1 | Batch Number: 720 | LR: 0.00050 | Train_loss: 58.11 | Valid_loss: 420.76 | Valid mAP: 17.24% | Valid Missed Images 106 / 2201\n",
      "Epoch 1/1 | Batch Number: 864 | LR: 0.00050 | Train_loss: 56.71 | Valid_loss: 407.67 | Valid mAP: 22.20% | Valid Missed Images 84 / 2201\n",
      "Epoch 1/1 | Batch Number: 1008 | LR: 0.00050 | Train_loss: 57.75 | Valid_loss: 418.67 | Valid mAP: 32.27% | Valid Missed Images 118 / 2201\n",
      "Epoch 1/1 | Batch Number: 1152 | LR: 0.00050 | Train_loss: 55.12 | Valid_loss: 414.13 | Valid mAP: 41.80% | Valid Missed Images 46 / 2201\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0cef6d5689ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmob_net_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmob_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-95225dd246ee>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch, lo_valid_dataset, lo_train_dataset, saving_path)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                         \u001b[0mvalid_loss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                         \u001b[0mvalid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_loss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalid_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bbox_deltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_anchors_per_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mfilter_proposals\u001b[0;34m(self, proposals, objectness, image_shapes, num_anchors_per_level)\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# non-maximum suppression, independently done per level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatched_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnms_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0;31m# keep only topk scoring predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_nms_top_n\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Not tracing, don't do anything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0mcompiled_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__original_fn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36mbatched_nms\u001b[0;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0moffsets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_coordinate\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mboxes_for_nms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffsets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes_for_nms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[1;32m     41\u001b[0m     \u001b[0m_assert_has_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mob_net_trained = train(mob_net, 1, train_loader, valid_loader, 0.0005, weight_decay = 1e-5, print_times_per_epoch = 300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: Effecient Det Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1122397 train images in total\n",
      "Amount of image files in Dataset 54647\n",
      "Amount of annotation files in Dataset 54647\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n",
      "Amount of image files in Dataset 2201\n",
      "Amount of annotation files in Dataset 2201\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n",
      " \n",
      " ... Seperate from Data Loader \n",
      "\n",
      "Length of train_dataset 500\n",
      "Length of valid_dataset 100\n"
     ]
    }
   ],
   "source": [
    "def detection_collate(batch):\n",
    "    imgs = [s['image'] for s in batch]\n",
    "    annots = [s['bboxes'] for s in batch]\n",
    "    labels = [s['category_id'] for s in batch]\n",
    "\n",
    "    max_num_annots = max(len(annot) for annot in annots)\n",
    "    annot_padded = np.ones((len(annots), max_num_annots, 5))*-1\n",
    "\n",
    "    if max_num_annots > 0:\n",
    "        for idx, (annot, lab) in enumerate(zip(annots, labels)):\n",
    "            if len(annot) > 0:\n",
    "                annot_padded[idx, :len(annot), :4] = annot\n",
    "                annot_padded[idx, :len(annot), 4] = lab\n",
    "    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded))\n",
    "\n",
    "eff_train_batch = 1\n",
    "eff_test_batch = 1\n",
    "\n",
    "eff_train_dataset = VideoFrameDataset(\"train\", os.path.join(\"Data/VID\", \"train\"), os.path.join(\"Annotations/VID\", \"train\"), get_transforms(mode = \"effdet_train\"), \n",
    "                                  seg_len = 20, det_text_file_paths = None, effdet_data = True, data_size = 500)\n",
    "eff_valid_dataset = VideoFrameDataset(\"validation\", os.path.join(\"Data/VID\", \"val\"), os.path.join(\"Annotations/VID\", \"val\"), get_transforms(mode = \"effdet_test\"),\n",
    "                                  make_valid_smaller_percent = 0.0125, effdet_data = True, data_size = 100)\n",
    "\n",
    "eff_train_loader = torch.utils.data.DataLoader(eff_train_dataset, batch_size = eff_train_batch, shuffle = True, collate_fn= detection_collate)\n",
    "eff_valid_loader = torch.utils.data.DataLoader(eff_valid_dataset, batch_size = eff_test_batch, shuffle = True, collate_fn = detection_collate)\n",
    "\n",
    "print(\" \\n ... Seperate from Data Loader \\n\")\n",
    "print(\"Length of train_dataset {}\".format(len(eff_train_dataset)))\n",
    "print(\"Length of valid_dataset {}\".format(len(eff_valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6224fb74576a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze_bn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Run with DataParallel ....'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "MODEL_MAP = {\n",
    "    'efficientdet-d0': 'efficientnet-b0',\n",
    "    'efficientdet-d1': 'efficientnet-b1',\n",
    "    'efficientdet-d2': 'efficientnet-b2',\n",
    "    'efficientdet-d3': 'efficientnet-b3',\n",
    "    'efficientdet-d4': 'efficientnet-b4',\n",
    "    'efficientdet-d5': 'efficientnet-b5',\n",
    "    'efficientdet-d6': 'efficientnet-b6',\n",
    "    'efficientdet-d7': 'efficientnet-b6',\n",
    "}\n",
    "class EfficientDet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 network='efficientdet-d0',\n",
    "                 D_bifpn=3,\n",
    "                 W_bifpn=88,\n",
    "                 D_class=3,\n",
    "                 is_training=True,\n",
    "                 threshold=0.001, #can change this value 0.01\n",
    "                 iou_threshold=1): # can change this value 0.5\n",
    "        super(EfficientDet, self).__init__()\n",
    "        \n",
    "        self.backbone = EfficientNet.from_pretrained(MODEL_MAP[network])\n",
    "        self.is_training = is_training\n",
    "        self.neck = BIFPN(in_channels=self.backbone.get_list_features()[-5:],\n",
    "                          out_channels=W_bifpn,\n",
    "                          stack=D_bifpn,\n",
    "                          num_outs=5)\n",
    "        self.bbox_head = RetinaHead(num_classes=num_classes,\n",
    "                                    in_channels=W_bifpn)\n",
    "\n",
    "        self.anchors = Anchors()\n",
    "        self.regressBoxes = BBoxTransform()\n",
    "        self.clipBoxes = ClipBoxes()\n",
    "        self.threshold = threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        self.freeze_bn()\n",
    "        self.criterion = FocalLoss()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.is_training:\n",
    "            inputs, annotations = inputs\n",
    "        else:\n",
    "            inputs = inputs\n",
    "        x = self.extract_feat(inputs)\n",
    "        outs = self.bbox_head(x)\n",
    "        classification = torch.cat([out for out in outs[0]], dim=1)\n",
    "        regression = torch.cat([out for out in outs[1]], dim=1)\n",
    "        anchors = self.anchors(inputs)\n",
    "        if self.is_training:\n",
    "            return self.criterion(classification, regression, anchors, annotations)\n",
    "        else:\n",
    "            transformed_anchors = self.regressBoxes(anchors, regression)\n",
    "            transformed_anchors = self.clipBoxes(transformed_anchors, inputs)\n",
    "            scores = torch.max(classification, dim=2, keepdim=True)[0]\n",
    "            scores_over_thresh = (scores > self.threshold)[0, :, 0]\n",
    "\n",
    "            if scores_over_thresh.sum() == 0:\n",
    "                # print('No boxes to NMS')\n",
    "                # no boxes to NMS, just return\n",
    "                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n",
    "            classification = classification[:, scores_over_thresh, :]\n",
    "            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
    "            scores = scores[:, scores_over_thresh, :]\n",
    "            anchors_nms_idx = nms(\n",
    "                transformed_anchors[0, :, :], scores[0, :, 0], iou_threshold=self.iou_threshold)\n",
    "            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(\n",
    "                dim=1)\n",
    "            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        '''Freeze BatchNorm layers.'''\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.eval()\n",
    "\n",
    "    def extract_feat(self, img):\n",
    "        \"\"\"\n",
    "            Directly extract features from the backbone+neck\n",
    "        \"\"\"\n",
    "        x = self.backbone(img)\n",
    "        x = self.neck(x[-5:])\n",
    "        return x\n",
    "\n",
    "model= EfficientDet(num_classes=len(get_class_info()),is_training=True)\n",
    "model.train()\n",
    "\n",
    "model.freeze_bn()\n",
    "\n",
    "model = model.cuda()\n",
    "print('Run with DataParallel ....')\n",
    "\n",
    "## Make sure that you add this line, even though you are not using more than one \n",
    "# GPU DataParallel adds \"module\" to the start of the model structure \n",
    "# allowing for the syntax to be correct when calling \"model.module.freeze_bn()\" for example\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "# I am doing this here an example, you do not have to call the lines below here\n",
    "model.module.is_training = True\n",
    "model.module.freeze_bn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mish activation loaded...\n"
     ]
    }
   ],
   "source": [
    "convert_it(model, Mish(), nn.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1122397 train images in total\n",
      "Amount of image files in Dataset 54647\n",
      "Amount of annotation files in Dataset 54647\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n",
      "Amount of image files in Dataset 88\n",
      "Amount of annotation files in Dataset 88\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
    "    assert (len(scores) == len(classification) == len(transformed_anchors))\n",
    "    if len(transformed_anchors) != 0:\n",
    "      print(targets[0][:, :4], \"\\n\", transformed_anchors, scores)\n",
    "      curr_mAP, curr_mAR = calculate_metrics(targets[0][:, :4], transformed_anchors, scores, device)\n",
    "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
    "    else:\n",
    "      missed_images += 1 \n",
    "    \n",
    "# def run_metrics_for_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
    "#   for pos_in_batch, image_pred in enumerate(output):\n",
    "#     assert (len(scores) == len(classification) == len(transformed_anchors))\n",
    "#     if len(image_pred[\"boxes\"]) != 0:\n",
    "#       mAP += calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
    "#     else:\n",
    "#       missed_images += 1\n",
    "  \n",
    "#   return mAP, missed_images\n",
    "      \n",
    "    return mAP, mAR, missed_images\n",
    "def detection_collate(batch):\n",
    "    imgs = [s['image'] for s in batch]\n",
    "    annots = [s['bboxes'] for s in batch]\n",
    "    labels = [s['category_id'] for s in batch]\n",
    "\n",
    "    max_num_annots = max(len(annot) for annot in annots)\n",
    "    annot_padded = np.ones((len(annots), max_num_annots, 5))*-1\n",
    "\n",
    "    if max_num_annots > 0:\n",
    "        for idx, (annot, lab) in enumerate(zip(annots, labels)):\n",
    "            if len(annot) > 0:\n",
    "                annot_padded[idx, :len(annot), :4] = annot\n",
    "                annot_padded[idx, :len(annot), 4] = lab\n",
    "    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded))\n",
    "\n",
    "train_batch_size = 8\n",
    "valid_batch_size = 1\n",
    "\n",
    "eff_train_dataset = VideoFrameDataset(\"train\", os.path.join(\"Data/VID\", \"train\"), os.path.join(\"Annotations/VID\", \"train\"), get_transforms(mode = \"effdet_train\"), \n",
    "                                  seg_len = 20, effdet_data = True)\n",
    "eff_train_loader = torch.utils.data.DataLoader(eff_train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= detection_collate)\n",
    "\n",
    "\n",
    "eff_valid_dataset = VideoFrameDataset(\"validation\", os.path.join(\"Data/VID\", \"val\"), os.path.join(\"Annotations/VID\", \"val\"), get_transforms(mode = \"effdet_test\"),\n",
    "                                  make_valid_smaller_percent = 0.01, effdet_data = True)\n",
    "eff_valid_loader = torch.utils.data.DataLoader(eff_valid_dataset, batch_size = valid_batch_size, shuffle = True, collate_fn = detection_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_effdet(net, epochs, train_loader, test_loader, lr, weight_decay, \n",
    "          print_times_per_epoch, lo_test_dataset = len(eff_valid_dataset), lo_train_dataset = len(eff_train_dataset)):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device: {}\".format(device))\n",
    "    print(\"Note: Train Accuracies are only run through one train image per batch\")\n",
    "    \n",
    "    print_every = len(train_dataset) / train_batch_size // print_times_per_epoch\n",
    "\n",
    "    if device == torch.device(\"cpu\"):\n",
    "      warnings.warn(\"Code does not support running on CPU but only GPU\")\n",
    "\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
    "\n",
    "    start_time = time.time()\n",
    "    net.module.freeze_bn()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        net.module.is_training = True\n",
    "        \n",
    "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "\n",
    "            net.train()\n",
    "            net.module.is_training = True\n",
    "\n",
    "            steps += 1\n",
    "            \n",
    "            images = images.cuda().float()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            classification_loss, regression_loss = model([images, targets])\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "            loss = classification_loss + regression_loss\n",
    "            if bool(loss == 0):\n",
    "              print('loss equal zero(0)')\n",
    "              continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            net.eval()\n",
    "            net.module.is_training = False\n",
    "            scores, classification, transformed_anchors = net(images[0].unsqueeze(0))\n",
    "#             train_mAP, train_mAR, missed_train_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, train_mAP, \n",
    "#                                                                                      train_mAR, missed_train_images, device)\n",
    "            net.train()\n",
    "            net.module.is_training = True\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if (steps % print_every) == 0:\n",
    "\n",
    "              with torch.no_grad():\n",
    "                test_mAP = test_mAR = missed_test_images = test_loss = 0\n",
    "\n",
    "                for images, targets in test_loader:\n",
    "\n",
    "                  if images.size(0) != 1:\n",
    "                    warning.warn(\"Only can validate fully with batch size of 1, \\\n",
    "                    bigger batch sizes risk Errors or Incomplete Validation\")\n",
    "                  \n",
    "                  net.eval()\n",
    "                  net.module.is_training = False\n",
    "\n",
    "                  if device == torch.device(\"cuda\"):\n",
    "                    images = images.cuda().float()\n",
    "                    targets = targets.cuda()\n",
    "\n",
    "                  scores, classification, transformed_anchors = net(images)\n",
    "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, \n",
    "                                                                                 test_mAP, test_mAR, missed_test_images, device)\n",
    "\n",
    "                  net.train()\n",
    "                  net.module.is_training = True\n",
    "\n",
    "                  classification_loss, regression_loss = model([images, targets])\n",
    "                  classification_loss = classification_loss.mean()\n",
    "                  regression_loss = regression_loss.mean()\n",
    "                  loss = classification_loss + regression_loss\n",
    "\n",
    "                  test_loss += loss.item()\n",
    "\n",
    "                for param_group in optimizer.param_groups:\n",
    "                  learning_rate_extract = param_group[\"lr\"]\n",
    "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Test mAR: {:0.2f}% | Missed Valid Images: {}\".format(\n",
    "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
    "                    (test_mAP / float(lo_test_dataset)) * 100., (test_mAR / float(lo_test_dataset)) * 100.,missed_test_images))\n",
    "\n",
    "              assert (steps % print_every) == 0\n",
    "              train_loss = 0\n",
    "              # scheduler.step(test_loss / float(lo_test_dataset))\n",
    "             \n",
    "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Train mAR: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
    "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
    "            epoch + 1, (train_mAR / float(lo_train_dataset)) * 100., \n",
    "            epoch + 1, missed_train_images, lo_train_dataset\n",
    "        ))\n",
    "    \n",
    "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Note: Train Accuracies are only run through one train image per batch\n",
      "tensor([[143.,  96., 292., 235.]], device='cuda:0') \n",
      " tensor([[  0.0000,   0.0000, 512.0000, 512.0000],\n",
      "        [  0.0000,   0.0000, 512.0000, 512.0000],\n",
      "        [  0.0000,   0.0000, 512.0000, 512.0000],\n",
      "        ...,\n",
      "        [161.3834,  33.3957, 279.5280,  35.1457],\n",
      "        [  0.0000,  61.7551, 150.5256,  63.4067],\n",
      "        [  0.0000, 102.2922, 512.0000, 103.9337]], device='cuda:0') tensor([1.0000, 1.0000, 1.0000,  ..., 0.0010, 0.0010, 0.0010], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c8faeeaf9f49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_effdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meff_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meff_valid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-51c91b5f1eed>\u001b[0m in \u001b[0;36mtrain_effdet\u001b[0;34m(net, epochs, train_loader, test_loader, lr, weight_decay, print_times_per_epoch, lo_test_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     79\u001b[0m                   \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                   test_mAP, test_mAR, missed_test_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, \n\u001b[0;32m---> 81\u001b[0;31m                                                                                  test_mAP, test_mAR, missed_test_images, device)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                   \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f903546c6ec3>\u001b[0m in \u001b[0;36mrun_metrics_for_effdet_batch\u001b[0;34m(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mcurr_mAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_mAR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mmAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmAR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmAP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr_mAP\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmAR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr_mAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration over a 0-d tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "train_effdet(model, 3, eff_train_loader, eff_valid_loader, 0.001, 1e-4, print_times_per_epoch = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0cu33Jwf_K4"
   },
   "source": [
    "# Write inference code for this pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aShMihw2d4tB"
   },
   "source": [
    "### Using Recurrent Neural Networks with Faster R CNNs\n",
    "\n",
    "https://arxiv.org/pdf/2010.15740.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH2vWjX4IG_U"
   },
   "source": [
    "####  Get all images file paths in a folder and all xml file paths in a folder. Then put in tuple [(image file path1, xml file path1), (image2, xml 2)]\n",
    "\n",
    "Pair the image file paths and xml file paths into a particular scene. \n",
    "So all_scenes dict(): {folder name of one scene: list[(imf_path1, xml1), (img_path2, xml2), (img_path3, xml3)], foler name of second scene}\n",
    "\n",
    "key_path: [scene 1, scene 2, scene 3, scene 4, scene 5]\n",
    "\n",
    "class will get certain_scene = key_path[index] and then all_scenes[certain_scene] -> get access to list of images and labels and then load into image file path. \n",
    "\n",
    "create a tensor called single_scene\n",
    "\n",
    "For one indexed scene\n",
    "for tuple in list we get a tupe like this (img file path, annot file path)\n",
    "open image file path of (tup[0])\n",
    "open xml file and parse to get bounding boxes and other info (tup[1])\n",
    "\n",
    "use torch.stack()\n",
    "\n",
    "Now we have image tensor and target tensor. We can append to the single_scene.\n",
    "\n",
    "return single_scene which is [(image 1 tensor, target tensor of 1), (image 2 tensor, target tensor of 2)] also known as all the images and targets of one scene\n",
    "\n",
    "return this data to dataloader.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFgKvZ_k19LQ"
   },
   "outputs": [],
   "source": [
    "def get_per_scene_dict(img_root_path, annotations_root_path):\n",
    "  scene_names = glob.glob(\"{}/*\".format(img_root_path))\n",
    "  all_scenes = dict()\n",
    "  for scene in scene_names:\n",
    "  \n",
    "    image_file_paths = glob.glob(\"{}{}/*.JPEG\".format(img_root_path, scene.split(\"/\")[-1]))\n",
    "    xml_file_paths = glob.glob(\"{}{}/*.xml\".format(annotations_root_path, scene.split(\"/\")[-1]))\n",
    "    image_file_paths, xml_file_paths = sorted(image_file_paths), sorted(xml_file_paths)\n",
    "\n",
    "    assert len(image_file_paths) == len(xml_file_paths)\n",
    "\n",
    "    scene_list = [(image_file_path, xml_file_paths[ii]) for ii, image_file_path in enumerate(image_file_paths)]\n",
    "    all_scenes[scene] = scene_list\n",
    "  \n",
    "  return all_scenes\n",
    "\n",
    "class VideoFrameDataset():\n",
    "    \n",
    "    def __init__(self, all_scenes_dict, transforms, seg_len = 5):\n",
    "\n",
    "      self.all_scenes_dict = all_scenes_dict\n",
    "      self.seg_len = seg_len\n",
    "      self.transforms = transforms\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "      \n",
    "      current_scene = list(self.all_scenes_dict.keys())[idx]\n",
    "      list_of_fp_per_scene = self.all_scenes_dict[current_scene]\n",
    "\n",
    "      #Random Sampling. \n",
    "      if seg_len % 5 != 0:\n",
    "        raise ValueError(\"Not allowed value for seg_len must be divisible by 5\")\n",
    "      if seg_len >= len(list_of_fp_per_scene):\n",
    "        raise ValueError(\"Segments are bigger than the amount of frames available for a scene\")\n",
    "      \n",
    "      list_of_fp_per_scene = list_of_fp_per_scene[:-(len(list_of_fp_per_scene) % seg_len)]\n",
    "      reduced_fp_per_scene, start_index = list(), 0\n",
    "      for window in range(int(len(list_of_fp_per_scene) / self.seg_len))\n",
    "        end_index = start_index + self.seg_len\n",
    "        reduced_fp_per_scene.append(random.sample(list_of_fp_per_scene[start_index: end_index], 1)[0])\n",
    "        start_index = end_index\n",
    "      \n",
    "      scene_images, scene_bboxes = [], []\n",
    "      for data in reduced_fp_per_scene:\n",
    "        img_path, xml_path = data\n",
    "\n",
    "        img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        xml_doc = ElementTree.parse(xml_path)\n",
    "\n",
    "        bounding_boxes = xml_doc.findall(\"object/bndbox\")\n",
    "\n",
    "        bbox = []\n",
    "        for node in bounding_boxes:\n",
    "          xmax = node.find(\"xmax\").text\n",
    "          xmin = node.find(\"xmin\").text\n",
    "          ymax = node.find(\"ymax\").text\n",
    "          ymin = node.find(\"ymin\").text\n",
    "\n",
    "          bbox.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
    "        \n",
    "        bbox = torch.as_tensor(bbox, dtype = torch.float32)\n",
    "        \n",
    "        if self.transforms: \n",
    "          sample = {\n",
    "              'image': img,\n",
    "              'bboxes': bbox,\n",
    "              'labels': labels\n",
    "              }\n",
    "\n",
    "        sample = self.transforms(**sample)\n",
    "        augmented_img = sample['image']\n",
    "        augmented_bbox = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        scene_bboxes.append(augmented_bbox)\n",
    "        scene_images.append(augmented_img)\n",
    "      \n",
    "      scene_images, scene_bboxes = torch.stack(scene_images), torch.stack(scene_bboxes)\n",
    "\n",
    "      scene_target = dict() \n",
    "      scene_target[\"boxes\"] = scene_bboxes\n",
    "\n",
    "      scene_target[\"image_id\"] = ###############################################\n",
    "      scene_target[\"labels\"] = #################################################\n",
    "\n",
    "      return scene_images, scene_target\n",
    "      \n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nChPPsoeUE2M"
   },
   "outputs": [],
   "source": [
    "def test(net, test_loader, ####):\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    test_mAP = test_loss = test_missed_images = 0\n",
    "    for images, targets in test_loader:\n",
    "\n",
    "        if device == torch.device(\"cuda\"):\n",
    "          images = [image.to(device) for image in images]\n",
    "          targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
    "\n",
    "        net.eval()\n",
    "        output = net(images)\n",
    "        test_mAP, test_missed_images = run_metrics_for_batch(output, targets, test_mAP, test_missed_images, device)\n",
    "\n",
    "        net.train()\n",
    "        test_loss_dict = net(images, targets)\n",
    "        test_losses = sum(loss for loss in test_loss_dict.values())\n",
    "        test_loss += test_losses\n",
    "\n",
    "    print(\"Test mAP {:0.2f}% | Test Loss {:0.2f} | Test Missed Images {} / {}\".format(test_mAP, test_loss, test_missed_images, #####))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links that I used to get notebook running with correct 3rd party packages\n",
    "\n",
    "* https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "* https://www.digitalocean.com/community/tutorials/how-to-set-up-jupyter-notebook-with-python-3-on-ubuntu-18-04\n",
    "* Make sure after running packages update on conda environment to shut down and reopen notebook for update.\n",
    "* Just git clone in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "NHV44UYGVab0"
   },
   "outputs": [],
   "source": [
    "class VideoFrameDataset():\n",
    "    \n",
    "    def __init__(self, mode, vid_data_root_dir, vid_annotations_root_dir, transforms, amount_per_folder = None, \n",
    "               make_valid_smaller_percent = None, effdet_data = None, rcnn_big = None, det_text_file_paths = None):\n",
    "        \n",
    "    # If valid smaller is true cut the length of valid list to certain length with if statement. \n",
    "        if (mode == \"train\"):\n",
    "            if (amount_per_folder):\n",
    "                #Subset images from every scene there are 3862 data folders.\n",
    "                print(\"There are 3862 train data folders in total\")\n",
    "                globbed_image_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_data_root_dir)))\n",
    "                globbed_annotations_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_annotations_root_dir)))\n",
    "                \n",
    "                self.image_file_paths, self.annotations_file_paths = list(), list()\n",
    "                \n",
    "                for folder in list(zip(globbed_image_file_paths, globbed_annotations_file_paths)):\n",
    "                    scene_images = sorted(glob.glob(\"{}/*.JPEG\".format(folder[0])))\n",
    "                    scene_annotations = sorted(glob.glob(\"{}/*.xml\".format(folder[1])))\n",
    "                    \n",
    "                    image_annot = list(zip(scene_images, scene_annotations))\n",
    "                    #Use span evenly distributed \n",
    "                    \n",
    "                    image_annot = random.sample(image_annot, amount_per_folder)\n",
    "                    \n",
    "                    scene_images, scene_annotations = zip(*image_annot) \n",
    "                    \n",
    "                    self.annotations_file_paths.extend(scene_annotations)\n",
    "                    self.image_file_paths.extend(scene_images)\n",
    "                    \n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "            \n",
    "        elif (mode == \"validation\"):\n",
    "            if make_valid_smaller_percent:\n",
    "                #Subset a percent of the valid data\n",
    "                valid_image_list = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir)))\n",
    "                valid_annotations_list = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "\n",
    "                subset = int(len(valid_image_list) * make_valid_smaller_percent)\n",
    "\n",
    "                #Shuffle both lists at once with same order\n",
    "                mapIndexPosition = list(zip(valid_image_list, valid_annotations_list))\n",
    "                random.shuffle(mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = zip(*mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = list(valid_image_list), list(valid_annotations_list)\n",
    "\n",
    "                self.image_file_paths = valid_image_list[:subset]\n",
    "                self.annotations_file_paths = valid_annotations_list[:subset]\n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "        else:\n",
    "            raise ValueError(\"Choose mode between train or validation only\")\n",
    "\n",
    "        self.labels_key = get_class_info(get_keys = True)\n",
    "\n",
    "        assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "        print(\"Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "        print(\"Amount of annotation files in Dataset {}\".format(len(self.annotations_file_paths)))\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.effdet_data = effdet_data\n",
    "        self.rcnn_big = rcnn_big\n",
    "        \n",
    "#         if det_text_file_paths:\n",
    "#             det_txt = open(det_text_file_paths, \"r\").readlines()\n",
    "#             for line in det_txt:\n",
    "#                 if len(line) == 56:\n",
    "#                     self.image_file_paths.append((line.split(\" \")[0] + \".JPEG\"))\n",
    "#                     self.annotations_file_paths.append((line.split(\" \")[0] + \".xml\"))\n",
    "        \n",
    "        \n",
    "        if self.effdet_data:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with Effecient Det Structure ... \\n\")\n",
    "        elif self.rcnn_big:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with bigger rcnn with ROI Structure ... \\n\")\n",
    "        else:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with mobilenet Faster R CNN Structure ... \\n\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path, xml_path = self.image_file_paths[idx], self.annotations_file_paths[idx]\n",
    "\n",
    "        img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        # img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        xml_doc = ElementTree.parse(xml_path)\n",
    "\n",
    "        bounding_boxes_nodes = xml_doc.findall(\"object/bndbox\")\n",
    "        labels_nodes = xml_doc.findall(\"object/name\")\n",
    "\n",
    "        bbox, labels = [], []\n",
    "\n",
    "        for node in bounding_boxes_nodes:\n",
    "            xmax = node.find(\"xmax\").text\n",
    "            xmin = node.find(\"xmin\").text\n",
    "            ymax = node.find(\"ymax\").text\n",
    "            ymin = node.find(\"ymin\").text\n",
    "            bbox.append([int(xmin), int(ymin), int(xmax), int(ymax)])  \n",
    "        bbox = torch.as_tensor(bbox, dtype = torch.float32)\n",
    "\n",
    "        for node in labels_nodes:\n",
    "            if self.effdet_data or self.rcnn_big:\n",
    "                label = self.labels_key.index(node.text)\n",
    "            else:\n",
    "                label = self.labels_key.index(node.text) + 1\n",
    "            labels.append(label)\n",
    "        labels = torch.as_tensor(labels, dtype = torch.int64)\n",
    "\n",
    "        # labels = tf.cast(labels, dtype = tf.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        try:\n",
    "            if self.transforms:  \n",
    "                sample = {\n",
    "                    'image': img,\n",
    "                    'bboxes': bbox,\n",
    "                    'labels': labels\n",
    "                      }\n",
    "\n",
    "                sample = self.transforms(**sample)\n",
    "                img = sample['image']\n",
    "                try:\n",
    "                    bbox = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                except:\n",
    "                    bbox = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        except:\n",
    "            print(\"Caught error. Now trying to instill transforms using Pytorch transforms\")\n",
    "            \n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = transforms.ToTensor()(img) \n",
    "                \n",
    "          # img = tf.cast(sample['image'], dtype = tf.float32) / 255.0\n",
    "          # bbox = tf.convert_to_tensor(torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0), dtype = tf.float32)\n",
    "\n",
    "        if self.effdet_data:\n",
    "            return {\"image\": img, \"bboxes\": bbox, \"category_id\": labels}\n",
    "        \n",
    "        target = dict()\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        return img, target  \n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.image_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wO5yLFymZOSZ",
    "outputId": "c4684fb1-e61e-452d-cad6-5c833149b1ec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Device: cuda\n",
      "Optimizer: SAM (\n",
      "Parameter Group 0\n",
      "    N_sma_threshhold: 5\n",
      "    alpha: 0.5\n",
      "    betas: (0.95, 0.999)\n",
      "    eps: 1e-05\n",
      "    initial_lr: 0.001\n",
      "    k: 6\n",
      "    lr: 0.001\n",
      "    rho: 0.05\n",
      "    step_counter: 0\n",
      "    weight_decay: 0.0001\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer/ranger/ranger.py:138: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370128159/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "/home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch Number: 28 | LR: 0.00100 | Train_loss: 94.32 | Valid_loss: 76.92 | Valid mAP: 1.16% | Valid Missed Images 75 / 88\n",
      "Epoch 1/1 | Batch Number: 56 | LR: 0.00100 | Train_loss: 27.41 | Valid_loss: 62.92 | Valid mAP: 3.97% | Valid Missed Images 76 / 88\n",
      "Epoch 1/1 | Batch Number: 84 | LR: 0.00100 | Train_loss: 25.47 | Valid_loss: 50.36 | Valid mAP: 11.10% | Valid Missed Images 57 / 88\n",
      "Epoch 1/1 | Batch Number: 112 | LR: 0.00100 | Train_loss: 18.71 | Valid_loss: 40.04 | Valid mAP: 7.95% | Valid Missed Images 69 / 88\n",
      "Epoch 1/1 | Batch Number: 140 | LR: 0.00100 | Train_loss: 21.55 | Valid_loss: 39.10 | Valid mAP: 0.32% | Valid Missed Images 66 / 88\n",
      "Epoch 1/1 | Batch Number: 168 | LR: 0.00100 | Train_loss: 17.03 | Valid_loss: 32.78 | Valid mAP: 3.97% | Valid Missed Images 77 / 88\n",
      "Epoch 1/1 | Batch Number: 196 | LR: 0.00100 | Train_loss: 18.85 | Valid_loss: 36.41 | Valid mAP: 24.06% | Valid Missed Images 32 / 88\n",
      "Epoch 1/1 | Batch Number: 224 | LR: 0.00100 | Train_loss: 14.55 | Valid_loss: 35.09 | Valid mAP: 2.53% | Valid Missed Images 78 / 88\n",
      "Epoch 1/1 | Batch Number: 252 | LR: 0.00100 | Train_loss: 15.28 | Valid_loss: 35.01 | Valid mAP: 1.14% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 280 | LR: 0.00100 | Train_loss: 12.37 | Valid_loss: 33.99 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 308 | LR: 0.00100 | Train_loss: 14.67 | Valid_loss: 32.56 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 336 | LR: 0.00100 | Train_loss: 13.46 | Valid_loss: 31.55 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 364 | LR: 0.00100 | Train_loss: 15.14 | Valid_loss: 34.07 | Valid mAP: 11.36% | Valid Missed Images 76 / 88\n",
      "Epoch 1/1 | Batch Number: 392 | LR: 0.00100 | Train_loss: 12.74 | Valid_loss: 32.65 | Valid mAP: 0.00% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 420 | LR: 0.00100 | Train_loss: 16.99 | Valid_loss: 35.44 | Valid mAP: 0.57% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 448 | LR: 0.00100 | Train_loss: 15.36 | Valid_loss: 31.74 | Valid mAP: 3.41% | Valid Missed Images 79 / 88\n",
      "Epoch 1/1 | Batch Number: 476 | LR: 0.00100 | Train_loss: 12.81 | Valid_loss: 32.34 | Valid mAP: 2.27% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 504 | LR: 0.00100 | Train_loss: 19.45 | Valid_loss: 43.06 | Valid mAP: 0.19% | Valid Missed Images 86 / 88\n",
      "Epoch 1/1 | Batch Number: 532 | LR: 0.00099 | Train_loss: 15.30 | Valid_loss: 33.95 | Valid mAP: 4.84% | Valid Missed Images 81 / 88\n",
      "Epoch 1/1 | Batch Number: 560 | LR: 0.00099 | Train_loss: 11.59 | Valid_loss: 36.38 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 588 | LR: 0.00099 | Train_loss: 16.96 | Valid_loss: 39.23 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 616 | LR: 0.00099 | Train_loss: 71.33 | Valid_loss: 50.94 | Valid mAP: 34.25% | Valid Missed Images 41 / 88\n",
      "Epoch 1/1 | Batch Number: 644 | LR: 0.00099 | Train_loss: 20.04 | Valid_loss: 50.85 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 672 | LR: 0.00099 | Train_loss: 46.87 | Valid_loss: 44.66 | Valid mAP: 1.14% | Valid Missed Images 86 / 88\n",
      "Epoch 1/1 | Batch Number: 700 | LR: 0.00099 | Train_loss: 22.35 | Valid_loss: 36.40 | Valid mAP: 2.27% | Valid Missed Images 86 / 88\n",
      "Epoch 1/1 | Batch Number: 728 | LR: 0.00099 | Train_loss: 13.44 | Valid_loss: 37.08 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 756 | LR: 0.00099 | Train_loss: 11.84 | Valid_loss: 37.66 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 784 | LR: 0.00099 | Train_loss: 14.93 | Valid_loss: 39.79 | Valid mAP: 0.57% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 812 | LR: 0.00099 | Train_loss: 13.54 | Valid_loss: 36.03 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 840 | LR: 0.00099 | Train_loss: 15.20 | Valid_loss: 34.60 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 868 | LR: 0.00099 | Train_loss: 12.57 | Valid_loss: 34.46 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 896 | LR: 0.00099 | Train_loss: 12.98 | Valid_loss: 39.05 | Valid mAP: 0.23% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 924 | LR: 0.00098 | Train_loss: 19.98 | Valid_loss: 40.42 | Valid mAP: 0.57% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 952 | LR: 0.00098 | Train_loss: 13.05 | Valid_loss: 41.11 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 980 | LR: 0.00098 | Train_loss: 16.54 | Valid_loss: 33.79 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c125db021ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr_cnn_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-158e2967f03d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch, lo_valid_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Check for degenerate boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 raise ValueError(\"images is expected to be a list of 3d tensors \"\n\u001b[1;32m    103\u001b[0m                                  \"of shape [C, H, W], got {}\".format(image.shape))\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r_cnn_trained = train(model, 1, train_loader, valid_loader, 0.001, weight_decay = 1e-4, print_times_per_epoch = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Device: cuda\n",
      "Optimizer: SAM (\n",
      "Parameter Group 0\n",
      "    N_sma_threshhold: 5\n",
      "    alpha: 0.5\n",
      "    betas: (0.95, 0.999)\n",
      "    eps: 1e-05\n",
      "    initial_lr: 0.0001\n",
      "    k: 6\n",
      "    lr: 0.0001\n",
      "    rho: 0.05\n",
      "    step_counter: 0\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer/ranger/ranger.py:138: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370128159/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "/home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch Number: 22447 | LR: 0.00010 | Train_loss: 112178093.03 | Valid_loss: 1046.19 | Valid mAP: 6.47% | Valid Missed Images 0 / 1761\n",
      "Epoch 1/1 | Batch Number: 44894 | LR: 0.00010 | Train_loss: 3822367.73 | Valid_loss: 1813196.61 | Valid mAP: 20.79% | Valid Missed Images 0 / 1761\n",
      "Epoch 1/1 | Batch Number: 67341 | LR: 0.00010 | Train_loss: 68994806.74 | Valid_loss: 666.07 | Valid mAP: 1.79% | Valid Missed Images 1709 / 1761\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected x_max for bbox (tensor(0.9219), tensor(0.3847), tensor(1.0063), tensor(0.5528), tensor(7)) to be in the range [0.0, 1.0], got 1.006250023841858.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b2c9aa3dc8d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmob_net_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmob_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-a4be57bd6387>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch, lo_valid_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_missed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e12314bbb96e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   }\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdual_start_end\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdual_start_end\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/core/utils.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_and_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_and_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/core/utils.py\u001b[0m in \u001b[0;36mcheck_and_convert\u001b[0;34m(self, data, rows, cols, direction)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_from_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_to_albumentations\u001b[0;34m(self, data, rows, cols)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_bboxes_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_bboxes_to_albumentations\u001b[0;34m(bboxes, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\n\u001b[1;32m    302\u001b[0m     \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert_bbox_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\n\u001b[1;32m    302\u001b[0m     \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert_bbox_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_bbox_to_albumentations\u001b[0;34m(bbox, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mcheck_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mcheck_bbox\u001b[0;34m(bbox)\u001b[0m\n\u001b[1;32m    328\u001b[0m             raise ValueError(\n\u001b[1;32m    329\u001b[0m                 \u001b[0;34m\"Expected {name} for bbox {bbox} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0;34m\"to be in the range [0.0, 1.0], got {value}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             )\n\u001b[1;32m    332\u001b[0m     \u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected x_max for bbox (tensor(0.9219), tensor(0.3847), tensor(1.0063), tensor(0.5528), tensor(7)) to be in the range [0.0, 1.0], got 1.006250023841858."
     ]
    }
   ],
   "source": [
    "mob_net_trained = train(mob_net, 1, train_loader, valid_loader, 0.0001, weight_decay = 1e-5, print_times_per_epoch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resizer(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=512):\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, annots = sample['img'], sample['annot']\n",
    "        height, width, _ = image.shape\n",
    "        if height > width:\n",
    "            scale = self.img_size / height\n",
    "            resized_height = self.img_size\n",
    "            resized_width = int(width * scale)\n",
    "        else:\n",
    "            scale = self.img_size / width\n",
    "            resized_height = int(height * scale)\n",
    "            resized_width = self.img_size\n",
    "\n",
    "        image = cv2.resize(image, (resized_width, resized_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        new_image = np.zeros((self.img_size, self.img_size, 3))\n",
    "        new_image[0:resized_height, 0:resized_width] = image\n",
    "\n",
    "        annots[:, :4] *= scale\n",
    "\n",
    "        return {'img': torch.from_numpy(new_image).to(torch.float32), 'annot': torch.from_numpy(annots), 'scale': scale}\n",
    "\n",
    "\n",
    "class Augmenter(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample, flip_x=0.5):\n",
    "        if np.random.rand() < flip_x:\n",
    "            image, annots = sample['img'], sample['annot']\n",
    "            image = image[:, ::-1, :]\n",
    "\n",
    "            rows, cols, channels = image.shape\n",
    "\n",
    "            x1 = annots[:, 0].copy()\n",
    "            x2 = annots[:, 2].copy()\n",
    "\n",
    "            x_tmp = x1.copy()\n",
    "\n",
    "            annots[:, 0] = cols - x2\n",
    "            annots[:, 2] = cols - x_tmp\n",
    "\n",
    "            sample = {'img': image, 'annot': annots}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "\n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        self.mean = np.array([[mean]])\n",
    "        self.std = np.array([[std]])\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, annots = sample['img'], sample['annot']\n",
    "\n",
    "        return {'img': ((image.astype(np.float32) - self.mean) / self.std), 'annot': annots}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ILSVRCVid2015VideoDetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "169081d5b0d94288a2b0ebb020b790d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1c3baca68a4e42afa680e581e92a7e82": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28ba486f593a4109bd152e94c871919d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c3baca68a4e42afa680e581e92a7e82",
      "max": 14212972,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_169081d5b0d94288a2b0ebb020b790d3",
      "value": 14212972
     }
    },
    "4839a59e79b6413bab12980f433870dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc398d009abc4ff7872a46b6e7bd497e",
      "placeholder": "",
      "style": "IPY_MODEL_cb9ef1bcc2334f7190b70c08a16090f7",
      "value": " 13.6M/13.6M [00:16&lt;00:00, 875kB/s]"
     }
    },
    "afba5dfb1b1a4811b1b0c02f03cb3c8d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbfa8ade6ff548aab08c392fe4bb20f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_28ba486f593a4109bd152e94c871919d",
       "IPY_MODEL_4839a59e79b6413bab12980f433870dc"
      ],
      "layout": "IPY_MODEL_afba5dfb1b1a4811b1b0c02f03cb3c8d"
     }
    },
    "cb9ef1bcc2334f7190b70c08a16090f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc398d009abc4ff7872a46b6e7bd497e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
