{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Jvvfb1riBioe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch \n",
    "from torch import nn, optim \n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.utils.data\n",
    "import time\n",
    "import itertools\n",
    "import glob \n",
    "from PIL import Image\n",
    "import csv \n",
    "import cv2\n",
    "import re\n",
    "import torchvision\n",
    "import random\n",
    "from xml.etree import ElementTree\n",
    "from torchvision.ops.boxes import box_iou\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sarthak/DataSets/ILSVRC2015/EfficientDet.Pytorch-Updated\n",
      "/home/sarthak/DataSets/ILSVRC2015\n"
     ]
    }
   ],
   "source": [
    "#Packages commonly needed to install pycocotools, tqdm, and requests.\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "\n",
    "%cd EfficientDet.Pytorch-Updated/\n",
    "import math\n",
    "from models.efficientnet import EfficientNet\n",
    "from models.bifpn import BIFPN\n",
    "from models.retinahead import RetinaHead\n",
    "from models.module import RegressionModel, ClassificationModel, Anchors, ClipBoxes, BBoxTransform\n",
    "from torchvision.ops import nms\n",
    "from models.losses import FocalLoss\n",
    "from models.efficientdet import EfficientDet\n",
    "from models.losses import FocalLoss\n",
    "from datasets import VOCDetection, CocoDataset, get_augumentation, detection_collate, Resizer, Normalizer, Augmenter, collater\n",
    "from utils import EFFICIENTDET, get_state_dict\n",
    "from eval import evaluate, evaluate_coco\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Ranger-Deep-Learning-Optimizer' already exists and is not an empty directory.\n",
      "/home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer\n",
      "Obtaining file:///home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer\n",
      "Requirement already satisfied: torch in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from ranger==0.1.dev0) (1.8.1)\n",
      "Requirement already satisfied: numpy in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torch->ranger==0.1.dev0) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torch->ranger==0.1.dev0) (3.7.4.3)\n",
      "Installing collected packages: ranger\n",
      "  Attempting uninstall: ranger\n",
      "    Found existing installation: ranger 0.1.dev0\n",
      "    Uninstalling ranger-0.1.dev0:\n",
      "      Successfully uninstalled ranger-0.1.dev0\n",
      "  Running setup.py develop for ranger\n",
      "Successfully installed ranger\n",
      "/home/sarthak/DataSets/ILSVRC2015\n",
      "fatal: destination path 'sam' already exists and is not an empty directory.\n",
      "/home/sarthak/DataSets/ILSVRC2015/sam\n",
      "Imported SAM Successfully from github .py file\n",
      "/home/sarthak/DataSets/ILSVRC2015\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "%cd Ranger-Deep-Learning-Optimizer\n",
    "!pip install -e .\n",
    "from ranger import Ranger  \n",
    "%cd ..\n",
    "#https://paperswithcode.com/paper/sharpness-aware-minimization-for-efficiently-1\n",
    "!git clone https://github.com/davda54/sam.git\n",
    "%cd sam\n",
    "import sam\n",
    "print(\"Imported SAM Successfully from github .py file\")\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPKm4C39fzGD"
   },
   "source": [
    "## To DO \n",
    "\n",
    "### Configure fast rcnn model for training overnight\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Fix the metric creating of Effecient Det Model. \n",
    "\n",
    "* Push learning rate higher (current lr) * 10\n",
    "\n",
    "Way to make Effecient Det work: \n",
    "* Turn the model to train mode\n",
    "* Put some annotations and an image into the model and run it, which will get the output that will enter run metrics batch\n",
    "* Then try running model on that input with bigger batch sizes. \n",
    "* Figure out how to get the data for one image in calculate metrics \n",
    "* Generalize to see how a batch of predictions can be run to the metrics\n",
    "* Make sure mAP and missed images are the only things coming in and out.\n",
    "* Put code into function and run it.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Goal:\n",
    "\n",
    "Get acceptable resukts with a rcnn torchvision model\n",
    "Get acceptable results with effecient det\n",
    "\n",
    "### Some Errors I get\n",
    "\n",
    "Value Error in Det dataset\n",
    "Model sometimes gets error while running on data.\n",
    "\n",
    "### Advice\n",
    "\n",
    "Try to start with simplest (Adam basics) and keep on adding stuff to improve mAP\n",
    " How much data 57 k in Det dataset\n",
    " Full validation has 170,00 but use \n",
    " \n",
    " * (2,000 images in valid loader).\n",
    "* 10 - 15 images per folder.\n",
    "\n",
    "Great Proportions is that around 35 - 40% should be the Det Dataset and 60 - 65 % should be the sampled VID Dataset. \n",
    "\n",
    "### Get better results\n",
    "\n",
    "* Reduce dataset to 5000 train images and 500 valid images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: torch==1.8.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: typing-extensions in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 701
    },
    "collapsed": true,
    "id": "G9vLpqYsehKW",
    "outputId": "5ae89189-d6ba-4cdd-d080-4a77f7c5a410"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: PyYAML in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (5.4.1)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (0.18.1)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (0.4.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: scipy in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from albumentations) (1.19.2)\n",
      "Requirement already satisfied: Shapely in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
      "Requirement already satisfied: opencv-python in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (4.5.1.48)\n",
      "Requirement already satisfied: six in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
      "Requirement already satisfied: matplotlib in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (3.3.4)\n",
      "Requirement already satisfied: imageio in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (2.9.0)\n",
      "Requirement already satisfied: Pillow in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (8.2.0)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.5.1)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.4.8)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install --upgrade albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "juEv3bvw0yYp"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uXvczm7BMhQw"
   },
   "outputs": [],
   "source": [
    "def get_class_info(get_keys = False, smaller_mb_net = False):\n",
    "    obj_dict = {\n",
    "    \"n02691156\": \"airplane\", \n",
    "    \"n02419796\": \"antelope\", \n",
    "    \"n02131653\": \"bear\", \n",
    "    \"n02834778\": \"bicycle\", \n",
    "    \"n01503061\": \"bird\", \n",
    "    \"n02924116\": \"bus\", \n",
    "    \"n02958343\": \"car\", \n",
    "    \"n02402425\": \"cattle\", \n",
    "    \"n02084071\": \"dog\", \n",
    "    \"n02121808\": \"domestic_cat\", \n",
    "    \"n02503517\": \"elephant\", \n",
    "    \"n02118333\": \"fox\",\n",
    "    \"n02510455\": \"giant_panda\", \n",
    "    \"n02342885\": \"hamster\", \n",
    "    \"n02374451\": \"horse\", \n",
    "    \"n02129165\": \"lion\", \n",
    "    \"n01674464\": \"lizard\", \n",
    "    \"n02484322\": \"monkey\", \n",
    "    \"n03790512\": \"motorcycle\", \n",
    "    \"n02324045\": \"rabbit\",\n",
    "    \"n02509815\": \"red_panda\", \n",
    "    \"n02411705\": \"sheep\", \n",
    "    \"n01726692\": \"snake\", \n",
    "    \"n02355227\": \"squirrel\", \n",
    "    \"n02129604\": \"tiger\", \n",
    "    \"n04468005\": \"train\", \n",
    "    \"n01662784\": \"turtle\", \n",
    "    \"n04530566\": \"watercraft\", \n",
    "    \"n02062744\": \"whale\",\n",
    "    \"n02391049\": \"zebra\"\n",
    "    }\n",
    "    \n",
    "    if get_keys:\n",
    "        return list(obj_dict.keys())\n",
    "    \n",
    "    return obj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFrameDataset():\n",
    "    \n",
    "    def __init__(self, mode, vid_data_root_dir, vid_annotations_root_dir, transforms, seg_len = None, \n",
    "               make_valid_smaller_percent = None, effdet_data = None, rcnn_big = None, det_text_file_paths = None, \n",
    "               data_size = None):\n",
    "                 \n",
    "        \n",
    "    # If valid smaller is true cut the length of valid list to certain length with if statement. \n",
    "        if (mode == \"train\"):\n",
    "            if (seg_len):\n",
    "                ORIG_SEG_LEN = seg_len\n",
    "                #Subset images from every scene there are 3862 data folders.\n",
    "                print(\"There are 1122397 train images in total\")\n",
    "                globbed_image_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_data_root_dir)))\n",
    "                globbed_annotations_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_annotations_root_dir)))\n",
    "                \n",
    "                self.image_file_paths, self.annotations_file_paths = list(), list()\n",
    "                \n",
    "                for folder in list(zip(globbed_image_file_paths, globbed_annotations_file_paths)):\n",
    "                    scene_images = sorted(glob.glob(\"{}/*.JPEG\".format(folder[0])))\n",
    "                    scene_annotations = sorted(glob.glob(\"{}/*.xml\".format(folder[1])))\n",
    "                    \n",
    "                    image_annot = list(zip(scene_images, scene_annotations))\n",
    "\n",
    "                    if seg_len % 5 != 0:\n",
    "                        raise ValueError(\"Not allowed value for seg_len must be divisible by 5\")\n",
    "                    if seg_len >= len(image_annot):\n",
    "                        seg_len = 5\n",
    "                    \n",
    "                    if len(image_annot) % seg_len != 0:\n",
    "                        image_annot = image_annot[:-(len(image_annot) % seg_len)]\n",
    "                    \n",
    "                    red_img_annot, start_index = list(), 0 \n",
    "                    \n",
    "                    for window in range(int(len(image_annot) / seg_len)):\n",
    "                        end_index = start_index + seg_len\n",
    "                        red_img_annot.append(random.sample(image_annot[start_index : end_index], 1)[0])\n",
    "                        start_index = end_index\n",
    "                    \n",
    "                   \n",
    "                    scene_images, scene_annotations = zip(*red_img_annot) \n",
    "                    \n",
    "                    self.annotations_file_paths.extend(scene_annotations)\n",
    "                    self.image_file_paths.extend(scene_images)\n",
    "                    \n",
    "                    seg_len = ORIG_SEG_LEN\n",
    "                    \n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "            \n",
    "        elif (mode == \"validation\"):\n",
    "            if make_valid_smaller_percent:\n",
    "                #Subset a percent of the valid data\n",
    "                valid_image_list = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir)))\n",
    "                valid_annotations_list = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "\n",
    "                subset = int(len(valid_image_list) * make_valid_smaller_percent)\n",
    "\n",
    "                #Shuffle both lists at once with same order\n",
    "                mapIndexPosition = list(zip(valid_image_list, valid_annotations_list))\n",
    "                random.shuffle(mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = zip(*mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = list(valid_image_list), list(valid_annotations_list)\n",
    "\n",
    "                self.image_file_paths = valid_image_list[:subset]\n",
    "                self.annotations_file_paths = valid_annotations_list[:subset]\n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "        else:\n",
    "            raise ValueError(\"Choose mode between train or validation only\")\n",
    "\n",
    "        self.labels_key = get_class_info(get_keys = True)\n",
    "\n",
    "        self.data_size = data_size\n",
    "        self.transforms = transforms\n",
    "        self.effdet_data = effdet_data\n",
    "        self.rcnn_big = rcnn_big\n",
    "        \n",
    "        if det_text_file_paths:\n",
    "            home_file_path_data = \"/data1/group/mlgroup/train_data/ILSVRC2015/Data/DET/\"\n",
    "            home_file_path_annot = \"/data1/group/mlgroup/train_data/ILSVRC2015/Annotations/DET/\"\n",
    "            \n",
    "            assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "            print(\"\\n\")\n",
    "            print(\"BEFORE DET: Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "            print(\"BEFORE DET: Amount of annotation files in Dataset {} \\n\".format(len(self.annotations_file_paths)))\n",
    "            det_txt = open(det_text_file_paths, \"r\").readlines()\n",
    "            np.random.shuffle(det_txt)\n",
    "            \n",
    "            #I am gonna sample about 10k images or around 20% of Det Dataset\n",
    "            det_txt = det_txt[:int(len(det_txt) * 0.6)]\n",
    "            print(\"Amount of images in Det Set (Approx.) {}\".format(len(det_txt)))\n",
    "            \n",
    "            for line in det_txt:\n",
    "                self.image_file_paths.append((home_file_path_data + line.split(\" \")[0] + \".JPEG\"))\n",
    "                self.annotations_file_paths.append((home_file_path_annot + line.split(\" \")[0] + \".xml\"))\n",
    "        \n",
    "        #Final sort to keep annotations and image file paths in same config\n",
    "        self.image_file_paths, self.annotations_file_paths = sorted(self.image_file_paths), sorted(self.annotations_file_paths)\n",
    "        \n",
    "        assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "        print(\"Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "        print(\"Amount of annotation files in Dataset {}\".format(len(self.annotations_file_paths)))\n",
    "        \n",
    "        \n",
    "        if self.effdet_data:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with Effecient Det Structure ... \\n\")\n",
    "        elif self.rcnn_big:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with bigger rcnn with ROI Structure ... \\n\")\n",
    "        else:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with mobilenet Faster R CNN Structure ... \\n\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path, xml_path = self.image_file_paths[idx], self.annotations_file_paths[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        # img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        marking = False\n",
    "        xml_doc = ElementTree.parse(xml_path)\n",
    "\n",
    "        bounding_boxes_nodes = xml_doc.findall(\"object/bndbox\")\n",
    "        labels_nodes = xml_doc.findall(\"object/name\")\n",
    "\n",
    "        bbox, labels = [], []\n",
    "\n",
    "        for node in bounding_boxes_nodes:\n",
    "            xmax = node.find(\"xmax\").text\n",
    "            xmin = node.find(\"xmin\").text\n",
    "            ymax = node.find(\"ymax\").text\n",
    "            ymin = node.find(\"ymin\").text\n",
    "            bbox.append([int(xmin), int(ymin), int(xmax), int(ymax)])  \n",
    "            \n",
    "\n",
    "        for node in labels_nodes:\n",
    "            if node.text in self.labels_key:\n",
    "                label = self.labels_key.index(node.text)    \n",
    "            else:\n",
    "                label = \"DNE\"\n",
    "                marking = True\n",
    "            labels.append(label)\n",
    "        \n",
    "        if (marking):\n",
    "            removed_indices = list()\n",
    "            \n",
    "            for ii in range(len(labels)):\n",
    "                if (labels[ii] == \"DNE\"):\n",
    "                    removed_indices.append(ii)\n",
    "            labels = [i for j, i in enumerate(labels) if j not in removed_indices]\n",
    "            bbox = [i for j, i in enumerate(bbox) if j not in removed_indices]\n",
    "                \n",
    "        if not(self.effdet_data or self.rcnn_big):\n",
    "            #Need to add one to labels\n",
    "            labels = [label + 1 for label in labels]\n",
    "        \n",
    "        bbox = torch.as_tensor(bbox, dtype = torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype = torch.int64)\n",
    "\n",
    "        # labels = tf.cast(labels, dtype = tf.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        try:\n",
    "            if self.transforms:  \n",
    "                sample = {\n",
    "                    'image': img,\n",
    "                    'bboxes': bbox,\n",
    "                    'labels': labels\n",
    "                      }\n",
    "\n",
    "                sample = self.transforms(**sample)\n",
    "                img = sample['image']\n",
    "                try:\n",
    "                    bbox = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                except:\n",
    "                    bbox = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                    \n",
    "        except:\n",
    "            print(\"Caught error. Now trying to instill transforms using Pytorch transforms\")\n",
    "            \n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            emergency_transforms = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "            \n",
    "            \n",
    "            img = emergency_transforms(img)\n",
    "                \n",
    "                \n",
    "          # img = tf.cast(sample['image'], dtype = tf.float32) / 255.0\n",
    "          # bbox = tf.convert_to_tensor(torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0), dtype = tf.float32)\n",
    "\n",
    "        if self.effdet_data:\n",
    "            return {\"image\": img, \"bboxes\": bbox, \"category_id\": labels}\n",
    "        \n",
    "        target = dict()\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        return img, target  \n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.data_size:\n",
    "            return self.data_size\n",
    "        return len(self.image_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change image size and try and except in dataclass before transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(mode):\n",
    "    if (mode == \"train\"):\n",
    "        return A.Compose([\n",
    "                          A.Resize(512, 512), \n",
    "                          A.OneOf([\n",
    "                          A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                         val_shift_limit=0.2, p=0.9),\n",
    "                          A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                               contrast_limit=0.2, p=0.9)],p=0.9),\n",
    "                          A.Cutout(num_holes=8, max_h_size=8, max_w_size=8, p=0.5),\n",
    "                          A.HorizontalFlip(),\n",
    "                          A.VerticalFlip(), \n",
    "                          ToTensorV2()\n",
    "                          ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    elif (mode == \"test\"):\n",
    "        return A.Compose([\n",
    "                          A.Resize(512, 512), \n",
    "                          ToTensorV2()\n",
    "                          ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    elif (mode == \"effdet_train\"):\n",
    "        return A.Compose([\n",
    "                          A.OneOf([\n",
    "                          A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                         val_shift_limit=0.2, p=0.9),\n",
    "                          A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                               contrast_limit=0.2, p=0.9)],p=0.9),\n",
    "                          A.Cutout(num_holes=8, max_h_size=4, max_w_size=4, p=0.5),\n",
    "                          A.HorizontalFlip(),\n",
    "                          A.VerticalFlip(), \n",
    "                          A.Resize(height = 512, width=512), \n",
    "                          ToTensorV2()\n",
    "                          ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "    elif (mode == \"effdet_test\"):\n",
    "        return A.Compose([\n",
    "                          A.Resize(height = 512, width = 512), \n",
    "                          ToTensorV2()])\n",
    "    else:\n",
    "        raise ValueError(\"mode is wrong value can either be train or test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "IwHLAJZQjrt6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1122397 train images in total\n",
      "\n",
      "\n",
      "BEFORE DET: Amount of image files in Dataset 54647\n",
      "BEFORE DET: Amount of annotation files in Dataset 54647 \n",
      "\n",
      "Amount of images in Det Set (Approx.) 32183\n",
      "Amount of image files in Dataset 86830\n",
      "Amount of annotation files in Dataset 86830\n",
      "\n",
      "\n",
      "Loading with mobilenet Faster R CNN Structure ... \n",
      "\n",
      "Amount of image files in Dataset 2201\n",
      "Amount of annotation files in Dataset 2201\n",
      "\n",
      "\n",
      "Loading with mobilenet Faster R CNN Structure ... \n",
      "\n",
      " \n",
      " ... Seperate from Data Loader \n",
      "\n",
      "Length of train_dataset 50\n",
      "Length of valid_dataset 10\n"
     ]
    }
   ],
   "source": [
    "# 1122397 Files in train set total\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "#The amount of scenes to load in one go. # 2 and 2 are the best values\n",
    "train_batch_size = 2\n",
    "valid_batch_size = 2\n",
    "det_text_file = \"/data1/group/mlgroup/train_data/ILSVRC2015/DET_train_30classes.txt\"\n",
    "\n",
    "\n",
    "train_dataset = VideoFrameDataset(\"train\", os.path.join(\"Data/VID\", \"train\"), os.path.join(\"Annotations/VID\", \"train\"), get_transforms(mode = \"train\"), \n",
    "                                  seg_len = 20, det_text_file_paths = det_text_file, data_size = 50)\n",
    "valid_dataset = VideoFrameDataset(\"validation\", os.path.join(\"Data/VID\", \"val\"), os.path.join(\"Annotations/VID\", \"val\"), get_transforms(mode = \"test\"),\n",
    "                                  make_valid_smaller_percent = 0.0125, data_size = 10)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= collate_fn)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = valid_batch_size, shuffle = True, collate_fn = collate_fn)\n",
    "\n",
    "print(\" \\n ... Seperate from Data Loader \\n\")\n",
    "print(\"Length of train_dataset {}\".format(len(train_dataset)))\n",
    "print(\"Length of valid_dataset {}\".format(len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "3iCwxnxIcRsr"
   },
   "outputs": [],
   "source": [
    "COLORS = [(0, 0, 0), (0, 255, 0), (0, 0 , 255), (255, 255, 0), (255, 0, 0)]\n",
    "\n",
    "def draw_boxes(boxes, labels, image, infer = False, put_text = True):\n",
    "    classes = get_class_info()\n",
    "    keys = list(classes.keys())\n",
    "\n",
    "    # read the image with OpenCV\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    if infer:\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    for i, box in enumerate(boxes):\n",
    "        color = COLORS[1]\n",
    "        cv2.rectangle(\n",
    "            image,\n",
    "            (int(box[0]), int(box[1])),\n",
    "            (int(box[2]), int(box[3])),\n",
    "            color, 5\n",
    "        )\n",
    "        if put_text:\n",
    "          cv2.putText(image, classes[keys[labels[i] - 1]], (int(box[0]), int(box[1]-5)),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 2, color, 3, \n",
    "                      lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBIf0ODU0Aod"
   },
   "source": [
    "### Create a draw function to visualize some data (Will give index error if batch size < 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 569
    },
    "id": "YMd58kVzZd2L",
    "outputId": "3c70eca3-8a99-4b35-8df9-99b0b895efc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/ipykernel_launcher.py:8: MatplotlibDeprecationWarning: Passing non-integers as three-element position specification is deprecated since 3.3 and will be removed two minor releases later.\n",
      "  \n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6c2b8dd2ea90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1800x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    image = draw_boxes(labels[idx][\"boxes\"], labels[idx][\"labels\"], images[idx], put_text = True)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "5ytYP752Zd7S"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(target_box,predictions_box,scores, device):\n",
    "\n",
    "    #Get most confident boxes first and least confident last\n",
    "    predictions_box = predictions_box[scores.argsort().flip(-1)]\n",
    "    iou_mat = box_iou(target_box,predictions_box)\n",
    "    #return a one by one matrix that is form (target_box, prediction_box) or (1, 1)\n",
    "    target_boxes_count, prediction_boxes_count = iou_mat.shape\n",
    "    \n",
    "    mAP_Matrix = torch.zeros_like(iou_mat)\n",
    "    # if not matrix coordinates that relate to nothing.\n",
    "    if not iou_mat[:,0].eq(0.).all():\n",
    "      index_of_biggest_iou = iou_mat[:,0].argsort()[-1]\n",
    "      mAP_Matrix[index_of_biggest_iou,0] = 1\n",
    "\n",
    "    for pr_idx in range(1,prediction_boxes_count):\n",
    "        not_assigned = torch.logical_not(mAP_Matrix[:,:pr_idx].sum(1)).long()\n",
    "        targets = not_assigned * iou_mat[:,pr_idx]\n",
    "\n",
    "        if targets.eq(0).all():\n",
    "            continue\n",
    "\n",
    "        pivot = targets.argsort()[-1]\n",
    "        mAP_Matrix[pivot,pr_idx] = 1\n",
    "\n",
    "    # mAP calculation\n",
    "    tp = mAP_Matrix.sum()\n",
    "    fp = mAP_Matrix.sum(0).eq(0).sum()\n",
    "    fn = mAP_Matrix.sum(1).eq(0).sum()\n",
    "\n",
    "    mAP = tp / (tp+fp)\n",
    "\n",
    "    return mAP\n",
    "\n",
    "def run_metrics_for_batch(output, targets, mAP, missed_images, device):\n",
    "  for pos_in_batch, image_pred in enumerate(output):\n",
    "    assert (len(image_pred[\"boxes\"]) == len(image_pred[\"labels\"]) == len(image_pred[\"scores\"]))\n",
    "    if len(image_pred[\"boxes\"]) != 0:\n",
    "      mAP += calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
    "    else:\n",
    "      missed_images += 1\n",
    "  \n",
    "  return mAP, missed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "IN1fBHzJZd92"
   },
   "outputs": [],
   "source": [
    "def train(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch,\n",
    "          lo_valid_dataset = len(valid_dataset), lo_train_dataset = len(train_dataset), saving_directory = None,\n",
    "          unique_char_for_saving = None):\n",
    "\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print_every = len(train_dataset) / train_batch_size // print_times_per_epoch\n",
    "    print(\"Print Every: {}\".format(print_every))\n",
    "\n",
    "    #Check which parameters can calculate gradients. \n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "\n",
    "    base_optimizer = Ranger\n",
    "    optimizer = sam.SAM(net.parameters(), base_optimizer, lr = lr, weight_decay = weight_decay)\n",
    "#     optimizer = optim.Adam(params, lr = lr, weight_decay = weight_decay)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
    "\n",
    "    #Might be some problems with the Data Parallel code\n",
    "#     if torch.cuda.device_count() > 1:\n",
    "#         print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "#         net = Some Distrubuted Parallel Function\n",
    "    net.to(device)\n",
    "    \n",
    "    print(\"Device: {}\".format(device))\n",
    "    print(\"Optimizer: {}\".format(optimizer))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_time = time.time()\n",
    "        net.train()\n",
    "        \n",
    "        train_loss = train_mAP = steps = train_missed_images = 0\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "            \n",
    "            \n",
    "            net.train()\n",
    "            steps += 1\n",
    "\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss_dict = net(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            net.eval()\n",
    "            try:\n",
    "                train_mAP, train_missed_images = run_metrics_for_batch(net(images), targets, train_mAP, train_missed_images, device)\n",
    "            except:\n",
    "                print(images[0].size(), targets)\n",
    "                print(\"Caught an exception in an image could not predict metric for it\")\n",
    "            net.train()\n",
    "\n",
    "            losses.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "            optimizer.first_step(zero_grad = True)\n",
    "\n",
    "            loss_dict = net(images, targets)\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            losses.backward()\n",
    "            optimizer.second_step(zero_grad = True)\n",
    "\n",
    "            train_loss +=  losses.item()\n",
    "            scheduler.step()\n",
    "\n",
    "            if (steps % print_every) == 0:  \n",
    "                with torch.no_grad():\n",
    "                \n",
    "                    valid_mAP = valid_loss = valid_missed_images = 0\n",
    "\n",
    "                    for images, targets in valid_loader:\n",
    "                        net.eval()\n",
    "                        if device == torch.device(\"cuda\"):\n",
    "                            images = [image.to(device) for image in images]\n",
    "                            targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
    "                        \n",
    "                        try:\n",
    "                            output = net(images)\n",
    "                            valid_mAP, valid_missed_images = run_metrics_for_batch(output, targets, valid_mAP, valid_missed_images, device)\n",
    "                        except:\n",
    "                            print(targets, images[0].size())\n",
    "                            print(\"Caught exception with running metrics for one valid image (skipped)\")\n",
    "\n",
    "                        net.train()\n",
    "                        valid_loss_dict = net(images, targets)\n",
    "                        valid_losses = sum(loss for loss in valid_loss_dict.values())\n",
    "                        valid_loss += valid_losses.item()\n",
    "\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        learning_rate_extract = param_group[\"lr\"]\n",
    "                    print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Valid_loss: {:0.2f} | Valid mAP: {:0.2f}% | Valid Missed Images {} / {}\".format(\n",
    "                        epoch + 1, epochs, steps, learning_rate_extract, train_loss, valid_loss,  \n",
    "                        (valid_mAP / float(lo_valid_dataset)) * 100., valid_missed_images, lo_valid_dataset))\n",
    "\n",
    "                assert (steps % print_every) == 0\n",
    "                train_loss = 0\n",
    "                \n",
    "        print(\"\\n Epoch {} | Epoch Time {:0.2f} | Final Train mAP: {:0.2f}% | Final Train Missed Images {} / {} \\n\".format(\n",
    "            epoch + 1, (time.time() - epoch_time),(train_mAP / float(lo_train_dataset)) * 100., train_missed_images, lo_train_dataset\n",
    "        ))\n",
    "        if saving_directory:\n",
    "            if os.path.isdir(saving_directory):\n",
    "                print(\"Saving Model path to directory {} ... \".format(saving_directory))\n",
    "                saving_path = os.path.join(saving_directory, \"Epoch\" + str(epoch + 1) + str(unique_char_for_saving) + \".pth\")\n",
    "                saving_content = {\"model_state_dict\": net.state_dict(), \n",
    "                                  \"optimizer_state_dict\": optimizer.state_dict(), \n",
    "                                  \"epoch\": epoch + 1, \n",
    "                                  \"model_type\": \"FasterRCNNResnet50FPN\"}\n",
    "                torch.save(saving_content, saving_path)\n",
    "                print(\"Succesfully saved model data to file path. \\n\")\n",
    "            else:\n",
    "                print(\"Directory Provided does not exist, hence will skip saving the model\")\n",
    "    \n",
    "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))\n",
    "    \n",
    "        # Example File path: /saved_models/epoch10small.pth \n",
    "        \n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mish helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU to Mish conversion for models \n",
    "#Option to switch any activation function for another.\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        print(\"Mish activation loaded...\")\n",
    "\n",
    "    def forward(self, x): \n",
    "        \n",
    "        x = x *( torch.tanh(F.softplus(x)))\n",
    "\n",
    "        return x\n",
    "    \n",
    "def convert_it(model, new, replaced_act):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, replaced_act):\n",
    "            setattr(model, child_name, new)\n",
    "        else:\n",
    "            convert_it(child, new, replaced_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UuNTqGHFaAyU"
   },
   "source": [
    "## Faster R CNN with mobile net backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "z-R_qJ3haFf-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mish activation loaded...\n"
     ]
    }
   ],
   "source": [
    "mob_net = torchvision.models.detection.faster_rcnn.fasterrcnn_resnet50_fpn(pretrained = True)\n",
    "# mob_net = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
    "mob_net.roi_heads.box_predictor.cls_score.out_features = len(get_class_info())\n",
    "mob_net.roi_heads.box_predictor.bbox_pred.out_features = len(get_class_info()) * 4\n",
    "convert_it(mob_net, Mish(), nn.ReLU6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing LR to 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print Every: 12.0\n",
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Device: cuda\n",
      "Optimizer: SAM (\n",
      "Parameter Group 0\n",
      "    N_sma_threshhold: 5\n",
      "    alpha: 0.5\n",
      "    betas: (0.95, 0.999)\n",
      "    eps: 1e-05\n",
      "    initial_lr: 0.0005\n",
      "    k: 6\n",
      "    lr: 0.0005\n",
      "    rho: 0.05\n",
      "    step_counter: 0\n",
      "    weight_decay: 1e-05\n",
      ")\n",
      "Epoch 1/3 | Batch Number: 12 | LR: 0.00047 | Train_loss: 12.92 | Valid_loss: 1.37 | Valid mAP: 86.67% | Valid Missed Images 0 / 10\n",
      "Epoch 1/3 | Batch Number: 24 | LR: 0.00038 | Train_loss: 5.23 | Valid_loss: 1.09 | Valid mAP: 45.83% | Valid Missed Images 0 / 10\n",
      "\n",
      " Epoch 1 | Epoch Time 52.28 | Final Train mAP: 11.18% | Final Train Missed Images 0 / 50 \n",
      "\n",
      "Saving Model path to directory Faster_rcnn_Saved_Models ... \n",
      "Succesfully saved model data to file path. \n",
      "\n",
      "Epoch 2/3 | Batch Number: 12 | LR: 0.00026 | Train_loss: 3.69 | Valid_loss: 1.15 | Valid mAP: 50.83% | Valid Missed Images 0 / 10\n",
      "Epoch 2/3 | Batch Number: 24 | LR: 0.00013 | Train_loss: 3.73 | Valid_loss: 1.27 | Valid mAP: 75.00% | Valid Missed Images 0 / 10\n",
      "\n",
      " Epoch 2 | Epoch Time 52.71 | Final Train mAP: 22.87% | Final Train Missed Images 0 / 50 \n",
      "\n",
      "Saving Model path to directory Faster_rcnn_Saved_Models ... \n",
      "Succesfully saved model data to file path. \n",
      "\n",
      "Epoch 3/3 | Batch Number: 12 | LR: 0.00004 | Train_loss: 2.95 | Valid_loss: 1.28 | Valid mAP: 73.33% | Valid Missed Images 0 / 10\n",
      "Epoch 3/3 | Batch Number: 24 | LR: 0.00000 | Train_loss: 3.56 | Valid_loss: 1.27 | Valid mAP: 73.33% | Valid Missed Images 0 / 10\n",
      "\n",
      " Epoch 3 | Epoch Time 53.19 | Final Train mAP: 27.26% | Final Train Missed Images 0 / 50 \n",
      "\n",
      "Saving Model path to directory Faster_rcnn_Saved_Models ... \n",
      "Succesfully saved model data to file path. \n",
      "\n",
      "Time for Total Training 160.14\n"
     ]
    }
   ],
   "source": [
    "mob_net_trained = train(mob_net, 3, train_loader, valid_loader, 0.0005, weight_decay = 1e-5, print_times_per_epoch = 2,\n",
    "                        saving_directory = \"Faster_rcnn_Saved_Models\", unique_char_for_saving = \"FastRCNNResNetV1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model function. \n",
    "* Only will run if you have .pth file that configures nicely with Faster RCNN Resnet50FPN\n",
    "* If model crashes while training, then might have to load in optimizer.\n",
    "\n",
    "#### Example Code of Succesful save: \n",
    "`\n",
    "checkpoint_file = \"Faster_rcnn_Saved_Models/Epoch3FastRCNNResNetV1.pth\"\n",
    "new_mob_net = load_model_fast_rcnn(checkpoint_file) `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_fast_rcnn(checkpoint_file, mished = True):\n",
    "    \n",
    "    #Get model configuration\n",
    "    model = torchvision.models.detection.faster_rcnn.fasterrcnn_resnet50_fpn(pretrained = False)\n",
    "    model.roi_heads.box_predictor.cls_score.out_features = len(get_class_info())\n",
    "    model.roi_heads.box_predictor.bbox_pred.out_features = len(get_class_info()) * 4\n",
    "    \n",
    "    if mished:\n",
    "        convert_it(mob_net, Mish(), nn.ReLU6)\n",
    "    \n",
    "    #Load in state dicts \n",
    "    if os.path.isfile(checkpoint_file):\n",
    "        checkpoint = torch.load(checkpoint_file)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])   \n",
    "    else:\n",
    "        raise ValueError(\"Checkpoint File does not exist\")\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2: Effecient Det Model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1122397 train images in total\n",
      "Amount of image files in Dataset 54647\n",
      "Amount of annotation files in Dataset 54647\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n",
      "Amount of image files in Dataset 2201\n",
      "Amount of annotation files in Dataset 2201\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n",
      " \n",
      " ... Seperate from Data Loader \n",
      "\n",
      "Length of train_dataset 500\n",
      "Length of valid_dataset 100\n"
     ]
    }
   ],
   "source": [
    "def detection_collate(batch):\n",
    "    imgs = [s['image'] for s in batch]\n",
    "    annots = [s['bboxes'] for s in batch]\n",
    "    labels = [s['category_id'] for s in batch]\n",
    "\n",
    "    max_num_annots = max(len(annot) for annot in annots)\n",
    "    annot_padded = np.ones((len(annots), max_num_annots, 5))*-1\n",
    "\n",
    "    if max_num_annots > 0:\n",
    "        for idx, (annot, lab) in enumerate(zip(annots, labels)):\n",
    "            if len(annot) > 0:\n",
    "                annot_padded[idx, :len(annot), :4] = annot\n",
    "                annot_padded[idx, :len(annot), 4] = lab\n",
    "    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded))\n",
    "\n",
    "eff_train_batch = 1\n",
    "eff_test_batch = 1\n",
    "\n",
    "eff_train_dataset = VideoFrameDataset(\"train\", os.path.join(\"Data/VID\", \"train\"), os.path.join(\"Annotations/VID\", \"train\"), get_transforms(mode = \"effdet_train\"), \n",
    "                                  seg_len = 20, det_text_file_paths = None, effdet_data = True, data_size = 500)\n",
    "eff_valid_dataset = VideoFrameDataset(\"validation\", os.path.join(\"Data/VID\", \"val\"), os.path.join(\"Annotations/VID\", \"val\"), get_transforms(mode = \"effdet_test\"),\n",
    "                                  make_valid_smaller_percent = 0.0125, effdet_data = True, data_size = 100)\n",
    "\n",
    "eff_train_loader = torch.utils.data.DataLoader(eff_train_dataset, batch_size = eff_train_batch, shuffle = True, collate_fn= detection_collate)\n",
    "eff_valid_loader = torch.utils.data.DataLoader(eff_valid_dataset, batch_size = eff_test_batch, shuffle = True, collate_fn = detection_collate)\n",
    "\n",
    "print(\" \\n ... Seperate from Data Loader \\n\")\n",
    "print(\"Length of train_dataset {}\".format(len(eff_train_dataset)))\n",
    "print(\"Length of valid_dataset {}\".format(len(eff_valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Run with DataParallel ....\n",
      "Mish activation loaded...\n"
     ]
    }
   ],
   "source": [
    "MODEL_MAP = {\n",
    "    'efficientdet-d0': 'efficientnet-b0',\n",
    "    'efficientdet-d1': 'efficientnet-b1',\n",
    "    'efficientdet-d2': 'efficientnet-b2',\n",
    "    'efficientdet-d3': 'efficientnet-b3',\n",
    "    'efficientdet-d4': 'efficientnet-b4',\n",
    "    'efficientdet-d5': 'efficientnet-b5',\n",
    "    'efficientdet-d6': 'efficientnet-b6',\n",
    "    'efficientdet-d7': 'efficientnet-b6',\n",
    "}\n",
    "class EfficientDet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 network='efficientdet-d0',\n",
    "                 D_bifpn=3,\n",
    "                 W_bifpn=88,\n",
    "                 D_class=3,\n",
    "                 is_training=True,\n",
    "                 threshold=0.001, #can change this value 0.01\n",
    "                 iou_threshold=1): # can change this value 0.5\n",
    "        super(EfficientDet, self).__init__()\n",
    "        \n",
    "        self.backbone = EfficientNet.from_pretrained(MODEL_MAP[network])\n",
    "        self.is_training = is_training\n",
    "        self.neck = BIFPN(in_channels=self.backbone.get_list_features()[-5:],\n",
    "                          out_channels=W_bifpn,\n",
    "                          stack=D_bifpn,\n",
    "                          num_outs=5)\n",
    "        self.bbox_head = RetinaHead(num_classes=num_classes,\n",
    "                                    in_channels=W_bifpn)\n",
    "\n",
    "        self.anchors = Anchors()\n",
    "        self.regressBoxes = BBoxTransform()\n",
    "        self.clipBoxes = ClipBoxes()\n",
    "        self.threshold = threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        self.freeze_bn()\n",
    "        self.criterion = FocalLoss()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.is_training:\n",
    "            inputs, annotations = inputs\n",
    "        else:\n",
    "            inputs = inputs\n",
    "        x = self.extract_feat(inputs)\n",
    "        outs = self.bbox_head(x)\n",
    "        classification = torch.cat([out for out in outs[0]], dim=1)\n",
    "        regression = torch.cat([out for out in outs[1]], dim=1)\n",
    "        anchors = self.anchors(inputs)\n",
    "        if self.is_training:\n",
    "            return self.criterion(classification, regression, anchors, annotations)\n",
    "        else:\n",
    "            transformed_anchors = self.regressBoxes(anchors, regression)\n",
    "            transformed_anchors = self.clipBoxes(transformed_anchors, inputs)\n",
    "            scores = torch.max(classification, dim=2, keepdim=True)[0]\n",
    "            scores_over_thresh = (scores > self.threshold)[0, :, 0]\n",
    "\n",
    "            if scores_over_thresh.sum() == 0:\n",
    "                # print('No boxes to NMS')\n",
    "                # no boxes to NMS, just return\n",
    "                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n",
    "            classification = classification[:, scores_over_thresh, :]\n",
    "            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n",
    "            scores = scores[:, scores_over_thresh, :]\n",
    "            anchors_nms_idx = nms(\n",
    "                transformed_anchors[0, :, :], scores[0, :, 0], iou_threshold=self.iou_threshold)\n",
    "            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(\n",
    "                dim=1)\n",
    "            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        '''Freeze BatchNorm layers.'''\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.BatchNorm2d):\n",
    "                layer.eval()\n",
    "\n",
    "    def extract_feat(self, img):\n",
    "        \"\"\"\n",
    "            Directly extract features from the backbone+neck\n",
    "        \"\"\"\n",
    "        x = self.backbone(img)\n",
    "        x = self.neck(x[-5:])\n",
    "        return x\n",
    "\n",
    "model= EfficientDet(num_classes=len(get_class_info()),is_training=True)\n",
    "model.train()\n",
    "\n",
    "model.freeze_bn()\n",
    "\n",
    "model = model.cuda()\n",
    "print('Run with DataParallel ....')\n",
    "\n",
    "## Make sure that you add this line, even though you are not using more than one \n",
    "# GPU DataParallel adds \"module\" to the start of the model structure \n",
    "# allowing for the syntax to be correct when calling \"model.module.freeze_bn()\" for example\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "# I am doing this here an example, you do not have to call the lines below here\n",
    "model.module.is_training = True\n",
    "model.module.freeze_bn()\n",
    "\n",
    "convert_it(model, Mish(), nn.ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1122397 train images in total\n",
      "Amount of image files in Dataset 54647\n",
      "Amount of annotation files in Dataset 54647\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n",
      "Amount of image files in Dataset 88\n",
      "Amount of annotation files in Dataset 88\n",
      "\n",
      "\n",
      "Loading with Effecient Det Structure ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
    "    assert (len(scores) == len(classification) == len(transformed_anchors))\n",
    "    if len(transformed_anchors) != 0:\n",
    "      print(targets[0][:, :4], \"\\n\", transformed_anchors, scores)\n",
    "      curr_mAP, curr_mAR = calculate_metrics(targets[0][:, :4], transformed_anchors, scores, device)\n",
    "      mAP, mAR = mAP + curr_mAP , mAR + curr_mAR\n",
    "    else:\n",
    "      missed_images += 1 \n",
    "    \n",
    "# def run_metrics_for_batch(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device):\n",
    "#   for pos_in_batch, image_pred in enumerate(output):\n",
    "#     assert (len(scores) == len(classification) == len(transformed_anchors))\n",
    "#     if len(image_pred[\"boxes\"]) != 0:\n",
    "#       mAP += calculate_metrics(targets[pos_in_batch][\"boxes\"], output[pos_in_batch][\"boxes\"], output[pos_in_batch][\"scores\"], device)\n",
    "#     else:\n",
    "#       missed_images += 1\n",
    "  \n",
    "#   return mAP, missed_images\n",
    "      \n",
    "    return mAP, mAR, missed_images\n",
    "def detection_collate(batch):\n",
    "    imgs = [s['image'] for s in batch]\n",
    "    annots = [s['bboxes'] for s in batch]\n",
    "    labels = [s['category_id'] for s in batch]\n",
    "\n",
    "    max_num_annots = max(len(annot) for annot in annots)\n",
    "    annot_padded = np.ones((len(annots), max_num_annots, 5))*-1\n",
    "\n",
    "    if max_num_annots > 0:\n",
    "        for idx, (annot, lab) in enumerate(zip(annots, labels)):\n",
    "            if len(annot) > 0:\n",
    "                annot_padded[idx, :len(annot), :4] = annot\n",
    "                annot_padded[idx, :len(annot), 4] = lab\n",
    "    return (torch.stack(imgs, 0), torch.FloatTensor(annot_padded))\n",
    "\n",
    "train_batch_size = 8\n",
    "valid_batch_size = 1\n",
    "\n",
    "eff_train_dataset = VideoFrameDataset(\"train\", os.path.join(\"Data/VID\", \"train\"), os.path.join(\"Annotations/VID\", \"train\"), get_transforms(mode = \"effdet_train\"), \n",
    "                                  seg_len = 20, effdet_data = True)\n",
    "eff_train_loader = torch.utils.data.DataLoader(eff_train_dataset, batch_size = train_batch_size, shuffle = True, collate_fn= detection_collate)\n",
    "\n",
    "\n",
    "eff_valid_dataset = VideoFrameDataset(\"validation\", os.path.join(\"Data/VID\", \"val\"), os.path.join(\"Annotations/VID\", \"val\"), get_transforms(mode = \"effdet_test\"),\n",
    "                                  make_valid_smaller_percent = 0.01, effdet_data = True)\n",
    "eff_valid_loader = torch.utils.data.DataLoader(eff_valid_dataset, batch_size = valid_batch_size, shuffle = True, collate_fn = detection_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_effdet(net, epochs, train_loader, test_loader, lr, weight_decay, \n",
    "          print_times_per_epoch, lo_test_dataset = len(eff_valid_dataset), lo_train_dataset = len(eff_train_dataset)):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device: {}\".format(device))\n",
    "    print(\"Note: Train Accuracies are only run through one train image per batch\")\n",
    "    \n",
    "    print_every = len(train_dataset) / train_batch_size // print_times_per_epoch\n",
    "\n",
    "    if device == torch.device(\"cpu\"):\n",
    "      warnings.warn(\"Code does not support running on CPU but only GPU\")\n",
    "\n",
    "    optimizer = optim.AdamW(net.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = len(train_loader) * epochs)\n",
    "\n",
    "    start_time = time.time()\n",
    "    net.module.freeze_bn()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        net.train()\n",
    "        net.module.is_training = True\n",
    "        \n",
    "        train_loss = steps = train_mAP = train_mAR = missed_train_images = 0\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "\n",
    "            net.train()\n",
    "            net.module.is_training = True\n",
    "\n",
    "            steps += 1\n",
    "            \n",
    "            images = images.cuda().float()\n",
    "            targets = targets.cuda()\n",
    "\n",
    "            classification_loss, regression_loss = model([images, targets])\n",
    "            classification_loss = classification_loss.mean()\n",
    "            regression_loss = regression_loss.mean()\n",
    "            loss = classification_loss + regression_loss\n",
    "            if bool(loss == 0):\n",
    "              print('loss equal zero(0)')\n",
    "              continue\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            net.eval()\n",
    "            net.module.is_training = False\n",
    "            scores, classification, transformed_anchors = net(images[0].unsqueeze(0))\n",
    "#             train_mAP, train_mAR, missed_train_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, train_mAP, \n",
    "#                                                                                      train_mAR, missed_train_images, device)\n",
    "            net.train()\n",
    "            net.module.is_training = True\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if (steps % print_every) == 0:\n",
    "\n",
    "              with torch.no_grad():\n",
    "                test_mAP = test_mAR = missed_test_images = test_loss = 0\n",
    "\n",
    "                for images, targets in test_loader:\n",
    "\n",
    "                  if images.size(0) != 1:\n",
    "                    warning.warn(\"Only can validate fully with batch size of 1, \\\n",
    "                    bigger batch sizes risk Errors or Incomplete Validation\")\n",
    "                  \n",
    "                  net.eval()\n",
    "                  net.module.is_training = False\n",
    "\n",
    "                  if device == torch.device(\"cuda\"):\n",
    "                    images = images.cuda().float()\n",
    "                    targets = targets.cuda()\n",
    "\n",
    "                  scores, classification, transformed_anchors = net(images)\n",
    "                  test_mAP, test_mAR, missed_test_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, \n",
    "                                                                                 test_mAP, test_mAR, missed_test_images, device)\n",
    "\n",
    "                  net.train()\n",
    "                  net.module.is_training = True\n",
    "\n",
    "                  classification_loss, regression_loss = model([images, targets])\n",
    "                  classification_loss = classification_loss.mean()\n",
    "                  regression_loss = regression_loss.mean()\n",
    "                  loss = classification_loss + regression_loss\n",
    "\n",
    "                  test_loss += loss.item()\n",
    "\n",
    "                for param_group in optimizer.param_groups:\n",
    "                  learning_rate_extract = param_group[\"lr\"]\n",
    "                print(\"Epoch {}/{} | Batch Number: {} | LR: {:0.5f} | Train_loss: {:0.2f} | Test_loss: {:0.2f} | Test mAP: {:0.2f}% | Test mAR: {:0.2f}% | Missed Valid Images: {}\".format(\n",
    "                    epoch + 1, epochs, steps, learning_rate_extract, train_loss, test_loss,  \n",
    "                    (test_mAP / float(lo_test_dataset)) * 100., (test_mAR / float(lo_test_dataset)) * 100.,missed_test_images))\n",
    "\n",
    "              assert (steps % print_every) == 0\n",
    "              train_loss = 0\n",
    "              # scheduler.step(test_loss / float(lo_test_dataset))\n",
    "             \n",
    "        print(\"\\n Epoch {} Final Train mAP: {:0.2f}% | Epoch {} Final Train mAR: {:0.2f}% | Epoch {} Final Missed Train Images: {} out of {} images \\n\".format(\n",
    "            epoch + 1, (train_mAP / float(lo_train_dataset)) * 100., \n",
    "            epoch + 1, (train_mAR / float(lo_train_dataset)) * 100., \n",
    "            epoch + 1, missed_train_images, lo_train_dataset\n",
    "        ))\n",
    "    \n",
    "    print(\"Time for Total Training {:0.2f}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Note: Train Accuracies are only run through one train image per batch\n",
      "tensor([[143.,  96., 292., 235.]], device='cuda:0') \n",
      " tensor([[  0.0000,   0.0000, 512.0000, 512.0000],\n",
      "        [  0.0000,   0.0000, 512.0000, 512.0000],\n",
      "        [  0.0000,   0.0000, 512.0000, 512.0000],\n",
      "        ...,\n",
      "        [161.3834,  33.3957, 279.5280,  35.1457],\n",
      "        [  0.0000,  61.7551, 150.5256,  63.4067],\n",
      "        [  0.0000, 102.2922, 512.0000, 103.9337]], device='cuda:0') tensor([1.0000, 1.0000, 1.0000,  ..., 0.0010, 0.0010, 0.0010], device='cuda:0')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c8faeeaf9f49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_effdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meff_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meff_valid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-51c91b5f1eed>\u001b[0m in \u001b[0;36mtrain_effdet\u001b[0;34m(net, epochs, train_loader, test_loader, lr, weight_decay, print_times_per_epoch, lo_test_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     79\u001b[0m                   \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                   test_mAP, test_mAR, missed_test_images = run_metrics_for_effdet_batch(scores, classification, transformed_anchors, targets, \n\u001b[0;32m---> 81\u001b[0;31m                                                                                  test_mAP, test_mAR, missed_test_images, device)\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                   \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-f903546c6ec3>\u001b[0m in \u001b[0;36mrun_metrics_for_effdet_batch\u001b[0;34m(scores, classification, transformed_anchors, targets, mAP, mAR, missed_images, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m       \u001b[0mcurr_mAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_mAR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0mmAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmAR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmAP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr_mAP\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmAR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurr_mAR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevant_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration over a 0-d tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
     ]
    }
   ],
   "source": [
    "train_effdet(model, 3, eff_train_loader, eff_valid_loader, 0.001, 1e-4, print_times_per_epoch = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0cu33Jwf_K4"
   },
   "source": [
    "# Write inference code for this pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aShMihw2d4tB"
   },
   "source": [
    "### Using Recurrent Neural Networks with Faster R CNNs\n",
    "\n",
    "https://arxiv.org/pdf/2010.15740.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH2vWjX4IG_U"
   },
   "source": [
    "####  Get all images file paths in a folder and all xml file paths in a folder. Then put in tuple [(image file path1, xml file path1), (image2, xml 2)]\n",
    "\n",
    "Pair the image file paths and xml file paths into a particular scene. \n",
    "So all_scenes dict(): {folder name of one scene: list[(imf_path1, xml1), (img_path2, xml2), (img_path3, xml3)], foler name of second scene}\n",
    "\n",
    "key_path: [scene 1, scene 2, scene 3, scene 4, scene 5]\n",
    "\n",
    "class will get certain_scene = key_path[index] and then all_scenes[certain_scene] -> get access to list of images and labels and then load into image file path. \n",
    "\n",
    "create a tensor called single_scene\n",
    "\n",
    "For one indexed scene\n",
    "for tuple in list we get a tupe like this (img file path, annot file path)\n",
    "open image file path of (tup[0])\n",
    "open xml file and parse to get bounding boxes and other info (tup[1])\n",
    "\n",
    "use torch.stack()\n",
    "\n",
    "Now we have image tensor and target tensor. We can append to the single_scene.\n",
    "\n",
    "return single_scene which is [(image 1 tensor, target tensor of 1), (image 2 tensor, target tensor of 2)] also known as all the images and targets of one scene\n",
    "\n",
    "return this data to dataloader.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFgKvZ_k19LQ"
   },
   "outputs": [],
   "source": [
    "def get_per_scene_dict(img_root_path, annotations_root_path):\n",
    "  scene_names = glob.glob(\"{}/*\".format(img_root_path))\n",
    "  all_scenes = dict()\n",
    "  for scene in scene_names:\n",
    "  \n",
    "    image_file_paths = glob.glob(\"{}{}/*.JPEG\".format(img_root_path, scene.split(\"/\")[-1]))\n",
    "    xml_file_paths = glob.glob(\"{}{}/*.xml\".format(annotations_root_path, scene.split(\"/\")[-1]))\n",
    "    image_file_paths, xml_file_paths = sorted(image_file_paths), sorted(xml_file_paths)\n",
    "\n",
    "    assert len(image_file_paths) == len(xml_file_paths)\n",
    "\n",
    "    scene_list = [(image_file_path, xml_file_paths[ii]) for ii, image_file_path in enumerate(image_file_paths)]\n",
    "    all_scenes[scene] = scene_list\n",
    "  \n",
    "  return all_scenes\n",
    "\n",
    "class VideoFrameDataset():\n",
    "    \n",
    "    def __init__(self, all_scenes_dict, transforms, seg_len = 5):\n",
    "\n",
    "      self.all_scenes_dict = all_scenes_dict\n",
    "      self.seg_len = seg_len\n",
    "      self.transforms = transforms\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "      \n",
    "      current_scene = list(self.all_scenes_dict.keys())[idx]\n",
    "      list_of_fp_per_scene = self.all_scenes_dict[current_scene]\n",
    "\n",
    "      #Random Sampling. \n",
    "      if seg_len % 5 != 0:\n",
    "        raise ValueError(\"Not allowed value for seg_len must be divisible by 5\")\n",
    "      if seg_len >= len(list_of_fp_per_scene):\n",
    "        raise ValueError(\"Segments are bigger than the amount of frames available for a scene\")\n",
    "      \n",
    "      list_of_fp_per_scene = list_of_fp_per_scene[:-(len(list_of_fp_per_scene) % seg_len)]\n",
    "      reduced_fp_per_scene, start_index = list(), 0\n",
    "      for window in range(int(len(list_of_fp_per_scene) / self.seg_len))\n",
    "        end_index = start_index + self.seg_len\n",
    "        reduced_fp_per_scene.append(random.sample(list_of_fp_per_scene[start_index: end_index], 1)[0])\n",
    "        start_index = end_index\n",
    "      \n",
    "      scene_images, scene_bboxes = [], []\n",
    "      for data in reduced_fp_per_scene:\n",
    "        img_path, xml_path = data\n",
    "\n",
    "        img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        xml_doc = ElementTree.parse(xml_path)\n",
    "\n",
    "        bounding_boxes = xml_doc.findall(\"object/bndbox\")\n",
    "\n",
    "        bbox = []\n",
    "        for node in bounding_boxes:\n",
    "          xmax = node.find(\"xmax\").text\n",
    "          xmin = node.find(\"xmin\").text\n",
    "          ymax = node.find(\"ymax\").text\n",
    "          ymin = node.find(\"ymin\").text\n",
    "\n",
    "          bbox.append([int(xmin), int(ymin), int(xmax), int(ymax)])\n",
    "        \n",
    "        bbox = torch.as_tensor(bbox, dtype = torch.float32)\n",
    "        \n",
    "        if self.transforms: \n",
    "          sample = {\n",
    "              'image': img,\n",
    "              'bboxes': bbox,\n",
    "              'labels': labels\n",
    "              }\n",
    "\n",
    "        sample = self.transforms(**sample)\n",
    "        augmented_img = sample['image']\n",
    "        augmented_bbox = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "\n",
    "        scene_bboxes.append(augmented_bbox)\n",
    "        scene_images.append(augmented_img)\n",
    "      \n",
    "      scene_images, scene_bboxes = torch.stack(scene_images), torch.stack(scene_bboxes)\n",
    "\n",
    "      scene_target = dict() \n",
    "      scene_target[\"boxes\"] = scene_bboxes\n",
    "\n",
    "      scene_target[\"image_id\"] = ###############################################\n",
    "      scene_target[\"labels\"] = #################################################\n",
    "\n",
    "      return scene_images, scene_target\n",
    "      \n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nChPPsoeUE2M"
   },
   "outputs": [],
   "source": [
    "def test(net, test_loader, ####):\n",
    "\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    test_mAP = test_loss = test_missed_images = 0\n",
    "    for images, targets in test_loader:\n",
    "\n",
    "        if device == torch.device(\"cuda\"):\n",
    "          images = [image.to(device) for image in images]\n",
    "          targets = [{key: value.to(device) for key, value in t.items()} for t in targets]\n",
    "\n",
    "        net.eval()\n",
    "        output = net(images)\n",
    "        test_mAP, test_missed_images = run_metrics_for_batch(output, targets, test_mAP, test_missed_images, device)\n",
    "\n",
    "        net.train()\n",
    "        test_loss_dict = net(images, targets)\n",
    "        test_losses = sum(loss for loss in test_loss_dict.values())\n",
    "        test_loss += test_losses\n",
    "\n",
    "    print(\"Test mAP {:0.2f}% | Test Loss {:0.2f} | Test Missed Images {} / {}\".format(test_mAP, test_loss, test_missed_images, #####))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links that I used to get notebook running with correct 3rd party packages\n",
    "\n",
    "* https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "* https://www.digitalocean.com/community/tutorials/how-to-set-up-jupyter-notebook-with-python-3-on-ubuntu-18-04\n",
    "* Make sure after running packages update on conda environment to shut down and reopen notebook for update.\n",
    "* Just git clone in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "NHV44UYGVab0"
   },
   "outputs": [],
   "source": [
    "class VideoFrameDataset():\n",
    "    \n",
    "    def __init__(self, mode, vid_data_root_dir, vid_annotations_root_dir, transforms, amount_per_folder = None, \n",
    "               make_valid_smaller_percent = None, effdet_data = None, rcnn_big = None, det_text_file_paths = None):\n",
    "        \n",
    "    # If valid smaller is true cut the length of valid list to certain length with if statement. \n",
    "        if (mode == \"train\"):\n",
    "            if (amount_per_folder):\n",
    "                #Subset images from every scene there are 3862 data folders.\n",
    "                print(\"There are 3862 train data folders in total\")\n",
    "                globbed_image_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_data_root_dir)))\n",
    "                globbed_annotations_file_paths = sorted(glob.glob(\"{}/*/*\".format(vid_annotations_root_dir)))\n",
    "                \n",
    "                self.image_file_paths, self.annotations_file_paths = list(), list()\n",
    "                \n",
    "                for folder in list(zip(globbed_image_file_paths, globbed_annotations_file_paths)):\n",
    "                    scene_images = sorted(glob.glob(\"{}/*.JPEG\".format(folder[0])))\n",
    "                    scene_annotations = sorted(glob.glob(\"{}/*.xml\".format(folder[1])))\n",
    "                    \n",
    "                    image_annot = list(zip(scene_images, scene_annotations))\n",
    "                    #Use span evenly distributed \n",
    "                    \n",
    "                    image_annot = random.sample(image_annot, amount_per_folder)\n",
    "                    \n",
    "                    scene_images, scene_annotations = zip(*image_annot) \n",
    "                    \n",
    "                    self.annotations_file_paths.extend(scene_annotations)\n",
    "                    self.image_file_paths.extend(scene_images)\n",
    "                    \n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "            \n",
    "        elif (mode == \"validation\"):\n",
    "            if make_valid_smaller_percent:\n",
    "                #Subset a percent of the valid data\n",
    "                valid_image_list = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir)))\n",
    "                valid_annotations_list = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "\n",
    "                subset = int(len(valid_image_list) * make_valid_smaller_percent)\n",
    "\n",
    "                #Shuffle both lists at once with same order\n",
    "                mapIndexPosition = list(zip(valid_image_list, valid_annotations_list))\n",
    "                random.shuffle(mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = zip(*mapIndexPosition)\n",
    "                valid_image_list, valid_annotations_list = list(valid_image_list), list(valid_annotations_list)\n",
    "\n",
    "                self.image_file_paths = valid_image_list[:subset]\n",
    "                self.annotations_file_paths = valid_annotations_list[:subset]\n",
    "            else:\n",
    "                self.image_file_paths = sorted(glob.glob(\"{}/*/*.JPEG\".format(vid_data_root_dir))) \n",
    "                self.annotations_file_paths = sorted(glob.glob(\"{}/*/*.xml\".format(vid_annotations_root_dir)))\n",
    "        else:\n",
    "            raise ValueError(\"Choose mode between train or validation only\")\n",
    "\n",
    "        self.labels_key = get_class_info(get_keys = True)\n",
    "\n",
    "        assert len(self.image_file_paths) == len(self.annotations_file_paths)\n",
    "        print(\"Amount of image files in Dataset {}\".format(len(self.image_file_paths)))\n",
    "        print(\"Amount of annotation files in Dataset {}\".format(len(self.annotations_file_paths)))\n",
    "\n",
    "        self.transforms = transforms\n",
    "        self.effdet_data = effdet_data\n",
    "        self.rcnn_big = rcnn_big\n",
    "        \n",
    "#         if det_text_file_paths:\n",
    "#             det_txt = open(det_text_file_paths, \"r\").readlines()\n",
    "#             for line in det_txt:\n",
    "#                 if len(line) == 56:\n",
    "#                     self.image_file_paths.append((line.split(\" \")[0] + \".JPEG\"))\n",
    "#                     self.annotations_file_paths.append((line.split(\" \")[0] + \".xml\"))\n",
    "        \n",
    "        \n",
    "        if self.effdet_data:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with Effecient Det Structure ... \\n\")\n",
    "        elif self.rcnn_big:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with bigger rcnn with ROI Structure ... \\n\")\n",
    "        else:\n",
    "            print(\"\\n\")\n",
    "            print(\"Loading with mobilenet Faster R CNN Structure ... \\n\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_path, xml_path = self.image_file_paths[idx], self.annotations_file_paths[idx]\n",
    "\n",
    "        img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        # img = cv2.cvtColor(cv2.imread(img_path, cv2.IMREAD_COLOR), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        xml_doc = ElementTree.parse(xml_path)\n",
    "\n",
    "        bounding_boxes_nodes = xml_doc.findall(\"object/bndbox\")\n",
    "        labels_nodes = xml_doc.findall(\"object/name\")\n",
    "\n",
    "        bbox, labels = [], []\n",
    "\n",
    "        for node in bounding_boxes_nodes:\n",
    "            xmax = node.find(\"xmax\").text\n",
    "            xmin = node.find(\"xmin\").text\n",
    "            ymax = node.find(\"ymax\").text\n",
    "            ymin = node.find(\"ymin\").text\n",
    "            bbox.append([int(xmin), int(ymin), int(xmax), int(ymax)])  \n",
    "        bbox = torch.as_tensor(bbox, dtype = torch.float32)\n",
    "\n",
    "        for node in labels_nodes:\n",
    "            if self.effdet_data or self.rcnn_big:\n",
    "                label = self.labels_key.index(node.text)\n",
    "            else:\n",
    "                label = self.labels_key.index(node.text) + 1\n",
    "            labels.append(label)\n",
    "        labels = torch.as_tensor(labels, dtype = torch.int64)\n",
    "\n",
    "        # labels = tf.cast(labels, dtype = tf.int64)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        \n",
    "        try:\n",
    "            if self.transforms:  \n",
    "                sample = {\n",
    "                    'image': img,\n",
    "                    'bboxes': bbox,\n",
    "                    'labels': labels\n",
    "                      }\n",
    "\n",
    "                sample = self.transforms(**sample)\n",
    "                img = sample['image']\n",
    "                try:\n",
    "                    bbox = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n",
    "                except:\n",
    "                    bbox = torch.zeros((0, 4), dtype=torch.float32)\n",
    "        except:\n",
    "            print(\"Caught error. Now trying to instill transforms using Pytorch transforms\")\n",
    "            \n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = transforms.ToTensor()(img) \n",
    "                \n",
    "          # img = tf.cast(sample['image'], dtype = tf.float32) / 255.0\n",
    "          # bbox = tf.convert_to_tensor(torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0), dtype = tf.float32)\n",
    "\n",
    "        if self.effdet_data:\n",
    "            return {\"image\": img, \"bboxes\": bbox, \"category_id\": labels}\n",
    "        \n",
    "        target = dict()\n",
    "        target[\"boxes\"] = bbox\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "\n",
    "        return img, target  \n",
    "    \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.image_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wO5yLFymZOSZ",
    "outputId": "c4684fb1-e61e-452d-cad6-5c833149b1ec",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Device: cuda\n",
      "Optimizer: SAM (\n",
      "Parameter Group 0\n",
      "    N_sma_threshhold: 5\n",
      "    alpha: 0.5\n",
      "    betas: (0.95, 0.999)\n",
      "    eps: 1e-05\n",
      "    initial_lr: 0.001\n",
      "    k: 6\n",
      "    lr: 0.001\n",
      "    rho: 0.05\n",
      "    step_counter: 0\n",
      "    weight_decay: 0.0001\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer/ranger/ranger.py:138: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370128159/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "/home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch Number: 28 | LR: 0.00100 | Train_loss: 94.32 | Valid_loss: 76.92 | Valid mAP: 1.16% | Valid Missed Images 75 / 88\n",
      "Epoch 1/1 | Batch Number: 56 | LR: 0.00100 | Train_loss: 27.41 | Valid_loss: 62.92 | Valid mAP: 3.97% | Valid Missed Images 76 / 88\n",
      "Epoch 1/1 | Batch Number: 84 | LR: 0.00100 | Train_loss: 25.47 | Valid_loss: 50.36 | Valid mAP: 11.10% | Valid Missed Images 57 / 88\n",
      "Epoch 1/1 | Batch Number: 112 | LR: 0.00100 | Train_loss: 18.71 | Valid_loss: 40.04 | Valid mAP: 7.95% | Valid Missed Images 69 / 88\n",
      "Epoch 1/1 | Batch Number: 140 | LR: 0.00100 | Train_loss: 21.55 | Valid_loss: 39.10 | Valid mAP: 0.32% | Valid Missed Images 66 / 88\n",
      "Epoch 1/1 | Batch Number: 168 | LR: 0.00100 | Train_loss: 17.03 | Valid_loss: 32.78 | Valid mAP: 3.97% | Valid Missed Images 77 / 88\n",
      "Epoch 1/1 | Batch Number: 196 | LR: 0.00100 | Train_loss: 18.85 | Valid_loss: 36.41 | Valid mAP: 24.06% | Valid Missed Images 32 / 88\n",
      "Epoch 1/1 | Batch Number: 224 | LR: 0.00100 | Train_loss: 14.55 | Valid_loss: 35.09 | Valid mAP: 2.53% | Valid Missed Images 78 / 88\n",
      "Epoch 1/1 | Batch Number: 252 | LR: 0.00100 | Train_loss: 15.28 | Valid_loss: 35.01 | Valid mAP: 1.14% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 280 | LR: 0.00100 | Train_loss: 12.37 | Valid_loss: 33.99 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 308 | LR: 0.00100 | Train_loss: 14.67 | Valid_loss: 32.56 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 336 | LR: 0.00100 | Train_loss: 13.46 | Valid_loss: 31.55 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 364 | LR: 0.00100 | Train_loss: 15.14 | Valid_loss: 34.07 | Valid mAP: 11.36% | Valid Missed Images 76 / 88\n",
      "Epoch 1/1 | Batch Number: 392 | LR: 0.00100 | Train_loss: 12.74 | Valid_loss: 32.65 | Valid mAP: 0.00% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 420 | LR: 0.00100 | Train_loss: 16.99 | Valid_loss: 35.44 | Valid mAP: 0.57% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 448 | LR: 0.00100 | Train_loss: 15.36 | Valid_loss: 31.74 | Valid mAP: 3.41% | Valid Missed Images 79 / 88\n",
      "Epoch 1/1 | Batch Number: 476 | LR: 0.00100 | Train_loss: 12.81 | Valid_loss: 32.34 | Valid mAP: 2.27% | Valid Missed Images 85 / 88\n",
      "Epoch 1/1 | Batch Number: 504 | LR: 0.00100 | Train_loss: 19.45 | Valid_loss: 43.06 | Valid mAP: 0.19% | Valid Missed Images 86 / 88\n",
      "Epoch 1/1 | Batch Number: 532 | LR: 0.00099 | Train_loss: 15.30 | Valid_loss: 33.95 | Valid mAP: 4.84% | Valid Missed Images 81 / 88\n",
      "Epoch 1/1 | Batch Number: 560 | LR: 0.00099 | Train_loss: 11.59 | Valid_loss: 36.38 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 588 | LR: 0.00099 | Train_loss: 16.96 | Valid_loss: 39.23 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 616 | LR: 0.00099 | Train_loss: 71.33 | Valid_loss: 50.94 | Valid mAP: 34.25% | Valid Missed Images 41 / 88\n",
      "Epoch 1/1 | Batch Number: 644 | LR: 0.00099 | Train_loss: 20.04 | Valid_loss: 50.85 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 672 | LR: 0.00099 | Train_loss: 46.87 | Valid_loss: 44.66 | Valid mAP: 1.14% | Valid Missed Images 86 / 88\n",
      "Epoch 1/1 | Batch Number: 700 | LR: 0.00099 | Train_loss: 22.35 | Valid_loss: 36.40 | Valid mAP: 2.27% | Valid Missed Images 86 / 88\n",
      "Epoch 1/1 | Batch Number: 728 | LR: 0.00099 | Train_loss: 13.44 | Valid_loss: 37.08 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 756 | LR: 0.00099 | Train_loss: 11.84 | Valid_loss: 37.66 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 784 | LR: 0.00099 | Train_loss: 14.93 | Valid_loss: 39.79 | Valid mAP: 0.57% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 812 | LR: 0.00099 | Train_loss: 13.54 | Valid_loss: 36.03 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 840 | LR: 0.00099 | Train_loss: 15.20 | Valid_loss: 34.60 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 868 | LR: 0.00099 | Train_loss: 12.57 | Valid_loss: 34.46 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 896 | LR: 0.00099 | Train_loss: 12.98 | Valid_loss: 39.05 | Valid mAP: 0.23% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 924 | LR: 0.00098 | Train_loss: 19.98 | Valid_loss: 40.42 | Valid mAP: 0.57% | Valid Missed Images 87 / 88\n",
      "Epoch 1/1 | Batch Number: 952 | LR: 0.00098 | Train_loss: 13.05 | Valid_loss: 41.11 | Valid mAP: 0.00% | Valid Missed Images 88 / 88\n",
      "Epoch 1/1 | Batch Number: 980 | LR: 0.00098 | Train_loss: 16.54 | Valid_loss: 33.79 | Valid mAP: 0.00% | Valid Missed Images 87 / 88\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-c125db021ce1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr_cnn_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-158e2967f03d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch, lo_valid_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Check for degenerate boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 raise ValueError(\"images is expected to be a list of 3d tensors \"\n\u001b[1;32m    103\u001b[0m                                  \"of shape [C, H, W], got {}\".format(image.shape))\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torchvision/models/detection/transform.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "r_cnn_trained = train(model, 1, train_loader, valid_loader, 0.001, weight_decay = 1e-4, print_times_per_epoch = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "Device: cuda\n",
      "Optimizer: SAM (\n",
      "Parameter Group 0\n",
      "    N_sma_threshhold: 5\n",
      "    alpha: 0.5\n",
      "    betas: (0.95, 0.999)\n",
      "    eps: 1e-05\n",
      "    initial_lr: 0.0001\n",
      "    k: 6\n",
      "    lr: 0.0001\n",
      "    rho: 0.05\n",
      "    step_counter: 0\n",
      "    weight_decay: 1e-05\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarthak/DataSets/ILSVRC2015/Ranger-Deep-Learning-Optimizer/ranger/ranger.py:138: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370128159/work/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
      "/home/sarthak/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Batch Number: 22447 | LR: 0.00010 | Train_loss: 112178093.03 | Valid_loss: 1046.19 | Valid mAP: 6.47% | Valid Missed Images 0 / 1761\n",
      "Epoch 1/1 | Batch Number: 44894 | LR: 0.00010 | Train_loss: 3822367.73 | Valid_loss: 1813196.61 | Valid mAP: 20.79% | Valid Missed Images 0 / 1761\n",
      "Epoch 1/1 | Batch Number: 67341 | LR: 0.00010 | Train_loss: 68994806.74 | Valid_loss: 666.07 | Valid mAP: 1.79% | Valid Missed Images 1709 / 1761\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected x_max for bbox (tensor(0.9219), tensor(0.3847), tensor(1.0063), tensor(0.5528), tensor(7)) to be in the range [0.0, 1.0], got 1.006250023841858.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b2c9aa3dc8d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmob_net_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmob_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_times_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-a4be57bd6387>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, epochs, train_loader, valid_loader, lr, weight_decay, print_times_per_epoch, lo_valid_dataset, lo_train_dataset)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mAP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_missed_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e12314bbb96e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    115\u001b[0m                   }\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdual_start_end\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdual_start_end\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/core/utils.py\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_fields\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_and_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_and_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"to\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/core/utils.py\u001b[0m in \u001b[0;36mcheck_and_convert\u001b[0;34m(self, data, rows, cols, direction)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdirection\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"to\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_from_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_to_albumentations\u001b[0;34m(self, data, rows, cols)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_bboxes_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_bboxes_to_albumentations\u001b[0;34m(bboxes, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\n\u001b[1;32m    302\u001b[0m     \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert_bbox_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\n\u001b[1;32m    302\u001b[0m     \"\"\"\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconvert_bbox_to_albumentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mconvert_bbox_to_albumentations\u001b[0;34m(bbox, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcheck_validity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mcheck_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep-learning-base/lib/python3.7/site-packages/albumentations/augmentations/bbox_utils.py\u001b[0m in \u001b[0;36mcheck_bbox\u001b[0;34m(bbox)\u001b[0m\n\u001b[1;32m    328\u001b[0m             raise ValueError(\n\u001b[1;32m    329\u001b[0m                 \u001b[0;34m\"Expected {name} for bbox {bbox} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m                 \u001b[0;34m\"to be in the range [0.0, 1.0], got {value}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m             )\n\u001b[1;32m    332\u001b[0m     \u001b[0mx_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected x_max for bbox (tensor(0.9219), tensor(0.3847), tensor(1.0063), tensor(0.5528), tensor(7)) to be in the range [0.0, 1.0], got 1.006250023841858."
     ]
    }
   ],
   "source": [
    "mob_net_trained = train(mob_net, 1, train_loader, valid_loader, 0.0001, weight_decay = 1e-5, print_times_per_epoch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resizer(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=512):\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, annots = sample['img'], sample['annot']\n",
    "        height, width, _ = image.shape\n",
    "        if height > width:\n",
    "            scale = self.img_size / height\n",
    "            resized_height = self.img_size\n",
    "            resized_width = int(width * scale)\n",
    "        else:\n",
    "            scale = self.img_size / width\n",
    "            resized_height = int(height * scale)\n",
    "            resized_width = self.img_size\n",
    "\n",
    "        image = cv2.resize(image, (resized_width, resized_height), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        new_image = np.zeros((self.img_size, self.img_size, 3))\n",
    "        new_image[0:resized_height, 0:resized_width] = image\n",
    "\n",
    "        annots[:, :4] *= scale\n",
    "\n",
    "        return {'img': torch.from_numpy(new_image).to(torch.float32), 'annot': torch.from_numpy(annots), 'scale': scale}\n",
    "\n",
    "\n",
    "class Augmenter(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample, flip_x=0.5):\n",
    "        if np.random.rand() < flip_x:\n",
    "            image, annots = sample['img'], sample['annot']\n",
    "            image = image[:, ::-1, :]\n",
    "\n",
    "            rows, cols, channels = image.shape\n",
    "\n",
    "            x1 = annots[:, 0].copy()\n",
    "            x2 = annots[:, 2].copy()\n",
    "\n",
    "            x_tmp = x1.copy()\n",
    "\n",
    "            annots[:, 0] = cols - x2\n",
    "            annots[:, 2] = cols - x_tmp\n",
    "\n",
    "            sample = {'img': image, 'annot': annots}\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "\n",
    "    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "        self.mean = np.array([[mean]])\n",
    "        self.std = np.array([[std]])\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, annots = sample['img'], sample['annot']\n",
    "\n",
    "        return {'img': ((image.astype(np.float32) - self.mean) / self.std), 'annot': annots}"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ILSVRCVid2015VideoDetection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "169081d5b0d94288a2b0ebb020b790d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1c3baca68a4e42afa680e581e92a7e82": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28ba486f593a4109bd152e94c871919d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c3baca68a4e42afa680e581e92a7e82",
      "max": 14212972,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_169081d5b0d94288a2b0ebb020b790d3",
      "value": 14212972
     }
    },
    "4839a59e79b6413bab12980f433870dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc398d009abc4ff7872a46b6e7bd497e",
      "placeholder": "",
      "style": "IPY_MODEL_cb9ef1bcc2334f7190b70c08a16090f7",
      "value": " 13.6M/13.6M [00:16&lt;00:00, 875kB/s]"
     }
    },
    "afba5dfb1b1a4811b1b0c02f03cb3c8d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbfa8ade6ff548aab08c392fe4bb20f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_28ba486f593a4109bd152e94c871919d",
       "IPY_MODEL_4839a59e79b6413bab12980f433870dc"
      ],
      "layout": "IPY_MODEL_afba5dfb1b1a4811b1b0c02f03cb3c8d"
     }
    },
    "cb9ef1bcc2334f7190b70c08a16090f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dc398d009abc4ff7872a46b6e7bd497e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
