{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch mAP: RetinaNet and Finetune fasterrcnn Fruit Defect Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa531febf62e4b0f86c367c3321ab3f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_41fb23ddc41440bb9111f6bf401f9808",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6dd01837aa71495ab12afb97d0a0a09b",
              "IPY_MODEL_ace5dd64842e4f2d848fa201553bd93e"
            ]
          }
        },
        "41fb23ddc41440bb9111f6bf401f9808": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6dd01837aa71495ab12afb97d0a0a09b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ad384b6d3dd5467d8a596d63b0d61591",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 136595076,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 136595076,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da74436850a449e68db804bc715176d0"
          }
        },
        "ace5dd64842e4f2d848fa201553bd93e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_62a8dd1c742a44da9a8fc8e6340c5acc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 130M/130M [00:16&lt;00:00, 8.04MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9de76ed7aa24e88b6e8940979156247"
          }
        },
        "ad384b6d3dd5467d8a596d63b0d61591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da74436850a449e68db804bc715176d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62a8dd1c742a44da9a8fc8e6340c5acc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9de76ed7aa24e88b6e8940979156247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfnWC_NF7NM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch \n",
        "from torch import nn, optim \n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import itertools\n",
        "import glob \n",
        "from PIL import Image\n",
        "import csv \n",
        "import cv2\n",
        "import re\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "import json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4UKDVNnIqQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4c81be-c124-4099-ad8d-e75edd48116a"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "!git clone https://github.com/SarthakJaingit/Fruits-and-Spot-Detection.git\n",
        "%cd \"Fruits-and-Spot-Detection\"\n",
        "with zipfile.ZipFile(\"/content/Fruits-and-Spot-Detection/LatestFruit Defects Dataset .zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Fruits-and-Spot-Detection'...\n",
            "remote: Enumerating objects: 267, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 267 (delta 34), reused 0 (delta 0), pack-reused 196\u001b[K\n",
            "Receiving objects: 100% (267/267), 34.02 MiB | 25.71 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n",
            "/content/Fruits-and-Spot-Detection\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY83cUEFAjoW",
        "outputId": "0fda9b61-4cd1-4af4-eed0-12a499ac2432"
      },
      "source": [
        "#For one strawberry batch please drop watermark rows\n",
        "strawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 3 Labeled/FreshStrawberryBatch3Labels.csv\", header = None)\n",
        "strawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 2 Labeled/FreshStrawberriesBatch2Labels.csv\", header = None)\n",
        "strawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 1 Labeled/Strawberrybatch1.csv\", header = None)\n",
        "rottenApple_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch1Labeled/RottenAppleBatch1Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch2Labeled/RottenApplesBatch2Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch3Labaled/RottenApplesBatch3Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch1RottenStrawBerryLabels/RottenStrawberriesBatch1Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch2RottenStrawBerryLabels/RottenStrawBerryBatch2.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch3RottenStrawberrylabel/rottenStrawberryBtch3labels.csv\", header = None)\n",
        "freshApples_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplebtch2label/FreshApplesBatch2LabelsFresh.csv\", header = None)\n",
        "freshApples_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplesBatch1Labels/FreshAppleBatch1Labels.csv\", header = None)\n",
        "rottenTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/Rotten TomatoBatch1/Batch1TomoatosLabelsBbox.csv\", header = None)\n",
        "rottenTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottnTomatoBatch2/RottenTomatyoBatch2Labelss.csv\", header = None)\n",
        "rottenTomato_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatBtch3/RottenTomatoesBatch3Labssles.csv\", header = None)\n",
        "rottenTomato_csv_batch_4 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatoesBatch4/Tomatobatch4labelssRotten.csv\", header = None)\n",
        "freshTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatoesBatch1Labelss/FreshTomatoesLabelsBatch1Labels.csv\", header = None)\n",
        "freshTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatBatch2Labessls/Batch2TomatlabelsFresh.csv\", header = None)\n",
        "\n",
        "strawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_3.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_4.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries59.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries9.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries93.jpeg\"].index, inplace = True)\n",
        "freshTomato_csv_batch_1.drop(freshTomato_csv_batch_1[freshTomato_csv_batch_1[\"Image_id\"] == \"Fresh Tomatoes66AddonPart1.jpeg\"].index, inplace = True)\n",
        "\n",
        "strawberry_csv_batch_1 = strawberry_csv_batch_1.reset_index(drop=True)\n",
        "freshTomato_csv_batch_1 = freshTomato_csv_batch_1.reset_index(drop = True)\n",
        "\n",
        "#Stack all the csv files together. \n",
        "list_of_all_dataframes = [strawberry_csv_batch_1, strawberry_csv_batch_2, strawberry_csv_batch_3, rottenApple_csv_batch_1, \n",
        "                          rottenApple_csv_batch_2, rottenApple_csv_batch_3, rottenStrawberry_csv_batch_1, rottenStrawberry_csv_batch_2, \n",
        "                          rottenStrawberry_csv_batch_3, freshApples_csv_batch_2, freshApples_csv_batch_1, rottenTomato_csv_batch_1, \n",
        "                          rottenTomato_csv_batch_2, rottenTomato_csv_batch_3, rottenTomato_csv_batch_4, freshTomato_csv_batch_1, \n",
        "                          freshTomato_csv_batch_2]\n",
        "fruit_df = pd.concat(list_of_all_dataframes, ignore_index = True)\n",
        "\n",
        "total_row_sum_check = 0 \n",
        "for dataframe in list_of_all_dataframes:\n",
        "  total_row_sum_check += dataframe.shape[0]\n",
        "print(\"Checked total rows from all the dataframes combined: {}\".format(total_row_sum_check))\n",
        "\n",
        "def run_dataframe_check():\n",
        "  assert total_row_sum_check == fruit_df.shape[0]\n",
        "  print(\"DataFrame shape: {}\".format(fruit_df.shape))\n",
        "  print(\"Unique Fruit Labels {}\".format(fruit_df[\"Fruit\"].unique()))\n",
        "  print(\"Number of Unique Images {}\".format(len(fruit_df[\"Image_id\"].unique())))\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Specify more image types when \n",
        "def more_specific_Image_id(image_id, fruit):\n",
        "  if fruit == \"Bad_Spots\":\n",
        "    if re.search(\"RottenStrawberries\", image_id):\n",
        "      return \"Strawberry_Bad_Spot\"\n",
        "    elif re.search(\"RottenApples\", image_id):\n",
        "      return \"Apple_Bad_Spot\"\n",
        "    elif re.search(\"Rotten Tomatoes\", image_id):\n",
        "      return \"Tomato_Bad_Spot\"\n",
        "    elif re.search(\"Rotten Peaches\", image_id):\n",
        "      return \"Peaches_Bad_Spot\"\n",
        "    else:\n",
        "      raise ValueError(\"Could not find a match for some of the Image_ids\")\n",
        "  else:\n",
        "    return fruit\n",
        "\n",
        "fruit_df[\"Fruit\"] = fruit_df.apply(lambda row: more_specific_Image_id(row.Image_id, row.Fruit), axis = 1)\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Post Processing \n",
        "fruit_df = fruit_df[fruit_df[\"Image_id\"] != \"FreshStrawberries15.jpeg\"]\n",
        "\n",
        "bounding_box_dict = dict()\n",
        "labels_dict = dict()\n",
        "\n",
        "classes = [\"Apples\", \"Strawberry\", \"Tomato\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\", \"Tomato_Bad_Spot\"]\n",
        "\n",
        "print(fruit_df)\n",
        "\n",
        "for row_index in range(len(fruit_df)): \n",
        "  if fruit_df.iloc[row_index, 0] in classes:\n",
        "    current_image_file = fruit_df.iloc[row_index][\"Image_id\"]\n",
        "    if current_image_file not in bounding_box_dict:\n",
        "      bounding_box_dict[current_image_file] = list()\n",
        "      labels_dict[current_image_file] = list()\n",
        "    bounding_box_dict[current_image_file].append(fruit_df.iloc[row_index, 1:5].to_list())\n",
        "    labels_dict[current_image_file].append(classes.index(fruit_df.iloc[row_index, 0]))\n",
        "\n",
        "print(len(bounding_box_dict))\n",
        "print(len(labels_dict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checked total rows from all the dataframes combined: 1882\n",
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Bad_Spots' 'Tomato']\n",
            "Number of Unique Images 532\n",
            "DataFrame shape: (1882, 8)\n",
            "Unique Fruit Labels ['Strawberry' 'Apples' 'Apple_Bad_Spot' 'Strawberry_Bad_Spot' 'Tomato'\n",
            " 'Tomato_Bad_Spot']\n",
            "Number of Unique Images 532\n",
            "           Fruit  Coord1  ...  OneSize  TwoSize\n",
            "0     Strawberry      19  ...      183      275\n",
            "1     Strawberry      83  ...      273      185\n",
            "2     Strawberry     129  ...      275      183\n",
            "3     Strawberry      41  ...      183      275\n",
            "4     Strawberry     131  ...      275      183\n",
            "...          ...     ...  ...      ...      ...\n",
            "1877      Tomato      99  ...      258      196\n",
            "1878      Tomato       4  ...      258      196\n",
            "1879      Tomato      12  ...      225      225\n",
            "1880      Tomato      27  ...      225      225\n",
            "1881      Tomato     128  ...      225      225\n",
            "\n",
            "[1880 rows x 8 columns]\n",
            "531\n",
            "531\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9bwvAaKzjg1"
      },
      "source": [
        "# First Step: Get the github repos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnXkUrojUb51",
        "outputId": "b29a504f-c8b0-46bb-926c-a1ff27102cb5"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 29130, done.\u001b[K\n",
            "remote: Counting objects: 100% (1673/1673), done.\u001b[K\n",
            "remote: Compressing objects: 100% (437/437), done.\u001b[K\n",
            "remote: Total 29130 (delta 1249), reused 1603 (delta 1205), pack-reused 27457\u001b[K\n",
            "Receiving objects: 100% (29130/29130), 37.28 MiB | 28.26 MiB/s, done.\n",
            "Resolving deltas: 100% (21865/21865), done.\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be376084 version check against PyTorch's CUDA version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFtfISUzzrSv"
      },
      "source": [
        "# Second Step to get the imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfvp2VT7jBaj",
        "outputId": "47894fb5-442e-4fe6-d783-e3f84d8b038b"
      },
      "source": [
        "!pip install pycocotools\n",
        "\n",
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import transforms as T\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (57.0.0)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (3.2.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools) (0.29.23)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1.0->pycocotools) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.1.0->pycocotools) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7KQVBKEfLwW"
      },
      "source": [
        "# COCO Bounding box: (x-top left, y-top left, width, height)\n",
        "# Pascal VOC Bounding box :(x-top left, y-top left,x-bottom right, y-bottom right)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvui3otiJNXE"
      },
      "source": [
        "class PennFudanDataset(object):\n",
        "    def __init__(self, labels_dict_class,bounding_box_dict_class,imgs_key_class, transforms):\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "\n",
        "        self.labels_dict=labels_dict_class\n",
        "        self.bounding_box_dict=  bounding_box_dict_class\n",
        "        self.imgs_key= imgs_key_class\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        boxes=[]\n",
        "\n",
        "        boxesxywh =  self.bounding_box_dict[self.imgs_key[idx]]\n",
        "        labels =  self.labels_dict[self.imgs_key[idx]]\n",
        "\n",
        "        labels_background_at_zero=[]\n",
        "        for label in labels:\n",
        "          labels_background_at_zero.append(label+1)\n",
        "\n",
        "        img_path = os.path.join('/content/Fruit Defects Dataset /images', str(self.imgs_key[idx]))\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        for i,box_xywh in enumerate(boxesxywh):\n",
        "          \n",
        "          x1= box_xywh[0]\n",
        "          w= box_xywh[2]\n",
        "\n",
        "          y1= box_xywh[1]\n",
        "          h= box_xywh[2]\n",
        "\n",
        "          x2 = w + x1\n",
        "          y2 = h + y1 \n",
        "\n",
        "          boxes.append([x1,y1,x2,y2])\n",
        "        \n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.as_tensor(labels_background_at_zero, dtype=torch.int64)\n",
        "\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((len(boxesxywh),), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_key)\n",
        "\n",
        "\n",
        "full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "imgs_key= sorted(bounding_box_dict.keys())\n",
        "imgs_key_train = imgs_key[:int(len(imgs_key) * 0.8)]\n",
        "imgs_key_test = imgs_key[int(len(imgs_key) * 0.8):]\n",
        "\n",
        "labels_dict_train = {k: labels_dict[k] for k in imgs_key_train}\n",
        "bounding_box_dict_train = {k: bounding_box_dict[k] for k in imgs_key_train}\n",
        "\n",
        "labels_dict_test = {k: labels_dict[k] for k in imgs_key_test}\n",
        "bounding_box_dict_test = {k: bounding_box_dict[k] for k in imgs_key_test}\n",
        "\n",
        "\n",
        "if not os.path.exists(\"/content/Fruit Defects Dataset /images\"):\n",
        "    os.mkdir(\"/content/Fruit Defects Dataset /images\")\n",
        "for imageName in full_image_file_paths:\n",
        "    shutil.copy(imageName, \"/content/Fruit Defects Dataset /images\")\n",
        "\n",
        "import vision.references.detection.transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "# use our dataset and defined transformations\n",
        "dataset = PennFudanDataset(labels_dict_train,bounding_box_dict_train,imgs_key_train, get_transform(train=True) )\n",
        "dataset_test = PennFudanDataset(labels_dict_test,bounding_box_dict_test,imgs_key_test, get_transform(train=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qwIoA9WEFM6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "embS6dvVjOrY"
      },
      "source": [
        "## RetinaNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smrEFfWj0C2H"
      },
      "source": [
        "# 3rd Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495,
          "referenced_widgets": [
            "fa531febf62e4b0f86c367c3321ab3f9",
            "41fb23ddc41440bb9111f6bf401f9808",
            "6dd01837aa71495ab12afb97d0a0a09b",
            "ace5dd64842e4f2d848fa201553bd93e",
            "ad384b6d3dd5467d8a596d63b0d61591",
            "da74436850a449e68db804bc715176d0",
            "62a8dd1c742a44da9a8fc8e6340c5acc",
            "f9de76ed7aa24e88b6e8940979156247"
          ]
        },
        "id": "0AFdR9SOSc03",
        "outputId": "82312a2f-8861-4718-99f0-ae47c1e1706c"
      },
      "source": [
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.retinanet import RetinaNet\n",
        "from torchvision.models.detection.retinanet import *\n",
        "from torchvision.models.detection.retinanet import RetinaNetHead\n",
        "\n",
        "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
        "num_classes = 7 \n",
        "\n",
        "# get number of input features and anchor boxed for the classifier\n",
        "in_features = model.head.classification_head.conv[0].in_channels\n",
        "num_anchors = model.head.classification_head.num_anchors\n",
        "\n",
        "# replace the pre-trained head with a new one\n",
        "model.head = RetinaNetHead(in_features, num_anchors, num_classes)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=4, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "# get the model using our helper function\n",
        "# move model to the right device\n",
        "model.cuda()\n",
        "\n",
        "for p in model.parameters():\n",
        "  p.requires_grad = False\n",
        "\n",
        "for p in model.head.parameters():\n",
        "  p.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01,\n",
        "                                momentum=0.9, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=0.1)\n",
        "\n",
        "# let's train it for 10 epochs\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth\" to /root/.cache/torch/hub/checkpoints/retinanet_resnet50_fpn_coco-eeacb38b.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa531febf62e4b0f86c367c3321ab3f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=136595076.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-cd7886484519>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# get the model using our helper function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# move model to the right device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \"\"\"\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \"\"\"\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOqOyIc1jSFF"
      },
      "source": [
        "## FasterRcnn Fine-tuned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1Kvp-FHujMVm",
        "outputId": "fcb99149-770d-4329-f0f6-cb01ac765ee2"
      },
      "source": [
        "\n",
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.retinanet import RetinaNet\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 7  # 6 class + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        " \n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=4, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=8, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "optimizer = torch.optim.Adam(params, lr=5e-3, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=0.1)\n",
        "\n",
        "# let's train it for 10 epochs\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    torch.save(model, 'model' +str(epoch)+ '.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0]  [  0/106]  eta: 0:01:19  lr: 0.000053  loss: 2.2035 (2.2035)  loss_classifier: 1.8047 (1.8047)  loss_box_reg: 0.3824 (0.3824)  loss_objectness: 0.0102 (0.0102)  loss_rpn_box_reg: 0.0062 (0.0062)  time: 0.7525  data: 0.1637  max mem: 5872\n",
            "Epoch: [0]  [ 10/106]  eta: 0:01:01  lr: 0.000528  loss: 2.1238 (2.1726)  loss_classifier: 1.5622 (1.5100)  loss_box_reg: 0.3824 (0.3731)  loss_objectness: 0.1492 (0.2468)  loss_rpn_box_reg: 0.0197 (0.0427)  time: 0.6365  data: 0.0252  max mem: 5872\n",
            "Epoch: [0]  [ 20/106]  eta: 0:00:54  lr: 0.001004  loss: 1.4809 (1.5878)  loss_classifier: 0.8809 (1.0363)  loss_box_reg: 0.3227 (0.3558)  loss_objectness: 0.1196 (0.1677)  loss_rpn_box_reg: 0.0142 (0.0279)  time: 0.6240  data: 0.0111  max mem: 5872\n",
            "Epoch: [0]  [ 30/106]  eta: 0:00:47  lr: 0.001480  loss: 0.8818 (1.3640)  loss_classifier: 0.4617 (0.8521)  loss_box_reg: 0.2876 (0.3271)  loss_objectness: 0.0653 (0.1593)  loss_rpn_box_reg: 0.0109 (0.0255)  time: 0.6258  data: 0.0110  max mem: 5872\n",
            "Epoch: [0]  [ 40/106]  eta: 0:00:41  lr: 0.001955  loss: 0.6700 (1.2055)  loss_classifier: 0.3962 (0.7146)  loss_box_reg: 0.2188 (0.2884)  loss_objectness: 0.0617 (0.1777)  loss_rpn_box_reg: 0.0167 (0.0248)  time: 0.6299  data: 0.0111  max mem: 5872\n",
            "Epoch: [0]  [ 50/106]  eta: 0:00:35  lr: 0.002431  loss: 0.5866 (1.0995)  loss_classifier: 0.2731 (0.6324)  loss_box_reg: 0.1935 (0.2775)  loss_objectness: 0.0507 (0.1634)  loss_rpn_box_reg: 0.0167 (0.0262)  time: 0.6245  data: 0.0108  max mem: 5872\n",
            "Epoch: [0]  [ 60/106]  eta: 0:00:28  lr: 0.002907  loss: 0.5294 (1.0220)  loss_classifier: 0.2220 (0.5637)  loss_box_reg: 0.2077 (0.2690)  loss_objectness: 0.0240 (0.1631)  loss_rpn_box_reg: 0.0148 (0.0262)  time: 0.6241  data: 0.0109  max mem: 5872\n",
            "Epoch: [0]  [ 70/106]  eta: 0:00:22  lr: 0.003383  loss: 0.5294 (0.9972)  loss_classifier: 0.1898 (0.5187)  loss_box_reg: 0.2077 (0.2764)  loss_objectness: 0.0785 (0.1756)  loss_rpn_box_reg: 0.0203 (0.0265)  time: 0.6155  data: 0.0110  max mem: 5872\n",
            "Epoch: [0]  [ 80/106]  eta: 0:00:16  lr: 0.003858  loss: 0.7494 (0.9648)  loss_classifier: 0.2124 (0.4846)  loss_box_reg: 0.3092 (0.2842)  loss_objectness: 0.1182 (0.1689)  loss_rpn_box_reg: 0.0203 (0.0271)  time: 0.6336  data: 0.0111  max mem: 5872\n",
            "Epoch: [0]  [ 90/106]  eta: 0:00:10  lr: 0.004334  loss: 0.7245 (0.9457)  loss_classifier: 0.2101 (0.4536)  loss_box_reg: 0.3176 (0.2885)  loss_objectness: 0.1333 (0.1759)  loss_rpn_box_reg: 0.0159 (0.0276)  time: 0.6349  data: 0.0110  max mem: 5872\n",
            "Epoch: [0]  [100/106]  eta: 0:00:03  lr: 0.004810  loss: 0.6390 (0.9167)  loss_classifier: 0.1927 (0.4282)  loss_box_reg: 0.3309 (0.2957)  loss_objectness: 0.0604 (0.1662)  loss_rpn_box_reg: 0.0153 (0.0266)  time: 0.5894  data: 0.0111  max mem: 5872\n",
            "Epoch: [0]  [105/106]  eta: 0:00:00  lr: 0.005000  loss: 0.6390 (0.9112)  loss_classifier: 0.1927 (0.4174)  loss_box_reg: 0.3139 (0.2994)  loss_objectness: 0.0604 (0.1676)  loss_rpn_box_reg: 0.0163 (0.0268)  time: 0.5693  data: 0.0110  max mem: 5872\n",
            "Epoch: [0] Total time: 0:01:05 (0.6185 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/14]  eta: 0:00:25  model_time: 1.4650 (1.4650)  evaluator_time: 0.0238 (0.0238)  time: 1.7892  data: 0.2968  max mem: 5872\n",
            "Test:  [13/14]  eta: 0:00:01  model_time: 1.2410 (1.2386)  evaluator_time: 0.0116 (0.0130)  time: 1.2911  data: 0.0367  max mem: 5872\n",
            "Test: Total time: 0:00:18 (1.2974 s / it)\n",
            "Averaged stats: model_time: 1.2410 (1.2386)  evaluator_time: 0.0116 (0.0130)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.052\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.150\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.003\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.002\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.070\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.044\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.060\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.107\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.112\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.015\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.138\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.167\n",
            "Epoch: [1]  [  0/106]  eta: 0:01:19  lr: 0.005000  loss: 0.9101 (0.9101)  loss_classifier: 0.1873 (0.1873)  loss_box_reg: 0.2125 (0.2125)  loss_objectness: 0.4796 (0.4796)  loss_rpn_box_reg: 0.0307 (0.0307)  time: 0.7456  data: 0.2253  max mem: 5872\n",
            "Epoch: [1]  [ 10/106]  eta: 0:01:00  lr: 0.005000  loss: 0.6633 (0.6316)  loss_classifier: 0.1670 (0.1558)  loss_box_reg: 0.2733 (0.2931)  loss_objectness: 0.1464 (0.1564)  loss_rpn_box_reg: 0.0162 (0.0263)  time: 0.6280  data: 0.0304  max mem: 5872\n",
            "Epoch: [1]  [ 20/106]  eta: 0:00:56  lr: 0.005000  loss: 0.5541 (0.6686)  loss_classifier: 0.1670 (0.1784)  loss_box_reg: 0.3092 (0.3094)  loss_objectness: 0.0550 (0.1589)  loss_rpn_box_reg: 0.0134 (0.0220)  time: 0.6483  data: 0.0108  max mem: 5872\n",
            "Epoch: [1]  [ 30/106]  eta: 0:00:48  lr: 0.005000  loss: 0.6177 (0.6952)  loss_classifier: 0.1859 (0.1837)  loss_box_reg: 0.3221 (0.3305)  loss_objectness: 0.0550 (0.1589)  loss_rpn_box_reg: 0.0146 (0.0222)  time: 0.6407  data: 0.0106  max mem: 5872\n",
            "Epoch: [1]  [ 40/106]  eta: 0:00:41  lr: 0.005000  loss: 0.7442 (0.7198)  loss_classifier: 0.1859 (0.1862)  loss_box_reg: 0.4305 (0.3521)  loss_objectness: 0.1074 (0.1556)  loss_rpn_box_reg: 0.0194 (0.0259)  time: 0.5919  data: 0.0108  max mem: 5872\n",
            "Epoch: [1]  [ 50/106]  eta: 0:00:34  lr: 0.005000  loss: 0.8379 (0.7538)  loss_classifier: 0.1883 (0.1915)  loss_box_reg: 0.4360 (0.3709)  loss_objectness: 0.0811 (0.1652)  loss_rpn_box_reg: 0.0141 (0.0262)  time: 0.5846  data: 0.0116  max mem: 5872\n",
            "Epoch: [1]  [ 60/106]  eta: 0:00:28  lr: 0.005000  loss: 0.6734 (0.7471)  loss_classifier: 0.1964 (0.1976)  loss_box_reg: 0.4090 (0.3738)  loss_objectness: 0.0548 (0.1508)  loss_rpn_box_reg: 0.0099 (0.0250)  time: 0.5963  data: 0.0119  max mem: 5872\n",
            "Epoch: [1]  [ 70/106]  eta: 0:00:22  lr: 0.005000  loss: 0.6734 (0.7478)  loss_classifier: 0.1884 (0.1976)  loss_box_reg: 0.3566 (0.3680)  loss_objectness: 0.0893 (0.1547)  loss_rpn_box_reg: 0.0134 (0.0276)  time: 0.6229  data: 0.0116  max mem: 5872\n",
            "Epoch: [1]  [ 80/106]  eta: 0:00:15  lr: 0.005000  loss: 0.7519 (0.7623)  loss_classifier: 0.1767 (0.1963)  loss_box_reg: 0.2880 (0.3590)  loss_objectness: 0.1352 (0.1784)  loss_rpn_box_reg: 0.0280 (0.0286)  time: 0.6163  data: 0.0114  max mem: 5872\n",
            "Epoch: [1]  [ 90/106]  eta: 0:00:09  lr: 0.005000  loss: 0.6214 (0.7488)  loss_classifier: 0.1692 (0.1933)  loss_box_reg: 0.2546 (0.3544)  loss_objectness: 0.1124 (0.1724)  loss_rpn_box_reg: 0.0224 (0.0286)  time: 0.6065  data: 0.0109  max mem: 5872\n",
            "Epoch: [1]  [100/106]  eta: 0:00:03  lr: 0.005000  loss: 0.6214 (0.7436)  loss_classifier: 0.1505 (0.1906)  loss_box_reg: 0.3421 (0.3582)  loss_objectness: 0.0348 (0.1671)  loss_rpn_box_reg: 0.0207 (0.0276)  time: 0.6007  data: 0.0109  max mem: 5872\n",
            "Epoch: [1]  [105/106]  eta: 0:00:00  lr: 0.005000  loss: 0.6214 (0.7340)  loss_classifier: 0.1486 (0.1886)  loss_box_reg: 0.3343 (0.3548)  loss_objectness: 0.0348 (0.1636)  loss_rpn_box_reg: 0.0161 (0.0271)  time: 0.5976  data: 0.0108  max mem: 5872\n",
            "Epoch: [1] Total time: 0:01:05 (0.6146 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/14]  eta: 0:00:24  model_time: 1.4848 (1.4848)  evaluator_time: 0.0218 (0.0218)  time: 1.7506  data: 0.2387  max mem: 5872\n",
            "Test:  [13/14]  eta: 0:00:01  model_time: 1.2838 (1.2617)  evaluator_time: 0.0122 (0.0132)  time: 1.3129  data: 0.0350  max mem: 5872\n",
            "Test: Total time: 0:00:18 (1.3193 s / it)\n",
            "Averaged stats: model_time: 1.2838 (1.2617)  evaluator_time: 0.0122 (0.0132)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.127\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.313\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.096\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.011\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.129\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.134\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.123\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.210\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.201\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.246\n",
            "Epoch: [2]  [  0/106]  eta: 0:01:37  lr: 0.005000  loss: 0.4951 (0.4951)  loss_classifier: 0.2034 (0.2034)  loss_box_reg: 0.2511 (0.2511)  loss_objectness: 0.0181 (0.0181)  loss_rpn_box_reg: 0.0225 (0.0225)  time: 0.9208  data: 0.2774  max mem: 5872\n",
            "Epoch: [2]  [ 10/106]  eta: 0:00:59  lr: 0.005000  loss: 0.4951 (0.6403)  loss_classifier: 0.1439 (0.1666)  loss_box_reg: 0.2614 (0.3440)  loss_objectness: 0.0276 (0.0998)  loss_rpn_box_reg: 0.0116 (0.0299)  time: 0.6168  data: 0.0325  max mem: 5872\n",
            "Epoch: [2]  [ 20/106]  eta: 0:00:52  lr: 0.005000  loss: 0.4964 (0.6212)  loss_classifier: 0.1417 (0.1639)  loss_box_reg: 0.3022 (0.3244)  loss_objectness: 0.0377 (0.1080)  loss_rpn_box_reg: 0.0114 (0.0249)  time: 0.5928  data: 0.0097  max mem: 5872\n",
            "Epoch: [2]  [ 30/106]  eta: 0:00:45  lr: 0.005000  loss: 0.7352 (0.7313)  loss_classifier: 0.1515 (0.1678)  loss_box_reg: 0.3055 (0.3367)  loss_objectness: 0.1234 (0.1919)  loss_rpn_box_reg: 0.0144 (0.0348)  time: 0.5913  data: 0.0116  max mem: 5872\n",
            "Epoch: [2]  [ 40/106]  eta: 0:00:40  lr: 0.005000  loss: 0.7024 (0.7085)  loss_classifier: 0.1534 (0.1644)  loss_box_reg: 0.3217 (0.3341)  loss_objectness: 0.0657 (0.1761)  loss_rpn_box_reg: 0.0173 (0.0339)  time: 0.6122  data: 0.0112  max mem: 5872\n",
            "Epoch: [2]  [ 50/106]  eta: 0:00:34  lr: 0.005000  loss: 0.5543 (0.6856)  loss_classifier: 0.1591 (0.1676)  loss_box_reg: 0.3164 (0.3339)  loss_objectness: 0.0616 (0.1539)  loss_rpn_box_reg: 0.0119 (0.0303)  time: 0.6392  data: 0.0109  max mem: 5872\n",
            "Epoch: [2]  [ 60/106]  eta: 0:00:28  lr: 0.005000  loss: 0.5807 (0.6722)  loss_classifier: 0.1744 (0.1701)  loss_box_reg: 0.3164 (0.3322)  loss_objectness: 0.0577 (0.1415)  loss_rpn_box_reg: 0.0120 (0.0285)  time: 0.6327  data: 0.0115  max mem: 5872\n",
            "Epoch: [2]  [ 70/106]  eta: 0:00:22  lr: 0.005000  loss: 0.5618 (0.6715)  loss_classifier: 0.1647 (0.1668)  loss_box_reg: 0.2980 (0.3319)  loss_objectness: 0.0577 (0.1449)  loss_rpn_box_reg: 0.0178 (0.0280)  time: 0.6082  data: 0.0113  max mem: 5872\n",
            "Epoch: [2]  [ 80/106]  eta: 0:00:15  lr: 0.005000  loss: 0.5618 (0.6765)  loss_classifier: 0.1647 (0.1685)  loss_box_reg: 0.3689 (0.3418)  loss_objectness: 0.0591 (0.1390)  loss_rpn_box_reg: 0.0184 (0.0272)  time: 0.5995  data: 0.0111  max mem: 5872\n",
            "Epoch: [2]  [ 90/106]  eta: 0:00:09  lr: 0.005000  loss: 0.6018 (0.6801)  loss_classifier: 0.1678 (0.1665)  loss_box_reg: 0.3129 (0.3341)  loss_objectness: 0.0945 (0.1525)  loss_rpn_box_reg: 0.0176 (0.0270)  time: 0.6298  data: 0.0115  max mem: 5872\n",
            "Epoch: [2]  [100/106]  eta: 0:00:03  lr: 0.005000  loss: 0.5711 (0.6704)  loss_classifier: 0.1642 (0.1669)  loss_box_reg: 0.2612 (0.3264)  loss_objectness: 0.1301 (0.1507)  loss_rpn_box_reg: 0.0175 (0.0264)  time: 0.6322  data: 0.0118  max mem: 5872\n",
            "Epoch: [2]  [105/106]  eta: 0:00:00  lr: 0.005000  loss: 0.5293 (0.6861)  loss_classifier: 0.1642 (0.1673)  loss_box_reg: 0.2384 (0.3241)  loss_objectness: 0.0948 (0.1681)  loss_rpn_box_reg: 0.0151 (0.0266)  time: 0.6269  data: 0.0117  max mem: 5872\n",
            "Epoch: [2] Total time: 0:01:05 (0.6190 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/14]  eta: 0:00:24  model_time: 1.4861 (1.4861)  evaluator_time: 0.0303 (0.0303)  time: 1.7311  data: 0.2104  max mem: 5872\n",
            "Test:  [13/14]  eta: 0:00:01  model_time: 1.2712 (1.2598)  evaluator_time: 0.0138 (0.0160)  time: 1.3125  data: 0.0338  max mem: 5872\n",
            "Test: Total time: 0:00:18 (1.3188 s / it)\n",
            "Averaged stats: model_time: 1.2712 (1.2598)  evaluator_time: 0.0138 (0.0160)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.07s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.130\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.328\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.073\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.013\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.158\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.109\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.142\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.245\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.249\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.046\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.238\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.299\n",
            "Epoch: [3]  [  0/106]  eta: 0:01:42  lr: 0.000500  loss: 1.1092 (1.1092)  loss_classifier: 0.1265 (0.1265)  loss_box_reg: 0.1534 (0.1534)  loss_objectness: 0.6792 (0.6792)  loss_rpn_box_reg: 0.1502 (0.1502)  time: 0.9659  data: 0.1864  max mem: 5872\n",
            "Epoch: [3]  [ 10/106]  eta: 0:01:00  lr: 0.000500  loss: 0.4604 (0.5356)  loss_classifier: 0.1413 (0.1462)  loss_box_reg: 0.1828 (0.2082)  loss_objectness: 0.0779 (0.1541)  loss_rpn_box_reg: 0.0123 (0.0270)  time: 0.6341  data: 0.0265  max mem: 5872\n",
            "Epoch: [3]  [ 20/106]  eta: 0:00:53  lr: 0.000500  loss: 0.4604 (0.5410)  loss_classifier: 0.1413 (0.1515)  loss_box_reg: 0.1961 (0.2070)  loss_objectness: 0.0715 (0.1563)  loss_rpn_box_reg: 0.0163 (0.0262)  time: 0.6078  data: 0.0105  max mem: 5872\n",
            "Epoch: [3]  [ 30/106]  eta: 0:00:46  lr: 0.000500  loss: 0.4806 (0.5452)  loss_classifier: 0.1637 (0.1601)  loss_box_reg: 0.1793 (0.1992)  loss_objectness: 0.0581 (0.1590)  loss_rpn_box_reg: 0.0211 (0.0269)  time: 0.6082  data: 0.0106  max mem: 5872\n",
            "Epoch: [3]  [ 40/106]  eta: 0:00:40  lr: 0.000500  loss: 0.3949 (0.5779)  loss_classifier: 0.1426 (0.1548)  loss_box_reg: 0.1549 (0.1928)  loss_objectness: 0.0581 (0.2023)  loss_rpn_box_reg: 0.0161 (0.0280)  time: 0.6093  data: 0.0112  max mem: 5872\n",
            "Epoch: [3]  [ 50/106]  eta: 0:00:34  lr: 0.000500  loss: 0.4531 (0.5713)  loss_classifier: 0.1426 (0.1546)  loss_box_reg: 0.1668 (0.1921)  loss_objectness: 0.0906 (0.1936)  loss_rpn_box_reg: 0.0148 (0.0311)  time: 0.6126  data: 0.0114  max mem: 5872\n",
            "Epoch: [3]  [ 60/106]  eta: 0:00:28  lr: 0.000500  loss: 0.4465 (0.5613)  loss_classifier: 0.1482 (0.1531)  loss_box_reg: 0.1574 (0.1846)  loss_objectness: 0.0906 (0.1924)  loss_rpn_box_reg: 0.0154 (0.0312)  time: 0.5921  data: 0.0110  max mem: 5872\n",
            "Epoch: [3]  [ 70/106]  eta: 0:00:21  lr: 0.000500  loss: 0.3873 (0.5477)  loss_classifier: 0.1346 (0.1504)  loss_box_reg: 0.1436 (0.1813)  loss_objectness: 0.0960 (0.1871)  loss_rpn_box_reg: 0.0151 (0.0289)  time: 0.5943  data: 0.0109  max mem: 5872\n",
            "Epoch: [3]  [ 80/106]  eta: 0:00:15  lr: 0.000500  loss: 0.3664 (0.5399)  loss_classifier: 0.1408 (0.1515)  loss_box_reg: 0.1495 (0.1806)  loss_objectness: 0.0598 (0.1795)  loss_rpn_box_reg: 0.0131 (0.0283)  time: 0.6120  data: 0.0111  max mem: 5872\n",
            "Epoch: [3]  [ 90/106]  eta: 0:00:09  lr: 0.000500  loss: 0.4232 (0.5289)  loss_classifier: 0.1439 (0.1519)  loss_box_reg: 0.1388 (0.1795)  loss_objectness: 0.0371 (0.1688)  loss_rpn_box_reg: 0.0198 (0.0286)  time: 0.5988  data: 0.0114  max mem: 5872\n",
            "Epoch: [3]  [100/106]  eta: 0:00:03  lr: 0.000500  loss: 0.3999 (0.5243)  loss_classifier: 0.1219 (0.1509)  loss_box_reg: 0.1310 (0.1758)  loss_objectness: 0.0498 (0.1700)  loss_rpn_box_reg: 0.0166 (0.0276)  time: 0.6021  data: 0.0119  max mem: 5872\n",
            "Epoch: [3]  [105/106]  eta: 0:00:00  lr: 0.000500  loss: 0.3337 (0.5174)  loss_classifier: 0.1182 (0.1500)  loss_box_reg: 0.1212 (0.1745)  loss_objectness: 0.0367 (0.1660)  loss_rpn_box_reg: 0.0131 (0.0269)  time: 0.6135  data: 0.0120  max mem: 5872\n",
            "Epoch: [3] Total time: 0:01:04 (0.6117 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/14]  eta: 0:00:25  model_time: 1.4693 (1.4693)  evaluator_time: 0.0941 (0.0941)  time: 1.8488  data: 0.2817  max mem: 5872\n",
            "Test:  [13/14]  eta: 0:00:01  model_time: 1.2759 (1.2646)  evaluator_time: 0.0125 (0.0182)  time: 1.3224  data: 0.0368  max mem: 5872\n",
            "Test: Total time: 0:00:18 (1.3292 s / it)\n",
            "Averaged stats: model_time: 1.2759 (1.2646)  evaluator_time: 0.0125 (0.0182)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.05s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.202\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.391\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.159\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.219\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.192\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.197\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.309\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.313\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.114\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.297\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.368\n",
            "Epoch: [4]  [  0/106]  eta: 0:01:37  lr: 0.000500  loss: 0.1963 (0.1963)  loss_classifier: 0.0930 (0.0930)  loss_box_reg: 0.0893 (0.0893)  loss_objectness: 0.0033 (0.0033)  loss_rpn_box_reg: 0.0108 (0.0108)  time: 0.9184  data: 0.2039  max mem: 5872\n",
            "Epoch: [4]  [ 10/106]  eta: 0:00:58  lr: 0.000500  loss: 0.4030 (0.4719)  loss_classifier: 0.1390 (0.1417)  loss_box_reg: 0.1755 (0.1703)  loss_objectness: 0.0321 (0.1216)  loss_rpn_box_reg: 0.0151 (0.0384)  time: 0.6060  data: 0.0284  max mem: 5872\n",
            "Epoch: [4]  [ 20/106]  eta: 0:00:54  lr: 0.000500  loss: 0.4030 (0.4453)  loss_classifier: 0.1364 (0.1371)  loss_box_reg: 0.1409 (0.1544)  loss_objectness: 0.0417 (0.1223)  loss_rpn_box_reg: 0.0128 (0.0315)  time: 0.6241  data: 0.0109  max mem: 5872\n",
            "Epoch: [4]  [ 30/106]  eta: 0:00:47  lr: 0.000500  loss: 0.3831 (0.4759)  loss_classifier: 0.1350 (0.1363)  loss_box_reg: 0.1310 (0.1507)  loss_objectness: 0.0776 (0.1598)  loss_rpn_box_reg: 0.0120 (0.0291)  time: 0.6439  data: 0.0108  max mem: 5872\n",
            "Epoch: [4]  [ 40/106]  eta: 0:00:41  lr: 0.000500  loss: 0.3663 (0.4437)  loss_classifier: 0.1224 (0.1313)  loss_box_reg: 0.1182 (0.1426)  loss_objectness: 0.0642 (0.1408)  loss_rpn_box_reg: 0.0158 (0.0290)  time: 0.6225  data: 0.0109  max mem: 5872\n",
            "Epoch: [4]  [ 50/106]  eta: 0:00:35  lr: 0.000500  loss: 0.4131 (0.4529)  loss_classifier: 0.1109 (0.1323)  loss_box_reg: 0.1213 (0.1458)  loss_objectness: 0.0885 (0.1457)  loss_rpn_box_reg: 0.0165 (0.0291)  time: 0.6242  data: 0.0109  max mem: 5872\n",
            "Epoch: [4]  [ 60/106]  eta: 0:00:28  lr: 0.000500  loss: 0.4540 (0.4523)  loss_classifier: 0.1499 (0.1354)  loss_box_reg: 0.1441 (0.1451)  loss_objectness: 0.0900 (0.1437)  loss_rpn_box_reg: 0.0185 (0.0281)  time: 0.6223  data: 0.0111  max mem: 5872\n",
            "Epoch: [4]  [ 70/106]  eta: 0:00:22  lr: 0.000500  loss: 0.4200 (0.4718)  loss_classifier: 0.1508 (0.1399)  loss_box_reg: 0.1502 (0.1478)  loss_objectness: 0.0931 (0.1564)  loss_rpn_box_reg: 0.0178 (0.0276)  time: 0.5822  data: 0.0115  max mem: 5872\n",
            "Epoch: [4]  [ 80/106]  eta: 0:00:15  lr: 0.000500  loss: 0.4200 (0.4688)  loss_classifier: 0.1487 (0.1397)  loss_box_reg: 0.1329 (0.1455)  loss_objectness: 0.0931 (0.1546)  loss_rpn_box_reg: 0.0154 (0.0289)  time: 0.5591  data: 0.0112  max mem: 5872\n",
            "Epoch: [4]  [ 90/106]  eta: 0:00:09  lr: 0.000500  loss: 0.4460 (0.4837)  loss_classifier: 0.1323 (0.1424)  loss_box_reg: 0.1235 (0.1478)  loss_objectness: 0.0570 (0.1652)  loss_rpn_box_reg: 0.0172 (0.0283)  time: 0.5923  data: 0.0110  max mem: 5872\n",
            "Epoch: [4]  [100/106]  eta: 0:00:03  lr: 0.000500  loss: 0.4298 (0.4920)  loss_classifier: 0.1446 (0.1437)  loss_box_reg: 0.1593 (0.1477)  loss_objectness: 0.0588 (0.1731)  loss_rpn_box_reg: 0.0174 (0.0276)  time: 0.6133  data: 0.0110  max mem: 5872\n",
            "Epoch: [4]  [105/106]  eta: 0:00:00  lr: 0.000500  loss: 0.4088 (0.4821)  loss_classifier: 0.1346 (0.1431)  loss_box_reg: 0.1400 (0.1462)  loss_objectness: 0.0418 (0.1661)  loss_rpn_box_reg: 0.0170 (0.0268)  time: 0.6242  data: 0.0107  max mem: 5872\n",
            "Epoch: [4] Total time: 0:01:05 (0.6162 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/14]  eta: 0:00:24  model_time: 1.4913 (1.4913)  evaluator_time: 0.0243 (0.0243)  time: 1.7203  data: 0.1982  max mem: 5872\n",
            "Test:  [13/14]  eta: 0:00:01  model_time: 1.2662 (1.2618)  evaluator_time: 0.0124 (0.0136)  time: 1.3116  data: 0.0330  max mem: 5872\n",
            "Test: Total time: 0:00:18 (1.3180 s / it)\n",
            "Averaged stats: model_time: 1.2662 (1.2618)  evaluator_time: 0.0124 (0.0136)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.215\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.394\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.226\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.218\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.214\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.332\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.339\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.141\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.310\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.423\n",
            "Epoch: [5]  [  0/106]  eta: 0:01:23  lr: 0.000500  loss: 0.9002 (0.9002)  loss_classifier: 0.1784 (0.1784)  loss_box_reg: 0.1655 (0.1655)  loss_objectness: 0.5324 (0.5324)  loss_rpn_box_reg: 0.0239 (0.0239)  time: 0.7870  data: 0.2446  max mem: 5872\n",
            "Epoch: [5]  [ 10/106]  eta: 0:00:55  lr: 0.000500  loss: 0.4903 (0.5433)  loss_classifier: 0.1512 (0.1559)  loss_box_reg: 0.1411 (0.1465)  loss_objectness: 0.1084 (0.2094)  loss_rpn_box_reg: 0.0227 (0.0314)  time: 0.5829  data: 0.0321  max mem: 5872\n",
            "Epoch: [5]  [ 20/106]  eta: 0:00:49  lr: 0.000500  loss: 0.4391 (0.5109)  loss_classifier: 0.1481 (0.1542)  loss_box_reg: 0.1404 (0.1500)  loss_objectness: 0.0934 (0.1770)  loss_rpn_box_reg: 0.0177 (0.0297)  time: 0.5649  data: 0.0110  max mem: 5872\n",
            "Epoch: [5]  [ 30/106]  eta: 0:00:45  lr: 0.000500  loss: 0.3105 (0.4414)  loss_classifier: 0.1184 (0.1417)  loss_box_reg: 0.1161 (0.1375)  loss_objectness: 0.0304 (0.1346)  loss_rpn_box_reg: 0.0137 (0.0275)  time: 0.6127  data: 0.0110  max mem: 5872\n",
            "Epoch: [5]  [ 40/106]  eta: 0:00:39  lr: 0.000500  loss: 0.2886 (0.4770)  loss_classifier: 0.1238 (0.1462)  loss_box_reg: 0.1130 (0.1382)  loss_objectness: 0.0249 (0.1671)  loss_rpn_box_reg: 0.0137 (0.0254)  time: 0.6350  data: 0.0109  max mem: 5872\n",
            "Epoch: [5]  [ 50/106]  eta: 0:00:34  lr: 0.000500  loss: 0.3485 (0.4736)  loss_classifier: 0.1313 (0.1448)  loss_box_reg: 0.1203 (0.1384)  loss_objectness: 0.0769 (0.1645)  loss_rpn_box_reg: 0.0158 (0.0259)  time: 0.6288  data: 0.0107  max mem: 5872\n",
            "Epoch: [5]  [ 60/106]  eta: 0:00:28  lr: 0.000500  loss: 0.3485 (0.4648)  loss_classifier: 0.1497 (0.1454)  loss_box_reg: 0.1220 (0.1380)  loss_objectness: 0.0494 (0.1570)  loss_rpn_box_reg: 0.0151 (0.0244)  time: 0.6332  data: 0.0104  max mem: 5872\n",
            "Epoch: [5]  [ 70/106]  eta: 0:00:22  lr: 0.000500  loss: 0.3155 (0.4727)  loss_classifier: 0.1497 (0.1445)  loss_box_reg: 0.1259 (0.1383)  loss_objectness: 0.0199 (0.1664)  loss_rpn_box_reg: 0.0145 (0.0235)  time: 0.6272  data: 0.0105  max mem: 5872\n",
            "Epoch: [5]  [ 80/106]  eta: 0:00:16  lr: 0.000500  loss: 0.4234 (0.4751)  loss_classifier: 0.1258 (0.1437)  loss_box_reg: 0.1261 (0.1372)  loss_objectness: 0.0916 (0.1694)  loss_rpn_box_reg: 0.0142 (0.0248)  time: 0.6258  data: 0.0108  max mem: 5872\n",
            "Epoch: [5]  [ 90/106]  eta: 0:00:09  lr: 0.000500  loss: 0.4171 (0.4763)  loss_classifier: 0.1277 (0.1438)  loss_box_reg: 0.1306 (0.1389)  loss_objectness: 0.1032 (0.1671)  loss_rpn_box_reg: 0.0154 (0.0265)  time: 0.6071  data: 0.0109  max mem: 5872\n",
            "Epoch: [5]  [100/106]  eta: 0:00:03  lr: 0.000500  loss: 0.3674 (0.4715)  loss_classifier: 0.1303 (0.1443)  loss_box_reg: 0.1455 (0.1387)  loss_objectness: 0.0602 (0.1621)  loss_rpn_box_reg: 0.0149 (0.0264)  time: 0.6002  data: 0.0112  max mem: 5872\n",
            "Epoch: [5]  [105/106]  eta: 0:00:00  lr: 0.000500  loss: 0.3675 (0.4736)  loss_classifier: 0.1443 (0.1454)  loss_box_reg: 0.1523 (0.1398)  loss_objectness: 0.0484 (0.1622)  loss_rpn_box_reg: 0.0149 (0.0261)  time: 0.5890  data: 0.0111  max mem: 5872\n",
            "Epoch: [5] Total time: 0:01:04 (0.6132 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/14]  eta: 0:00:24  model_time: 1.4566 (1.4566)  evaluator_time: 0.0230 (0.0230)  time: 1.7531  data: 0.2541  max mem: 5872\n",
            "Test:  [13/14]  eta: 0:00:01  model_time: 1.2690 (1.2623)  evaluator_time: 0.0124 (0.0133)  time: 1.3163  data: 0.0366  max mem: 5872\n",
            "Test: Total time: 0:00:18 (1.3224 s / it)\n",
            "Averaged stats: model_time: 1.2690 (1.2623)  evaluator_time: 0.0124 (0.0133)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.200\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.400\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.171\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.025\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.205\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.202\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.203\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.319\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.323\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.129\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.290\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.396\n",
            "Epoch: [6]  [  0/106]  eta: 0:01:23  lr: 0.000050  loss: 0.3101 (0.3101)  loss_classifier: 0.1075 (0.1075)  loss_box_reg: 0.1065 (0.1065)  loss_objectness: 0.0424 (0.0424)  loss_rpn_box_reg: 0.0538 (0.0538)  time: 0.7905  data: 0.2186  max mem: 5872\n",
            "Epoch: [6]  [ 10/106]  eta: 0:00:59  lr: 0.000050  loss: 0.3121 (0.3615)  loss_classifier: 0.1344 (0.1321)  loss_box_reg: 0.1130 (0.1196)  loss_objectness: 0.0424 (0.0905)  loss_rpn_box_reg: 0.0169 (0.0193)  time: 0.6236  data: 0.0295  max mem: 5872\n",
            "Epoch: [6]  [ 20/106]  eta: 0:00:51  lr: 0.000050  loss: 0.4087 (0.4378)  loss_classifier: 0.1489 (0.1461)  loss_box_reg: 0.1302 (0.1302)  loss_objectness: 0.0705 (0.1415)  loss_rpn_box_reg: 0.0169 (0.0200)  time: 0.5838  data: 0.0108  max mem: 5872\n",
            "Epoch: [6]  [ 30/106]  eta: 0:00:45  lr: 0.000050  loss: 0.3610 (0.4050)  loss_classifier: 0.1524 (0.1407)  loss_box_reg: 0.1302 (0.1268)  loss_objectness: 0.0607 (0.1164)  loss_rpn_box_reg: 0.0145 (0.0212)  time: 0.5838  data: 0.0109  max mem: 5872\n",
            "Epoch: [6]  [ 40/106]  eta: 0:00:39  lr: 0.000050  loss: 0.3396 (0.4193)  loss_classifier: 0.1206 (0.1389)  loss_box_reg: 0.1217 (0.1245)  loss_objectness: 0.0717 (0.1342)  loss_rpn_box_reg: 0.0132 (0.0216)  time: 0.5882  data: 0.0108  max mem: 5872\n",
            "Epoch: [6]  [ 50/106]  eta: 0:00:33  lr: 0.000050  loss: 0.3802 (0.4234)  loss_classifier: 0.1223 (0.1379)  loss_box_reg: 0.1218 (0.1255)  loss_objectness: 0.1232 (0.1358)  loss_rpn_box_reg: 0.0188 (0.0242)  time: 0.6208  data: 0.0109  max mem: 5872\n",
            "Epoch: [6]  [ 60/106]  eta: 0:00:28  lr: 0.000050  loss: 0.3817 (0.4528)  loss_classifier: 0.1322 (0.1387)  loss_box_reg: 0.1199 (0.1287)  loss_objectness: 0.0888 (0.1589)  loss_rpn_box_reg: 0.0183 (0.0265)  time: 0.6544  data: 0.0109  max mem: 5872\n",
            "Epoch: [6]  [ 70/106]  eta: 0:00:22  lr: 0.000050  loss: 0.4136 (0.4493)  loss_classifier: 0.1493 (0.1402)  loss_box_reg: 0.1444 (0.1299)  loss_objectness: 0.0505 (0.1537)  loss_rpn_box_reg: 0.0150 (0.0255)  time: 0.6267  data: 0.0108  max mem: 5872\n",
            "Epoch: [6]  [ 80/106]  eta: 0:00:15  lr: 0.000050  loss: 0.4013 (0.4496)  loss_classifier: 0.1539 (0.1418)  loss_box_reg: 0.1315 (0.1298)  loss_objectness: 0.0792 (0.1512)  loss_rpn_box_reg: 0.0154 (0.0268)  time: 0.6065  data: 0.0110  max mem: 5872\n",
            "Epoch: [6]  [ 90/106]  eta: 0:00:09  lr: 0.000050  loss: 0.4013 (0.4641)  loss_classifier: 0.1500 (0.1420)  loss_box_reg: 0.1253 (0.1307)  loss_objectness: 0.1145 (0.1653)  loss_rpn_box_reg: 0.0149 (0.0262)  time: 0.6106  data: 0.0110  max mem: 5872\n",
            "Epoch: [6]  [100/106]  eta: 0:00:03  lr: 0.000050  loss: 0.4058 (0.4653)  loss_classifier: 0.1460 (0.1421)  loss_box_reg: 0.1392 (0.1322)  loss_objectness: 0.0668 (0.1642)  loss_rpn_box_reg: 0.0161 (0.0268)  time: 0.6071  data: 0.0108  max mem: 5872\n",
            "Epoch: [6]  [105/106]  eta: 0:00:00  lr: 0.000050  loss: 0.4086 (0.4646)  loss_classifier: 0.1568 (0.1432)  loss_box_reg: 0.1392 (0.1320)  loss_objectness: 0.0438 (0.1629)  loss_rpn_box_reg: 0.0161 (0.0264)  time: 0.5975  data: 0.0107  max mem: 5872\n",
            "Epoch: [6] Total time: 0:01:04 (0.6113 s / it)\n",
            "creating index...\n",
            "index created!\n",
            "Test:  [ 0/14]  eta: 0:00:24  model_time: 1.4825 (1.4825)  evaluator_time: 0.0227 (0.0227)  time: 1.7345  data: 0.2252  max mem: 5872\n",
            "Test:  [13/14]  eta: 0:00:01  model_time: 1.2791 (1.2675)  evaluator_time: 0.0130 (0.0140)  time: 1.3190  data: 0.0344  max mem: 5872\n",
            "Test: Total time: 0:00:18 (1.3253 s / it)\n",
            "Averaged stats: model_time: 1.2791 (1.2675)  evaluator_time: 0.0130 (0.0140)\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.06s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.229\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.406\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.212\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.021\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.233\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.236\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.223\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.346\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.351\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.142\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.319\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.434\n",
            "Epoch: [7]  [  0/106]  eta: 0:01:25  lr: 0.000050  loss: 0.3320 (0.3320)  loss_classifier: 0.1472 (0.1472)  loss_box_reg: 0.1137 (0.1137)  loss_objectness: 0.0505 (0.0505)  loss_rpn_box_reg: 0.0206 (0.0206)  time: 0.8090  data: 0.2989  max mem: 5872\n",
            "Epoch: [7]  [ 10/106]  eta: 0:01:04  lr: 0.000050  loss: 0.3320 (0.3662)  loss_classifier: 0.1092 (0.1130)  loss_box_reg: 0.1008 (0.0983)  loss_objectness: 0.0573 (0.1251)  loss_rpn_box_reg: 0.0167 (0.0297)  time: 0.6707  data: 0.0360  max mem: 5872\n",
            "Epoch: [7]  [ 20/106]  eta: 0:00:53  lr: 0.000050  loss: 0.3522 (0.4059)  loss_classifier: 0.1092 (0.1188)  loss_box_reg: 0.1053 (0.1107)  loss_objectness: 0.1175 (0.1434)  loss_rpn_box_reg: 0.0167 (0.0331)  time: 0.6134  data: 0.0101  max mem: 5872\n",
            "Epoch: [7]  [ 30/106]  eta: 0:00:47  lr: 0.000050  loss: 0.3763 (0.4707)  loss_classifier: 0.1232 (0.1244)  loss_box_reg: 0.1094 (0.1151)  loss_objectness: 0.1475 (0.1965)  loss_rpn_box_reg: 0.0168 (0.0347)  time: 0.5974  data: 0.0107  max mem: 5872\n",
            "Epoch: [7]  [ 40/106]  eta: 0:00:40  lr: 0.000050  loss: 0.4442 (0.4962)  loss_classifier: 0.1533 (0.1346)  loss_box_reg: 0.1127 (0.1205)  loss_objectness: 0.1859 (0.2064)  loss_rpn_box_reg: 0.0204 (0.0347)  time: 0.6052  data: 0.0107  max mem: 5872\n",
            "Epoch: [7]  [ 50/106]  eta: 0:00:34  lr: 0.000050  loss: 0.3527 (0.4701)  loss_classifier: 0.1456 (0.1350)  loss_box_reg: 0.1210 (0.1216)  loss_objectness: 0.0464 (0.1819)  loss_rpn_box_reg: 0.0204 (0.0316)  time: 0.5989  data: 0.0110  max mem: 5872\n",
            "Epoch: [7]  [ 60/106]  eta: 0:00:28  lr: 0.000050  loss: 0.3527 (0.4687)  loss_classifier: 0.1345 (0.1362)  loss_box_reg: 0.1310 (0.1235)  loss_objectness: 0.0410 (0.1795)  loss_rpn_box_reg: 0.0122 (0.0295)  time: 0.6120  data: 0.0112  max mem: 5872\n",
            "Epoch: [7]  [ 70/106]  eta: 0:00:22  lr: 0.000050  loss: 0.3859 (0.4676)  loss_classifier: 0.1449 (0.1392)  loss_box_reg: 0.1396 (0.1259)  loss_objectness: 0.0759 (0.1729)  loss_rpn_box_reg: 0.0122 (0.0297)  time: 0.6247  data: 0.0110  max mem: 5872\n",
            "Epoch: [7]  [ 80/106]  eta: 0:00:16  lr: 0.000050  loss: 0.3634 (0.4708)  loss_classifier: 0.1567 (0.1437)  loss_box_reg: 0.1229 (0.1296)  loss_objectness: 0.0595 (0.1687)  loss_rpn_box_reg: 0.0166 (0.0287)  time: 0.6244  data: 0.0108  max mem: 5872\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1bb48150c9b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_bbox_deltas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manchor_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mnum_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image_list, feature_maps)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 123\u001b[0;31m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/anchor_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_maps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         strides = [[torch.tensor(image_size[0] // g[0], dtype=torch.int64, device=device),\n\u001b[0;32m--> 123\u001b[0;31m                     torch.tensor(image_size[1] // g[1], dtype=torch.int64, device=device)] for g in grid_sizes]\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cell_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0manchors_over_all_feature_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_anchors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "ZzIT8EqWqrHA",
        "outputId": "84781716-a841-4614-e057-200185687ef8"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.retinanet import RetinaNet\n",
        "\n",
        "\n",
        "# define training and validation data loaders\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=4, shuffle=True, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=8, shuffle=False, num_workers=4,\n",
        "    collate_fn=utils.collate_fn)\n",
        "\n",
        "model = torch.load('model4.pth')\n",
        "model.cuda()\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "    # evaluate on the test dataset\n",
        "    evaluate(model, data_loader_test, device=device)\n",
        "\n",
        "    torch.save(model, 'model' +str(epoch)+ '.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-f3dd766b9c56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     91\u001b[0m                                      .format(degen_bb, target_idx))\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/backbone_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ]
    }
  ]
}