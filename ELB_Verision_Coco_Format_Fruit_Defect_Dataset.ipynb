{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ELB Verision: Coco Format Fruit Defect Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZfnWC_NF7NM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch \n",
        "from torch import nn, optim \n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "import torch.utils.data\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import itertools\n",
        "import glob \n",
        "from PIL import Image\n",
        "import csv \n",
        "import cv2\n",
        "import re\n",
        "!pip install timm\n",
        "!pip install effdet\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "import json\n",
        "import yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LoGlMtQY77w"
      },
      "source": [
        "Link to COCO format tutorial\n",
        "\n",
        "https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4UKDVNnIqQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45ec917-2376-4c2d-e1d3-f8347c350e0f"
      },
      "source": [
        "import zipfile\n",
        "\n",
        "!git clone https://github.com/SarthakJaingit/Fruits-and-Spot-Detection.git\n",
        "%cd \"Fruits-and-Spot-Detection\"\n",
        "with zipfile.ZipFile(\"/content/Fruits-and-Spot-Detection/LatestFruit Defects Dataset .zip\", 'r') as zip_ref:\n",
        "     zip_ref.extractall('/content/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Fruits-and-Spot-Detection'...\n",
            "remote: Enumerating objects: 267, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (71/71), done.\u001b[K\n",
            "remote: Total 267 (delta 34), reused 0 (delta 0), pack-reused 196\u001b[K\n",
            "Receiving objects: 100% (267/267), 34.02 MiB | 29.40 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n",
            "/content/Fruits-and-Spot-Detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY83cUEFAjoW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "d6455219-b34e-4190-d658-6d1f2136239e"
      },
      "source": [
        "#For one strawberry batch please drop watermark rows\n",
        "strawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 3 Labeled/FreshStrawberryBatch3Labels.csv\", header = None)\n",
        "strawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 2 Labeled/FreshStrawberriesBatch2Labels.csv\", header = None)\n",
        "strawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 1 Labeled/Strawberrybatch1.csv\", header = None)\n",
        "rottenApple_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch1Labeled/RottenAppleBatch1Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch2Labeled/RottenApplesBatch2Labels.csv\", header = None)\n",
        "rottenApple_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch3Labaled/RottenApplesBatch3Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch1RottenStrawBerryLabels/RottenStrawberriesBatch1Labels.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch2RottenStrawBerryLabels/RottenStrawBerryBatch2.csv\", header = None)\n",
        "rottenStrawberry_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/RottenStrawberries/Batch3RottenStrawberrylabel/rottenStrawberryBtch3labels.csv\", header = None)\n",
        "freshApples_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplebtch2label/FreshApplesBatch2LabelsFresh.csv\", header = None)\n",
        "freshApples_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/FreshApples/FreshApplesBatch1Labels/FreshAppleBatch1Labels.csv\", header = None)\n",
        "rottenTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/Rotten TomatoBatch1/Batch1TomoatosLabelsBbox.csv\", header = None)\n",
        "rottenTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottnTomatoBatch2/RottenTomatyoBatch2Labelss.csv\", header = None)\n",
        "rottenTomato_csv_batch_3 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatBtch3/RottenTomatoesBatch3Labssles.csv\", header = None)\n",
        "rottenTomato_csv_batch_4 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Tomatoes/RottenTomatoesBatch4/Tomatobatch4labelssRotten.csv\", header = None)\n",
        "freshTomato_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatoesBatch1Labelss/FreshTomatoesLabelsBatch1Labels.csv\", header = None)\n",
        "freshTomato_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Fresh Tomatoes/FreshTomatBatch2Labessls/Batch2TomatlabelsFresh.csv\", header = None)\n",
        "rottenPeach_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Peaches/Bctch1RottenPeachesLabels/RottenPeachesBatch1Labelks.csv\", header = None)\n",
        "rottenPeach_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Rotten Peaches/RottenPeachesBatch2lbeLabels/Batch2PeahcesRottenLabels.csv\", header = None)\n",
        "freshPeach_csv_batch_1 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Peaches/Batch1FreshPeaches/freshpeahcheslabelsbatch1.csv\", header = None)\n",
        "freshPeach_csv_batch_2 = pd.read_csv(\"/content/Fruit Defects Dataset /Train/Peaches/Batch2FreshPeache/Batch2FreshPeachesLabels.csv\", header = None)\n",
        "\n",
        "strawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "strawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenApple_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_1.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_2.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenStrawberry_csv_batch_3.columns = [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "freshApples_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_3.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "rottenTomato_csv_batch_4.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "#freshTomato_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "#freshTomato_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "#rottenPeach_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "#rottenPeach_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "#freshPeach_csv_batch_1.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "#freshPeach_csv_batch_2.columns =  [\"Fruit\", \"Coord1\", \"Coord2\", \"Coord3\", \"Coord4\", \"Image_id\", \"OneSize\", \"TwoSize\"]\n",
        "\n",
        "#Drop some watermark data for Fresh StrawBerry Batch 1 Labeled images [59, 9, 93]\n",
        "\n",
        "# strawberry_csv_batch_1 = strawberry_csv_batch_1[Image_id not in [\"FreshStrawberries59.jpeg, FreshStrawberries9.jpeg, FreshStrawberries93.jpeg\"]]\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries59.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries9.jpeg\"].index, inplace = True)\n",
        "strawberry_csv_batch_1.drop(strawberry_csv_batch_1[strawberry_csv_batch_1[\"Image_id\"] == \"FreshStrawberries93.jpeg\"].index, inplace = True)\n",
        "freshTomato_csv_batch_1.drop(freshTomato_csv_batch_1[freshTomato_csv_batch_1[\"Image_id\"] == \"Fresh Tomatoes66AddonPart1.jpeg\"].index, inplace = True)\n",
        "\n",
        "strawberry_csv_batch_1 = strawberry_csv_batch_1.reset_index(drop=True)\n",
        "freshTomato_csv_batch_1 = freshTomato_csv_batch_1.reset_index(drop = True)\n",
        "\n",
        "#Stack all the csv files together. \n",
        "list_of_all_dataframes = [strawberry_csv_batch_1, strawberry_csv_batch_2, strawberry_csv_batch_3, rottenApple_csv_batch_1, \n",
        "                          rottenApple_csv_batch_2, rottenApple_csv_batch_3, rottenStrawberry_csv_batch_1, rottenStrawberry_csv_batch_2, \n",
        "                          rottenStrawberry_csv_batch_3, freshApples_csv_batch_2, freshApples_csv_batch_1, rottenTomato_csv_batch_1, \n",
        "                          rottenTomato_csv_batch_2, rottenTomato_csv_batch_3, rottenTomato_csv_batch_4, freshTomato_csv_batch_1, \n",
        "                          freshTomato_csv_batch_2, rottenPeach_csv_batch_1, rottenPeach_csv_batch_2, freshPeach_csv_batch_1, freshPeach_csv_batch_2]\n",
        "fruit_df = pd.concat(list_of_all_dataframes, ignore_index = True)\n",
        "\n",
        "total_row_sum_check = 0 \n",
        "for dataframe in list_of_all_dataframes:\n",
        "  total_row_sum_check += dataframe.shape[0]\n",
        "print(\"Checked total rows from all the dataframes combined: {}\".format(total_row_sum_check))\n",
        "\n",
        "def run_dataframe_check(after_mis = True):\n",
        "  assert total_row_sum_check == fruit_df.shape[0]\n",
        "  if after_mis:\n",
        "    print(\"DataFrame shape: {}\".format(fruit_df.shape))\n",
        "    print(\"Unique Fruit Labels {}\".format(fruit_df[\"Fruit\"].unique()))\n",
        "    print(\"Number of Unique Images {}\".format(len(fruit_df[\"Image_id\"].unique())))\n",
        "\n",
        "run_dataframe_check(after_mis = False)\n",
        "\n",
        "#Specify more image types when \n",
        "def more_specific_Image_id(image_id, fruit):\n",
        "  if fruit == \"Bad_Spots\":\n",
        "    if re.search(\"RottenStrawberries\", image_id):\n",
        "      return \"Strawberry_Bad_Spot\"\n",
        "    elif re.search(\"RottenApples\", image_id):\n",
        "      return \"Apple_Bad_Spot\"\n",
        "    elif re.search(\"Rotten Tomatoes\", image_id):\n",
        "      return \"Tomato_Bad_Spot\"\n",
        "    elif re.search(\"Rotten Peaches\", image_id):\n",
        "      return \"Peaches_Bad_Spot\"\n",
        "    else:\n",
        "      raise ValueError(\"Could not find a match for some of the Image_ids\")\n",
        "\n",
        "  else:\n",
        "    return fruit\n",
        "\n",
        "def fix_fruitdf_mistakes(fruit):\n",
        "\n",
        "  if fruit == \"Peach \":\n",
        "    return \"Peach\"\n",
        "  else:\n",
        "    return fruit\n",
        "\n",
        "fruit_df[\"Fruit\"] = fruit_df.apply(lambda row: more_specific_Image_id(row.Image_id, row.Fruit), axis = 1)\n",
        "fruit_df[\"Fruit\"] = fruit_df.apply(lambda row: fix_fruitdf_mistakes(row.Fruit), axis = 1)\n",
        "\n",
        "run_dataframe_check()\n",
        "\n",
        "#Post Processing \n",
        "fruit_df = fruit_df[fruit_df[\"Image_id\"] != \"FreshStrawberries15.jpeg\"]\n",
        "\n",
        "bounding_box_dict = dict()\n",
        "labels_dict = dict()\n",
        "classes = [\"Apples\", \"Strawberry\", \"Apple_Bad_Spot\", \"Strawberry_Bad_Spot\"]\n",
        "print(classes)\n",
        "# classes = [\"Bad_Spots\", \"Strawberry\", \"Apples\"]\n",
        "\n",
        "for row_index in range(len(fruit_df)): \n",
        "  current_image_file = fruit_df.iloc[row_index][\"Image_id\"]\n",
        "  if current_image_file not in bounding_box_dict:\n",
        "    bounding_box_dict[current_image_file] = list()\n",
        "    labels_dict[current_image_file] = list()\n",
        "  bounding_box_dict[current_image_file].append(fruit_df.iloc[row_index, 1:5].to_list())\n",
        "  labels_dict[current_image_file].append(classes.index(fruit_df.iloc[row_index, 0]))\n",
        "\n",
        "print(len(bounding_box_dict))\n",
        "print(len(labels_dict))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-486e8e79ddc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#For one strawberry batch please drop watermark rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstrawberry_csv_batch_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 3 Labeled/FreshStrawberryBatch3Labels.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstrawberry_csv_batch_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 2 Labeled/FreshStrawberriesBatch2Labels.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstrawberry_csv_batch_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Fruit Defects Dataset /Train/FreshStrawberries/Fresh StrawBerry Batch 1 Labeled/Strawberrybatch1.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrottenApple_csv_batch_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Fruit Defects Dataset /Train/RottenApples/RottenAppleBatch1Labeled/RottenAppleBatch1Labels.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15qhrCwGUPxp"
      },
      "source": [
        "## Class function + util functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7KQVBKEfLwW"
      },
      "source": [
        "# COCO Bounding box: (x-top left, y-top left, width, height)\n",
        "# Pascal VOC Bounding box :(x-top left, y-top left,x-bottom right, y-bottom right)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNNEFRWhU6R6"
      },
      "source": [
        "def ffile_path(image_id, full_image_file_paths):\n",
        "  for image_path in full_image_file_paths:\n",
        "    if image_id in image_path:\n",
        "      return image_path\n",
        "\n",
        "def Coco_Create(labels_dict_fn,bounding_box_dict_fn,imgs_key_fn):\n",
        "  Coco_dict={}\n",
        "  images_full_list=[]\n",
        "  annotations_full_list = []\n",
        "  bbox_id=0\n",
        "\n",
        "  for index in range(len(bounding_box_dict_fn)):\n",
        "\n",
        "        sub_annotation_dict={}\n",
        "\n",
        "        image_key= imgs_key_fn[index]\n",
        "        img_path = ffile_path(image_key, full_image_file_paths) \n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        width, height = img.size\n",
        "\n",
        "        images_sub_dict ={}\n",
        "\n",
        "        images_sub_dict['file_name'] = image_key\n",
        "        images_sub_dict['id'] = index\n",
        "        images_sub_dict['height'] = height\n",
        "        images_sub_dict['width'] = width\n",
        "        images_full_list.append(images_sub_dict)\n",
        "        boxesxywh = bounding_box_dict_fn[image_key]\n",
        "\n",
        "        ann_nest_dict={}\n",
        "        labels = labels_dict_fn[image_key]\n",
        "\n",
        "        for i,box_xywh in enumerate(boxesxywh):\n",
        "          bbox_id+=1\n",
        "          \n",
        "          x1= box_xywh[0]\n",
        "          w= box_xywh[2]\n",
        "\n",
        "          y1= box_xywh[1]\n",
        "          h= box_xywh[2]\n",
        "\n",
        "          x2 = w + x1\n",
        "          y2 = h + y1 \n",
        "\n",
        "          boxes_xyxy = [x1,y1,x2,y2]\n",
        "          area = (y2 - y1) * (x2 - x1)\n",
        "\n",
        "          box_xywh = [float(box_xywh[0]),\n",
        "                      float(box_xywh[1]),\n",
        "                      float(box_xywh[2]),\n",
        "                      float(box_xywh[3])]\n",
        "\n",
        "          ann_nest_dict['bbox'] = box_xywh\n",
        "          ann_nest_dict['id'] = bbox_id\n",
        "          ann_nest_dict['image_id'] = index\n",
        "          ann_nest_dict['area'] = int(area)\n",
        "          ann_nest_dict['iscrowd'] = 0\n",
        "          ann_nest_dict['category_id'] = labels[i]+1\n",
        "          annotations_full_list.append(ann_nest_dict)\n",
        "\n",
        "  Coco_dict['info'] = {\n",
        "          \"year\": \"\",\n",
        "          \"version\": \"\",\n",
        "          \"description\": \"\",\n",
        "          \"contributor\": \"\",\n",
        "          \"url\": \"\",\n",
        "          \"date_created\": \"\"\n",
        "      }\n",
        "  Coco_dict['licenses'] = []\n",
        "  Coco_dict['annotations'] = annotations_full_list\n",
        "  Coco_dict['images'] = images_full_list\n",
        "  categories= [{ \"id\" : 1, \n",
        "              \"name\" : 'Apples', \n",
        "              \"supercategory\" : 'Fruit'},\n",
        "              \n",
        "              {\"id\" : 2, \n",
        "              \"name\" : 'Strawberry', \n",
        "              \"supercategory\" : 'Fruit'},\n",
        "\n",
        "            {\"id\" : 3, \n",
        "              \"name\" : 'Tomato', \n",
        "              \"supercategory\" : 'Fruit'},\n",
        "            {\"id\" : 4, \n",
        "              \"name\" : 'Apple_Bad_Spot', \n",
        "              \"supercategory\" : 'Fruit'},\n",
        "            {\"id\" : 5, \n",
        "              \"name\" : 'Strawberry_Bad_Spot', \n",
        "              \"supercategory\" : 'Fruit'},\n",
        "            {\"id\" : 6, \n",
        "              \"name\" : 'Tomato_Bad_Spot',\n",
        "              \"supercategory\" : 'Fruit'}]\n",
        "  Coco_dict['categories'] = categories\n",
        "  return Coco_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNzMO2kxW29F"
      },
      "source": [
        "# Visualize the Coco Dataset, just to make sure everything seems ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LGpEcwNWMXC"
      },
      "source": [
        "!pip install fiftyone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z59pS9C8bsq4"
      },
      "source": [
        "### For fiftyone package to visualize \n",
        "# This is where all the images get dumped into one directory\n",
        "# Meaning that the train and test images are not split into seperate \n",
        "## Below is how it wants the images\n",
        " #   <coco>/\n",
        " #   data/\n",
        " #       <filename0>.<ext>\n",
        " #       <filename1>.<ext>\n",
        " #       ...\n",
        " #   labels.json\n",
        "\n",
        "full_image_file_paths = glob.glob(\"/content/Fruit Defects Dataset /Train/*/*/*.jpeg\")\n",
        "imgs_key= sorted(bounding_box_dict.keys())\n",
        "\n",
        "# This is creating the coco json file, not just for the train set, but for all images\n",
        "Coco_dict_fiftyone= Coco_Create(labels_dict,bounding_box_dict,imgs_key)\n",
        "\n",
        "os.makedirs(\n",
        "  os.path.dirname(os.path.abspath('/content/coco/labels.json')), exist_ok=True\n",
        ")\n",
        "json.dump(Coco_dict_fiftyone, open('/content/coco/labels.json', \"w\"), indent=4)\n",
        "\n",
        "if not os.path.exists(\"/content/coco/data\"):\n",
        "    os.mkdir(\"/content/coco/data\")\n",
        "for imageName in full_image_file_paths:\n",
        "    shutil.copy(imageName, \"/content/coco/data\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqgXpdHkZ4jm"
      },
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "# A name for the dataset\n",
        "name = \"Fruit-Defect-Dataset\"\n",
        "\n",
        "# The directory containing the dataset to import\n",
        "dataset_dir = \"/content/coco/\"\n",
        "\n",
        "# The type of the dataset being imported\n",
        "# Any subclass of `fiftyone.types.Dataset` is supported\n",
        "dataset_type = fo.types.COCODetectionDataset  # for example\n",
        "\n",
        "dataset = fo.Dataset.from_dir(dataset_dir, dataset_type, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMcXQ9e_WjNX"
      },
      "source": [
        "# View summary info about the dataset\n",
        "print(dataset)\n",
        "\n",
        "# Print the first few samples in the dataset\n",
        "print(dataset.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrEyvG8--Gzp"
      },
      "source": [
        "session = fo.launch_app(dataset)\n",
        "\n",
        "# If the session says \"no dataset selected\", click the FiftyOne logo on the top right of the display that comes up below. \n",
        "# Thats seems to launch the dataset for viewing."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mqs3h0QYXA3p"
      },
      "source": [
        "## Train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYrr1LSqU7ge"
      },
      "source": [
        "### This is the format for the rwightman/efficientdet-pytorch package\n",
        " #   <cococ>/\n",
        " #   train2017/\n",
        " #       <filename0>.<ext>\n",
        " #       <filename1>.<ext>\n",
        " #       ...\n",
        " #   val2017/\n",
        " #       <filename0>.<ext>\n",
        " #       <filename1>.<ext>\n",
        " #       ...\n",
        " #   annotations/\n",
        " #       instances_train2017.json\n",
        " #       instances_val2017.json\n",
        "\n",
        "# Running the Coco_Create function for train and test set\n",
        "imgs_key= sorted(bounding_box_dict.keys())\n",
        "imgs_key_train = imgs_key[:int(len(imgs_key) * 0.8)]\n",
        "imgs_key_test = imgs_key[int(len(imgs_key) * 0.8):]\n",
        "\n",
        "labels_dict_train = {k: labels_dict[k] for k in imgs_key_train}\n",
        "bounding_box_dict_train = {k: bounding_box_dict[k] for k in imgs_key_train}\n",
        "Coco_dict_train= Coco_Create(labels_dict_train,bounding_box_dict_train,imgs_key_train)\n",
        "\n",
        "labels_dict_test = {k: labels_dict[k] for k in imgs_key_test}\n",
        "bounding_box_dict_test = {k: bounding_box_dict[k] for k in imgs_key_test}\n",
        "Coco_dict_test= Coco_Create(labels_dict_test,bounding_box_dict_test,imgs_key_test)\n",
        "\n",
        "save_json_path_train ='/content/coco/annotations/instances_train2017.json'\n",
        "save_json_path_val ='/content/coco/annotations/instances_val2017.json'\n",
        "\n",
        "os.makedirs(\n",
        "  os.path.dirname(os.path.abspath(save_json_path_train)), exist_ok=True\n",
        ")\n",
        "json.dump(Coco_dict_train, open(save_json_path_train, \"w\"), indent=4)\n",
        "\n",
        "# For the time being I made the test set, the train set -- for code testing purposes\n",
        "# In the future this should be changed to include a blinded dataset to the model\n",
        "os.makedirs(\n",
        "  os.path.dirname(os.path.abspath(save_json_path_val)), exist_ok=True\n",
        ")\n",
        "json.dump(Coco_dict_test, open(save_json_path_val, \"w\"), indent=4)\n",
        "\n",
        "train_image_paths =[]\n",
        "for path in full_image_file_paths:\n",
        "  if os.path.basename(path) in imgs_key_train:\n",
        "      train_image_paths.append(path)\n",
        "\n",
        "test_image_paths =[]\n",
        "for path in full_image_file_paths:\n",
        "  if os.path.basename(path) in imgs_key_test:\n",
        "      test_image_paths.append(path)\n",
        "\n",
        "if not os.path.exists(\"/content/coco/train2017\"):\n",
        "    os.mkdir(\"/content/coco/train2017\")\n",
        "for imageName in train_image_paths:\n",
        "    shutil.copy(imageName, \"/content/coco/train2017\")\n",
        "\n",
        "if not os.path.exists(\"/content/coco/val2017\"):\n",
        "    os.mkdir(\"/content/coco/val2017\")\n",
        "for imageName in test_image_paths:\n",
        "    shutil.copy(imageName, \"/content/coco/val2017\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9jGXBld0aKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b88033a-17bc-4acf-a19d-11f79c193117"
      },
      "source": [
        "# %cd  '/content/'\n",
        "\n",
        "# !unzip '/content/efficientdet-pytorch-master.zip'\n",
        "!git clone https://github.com/rwightman/efficientdet-pytorch.git\n",
        "%cd  '/content/efficientdet-pytorch'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'efficientdet-pytorch'...\n",
            "remote: Enumerating objects: 917, done.\u001b[K\n",
            "remote: Counting objects: 100% (78/78), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 917 (delta 42), reused 35 (delta 15), pack-reused 839\u001b[K\n",
            "Receiving objects: 100% (917/917), 362.07 KiB | 1.68 MiB/s, done.\n",
            "Resolving deltas: 100% (585/585), done.\n",
            "/content/efficientdet-pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9fErPh-SZub"
      },
      "source": [
        "!rm -rf \"/content/coco\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk63aEXmDSlr",
        "outputId": "f027b015-1260-4172-95d2-ec399f606aa0"
      },
      "source": [
        "#!git clone https://github.com/rwightman/efficientdet-pytorch.git\n",
        "%cd  '/content/efficientdet-pytorch'\n",
        "\n",
        "!python train.py  /content/coco --dataset coco2017 --model efficientdet_d0 -b 32 --amp --lr 0.0005 --warmup-epochs 3 --opt momentum --num-classes 8 --model-ema --model-ema-decay 0.0003  --pretrained --epochs 80\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/efficientdet-pytorch\n",
            "Training with a single process on 1 GPU.\n",
            "Model efficientdet_d0 created, param count: 3829793\n",
            "Using native Torch AMP. Training in mixed precision.\n",
            "Scheduled epochs: 90\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "WARNING: Model 8 has more classes than dataset 6.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/timm/utils/clip_grad.py:16: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
            "  torch.nn.utils.clip_grad_norm_(parameters, value, norm_type=norm_type)\n",
            "Train: 0 [   0/17 (  0%)]  Loss: 28.432308 (28.4323)  Time: 8.490s,    3.77/s  (8.490s,    3.77/s)  LR: 1.000e-04  Data: 2.167 (2.167)\n",
            "Train: 0 [  16/17 (100%)]  Loss: 12.652603 (27.9761)  Time: 1.632s,    1.23/s  (1.198s,    1.67/s)  LR: 1.000e-04  Data: 0.081 (0.163)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "Test (EMA): [   0/4]  Time: 2.962 (2.962)  Loss:  1.5720 (1.5720)  \n",
            "Test (EMA): [   4/4]  Time: 0.288 (1.072)  Loss:  1.7081 (1.6413)  \n",
            "Loading and preparing results...\n",
            "DONE (t=0.05s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.51s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.11s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.015\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.018\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.018\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.016\n",
            "Current checkpoints:\n",
            " ('./output/train/20210621-065336-efficientdet_d0/checkpoint-0.pth.tar', 0.0002442796467869758)\n",
            "\n",
            "Train: 1 [   0/17 (  0%)]  Loss: 24.759523 (24.7595)  Time: 3.303s,    9.69/s  (3.303s,    9.69/s)  LR: 2.333e-04  Data: 2.169 (2.169)\n",
            "Train: 1 [  16/17 (100%)]  Loss:  2.507437 (8.7176)  Time: 0.235s,    8.52/s  (0.866s,    2.31/s)  LR: 2.333e-04  Data: 0.086 (0.184)\n",
            "Test (EMA): [   0/4]  Time: 2.349 (2.349)  Loss:  1.5750 (1.5750)  \n",
            "Test (EMA): [   4/4]  Time: 0.117 (0.712)  Loss:  1.7693 (1.6390)  \n",
            "Loading and preparing results...\n",
            "DONE (t=0.11s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.43s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.11s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.002\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.008\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.016\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.003\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.015\n",
            "Current checkpoints:\n",
            " ('./output/train/20210621-065336-efficientdet_d0/checkpoint-0.pth.tar', 0.0002442796467869758)\n",
            " ('./output/train/20210621-065336-efficientdet_d0/checkpoint-1.pth.tar', 7.783032972617531e-05)\n",
            "\n",
            "Train: 2 [   0/17 (  0%)]  Loss:  2.365758 (2.3658)  Time: 3.089s,   10.36/s  (3.089s,   10.36/s)  LR: 3.667e-04  Data: 2.145 (2.145)\n",
            "*** Best metric: 0.0002442796467869758 (epoch 0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQxj9PfwmpMy",
        "outputId": "b3d3f863-930b-4956-bbb7-ae0d4af6b699"
      },
      "source": [
        "import argparse\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.parallel\n",
        "from contextlib import suppress\n",
        "\n",
        "from effdet import create_model, create_evaluator, create_dataset, create_loader\n",
        "from effdet.data import resolve_input_config\n",
        "from timm.utils import AverageMeter, setup_default_logging\n",
        "from timm.models.layers import set_layer_config\n",
        "\n",
        "has_apex = False\n",
        "try:\n",
        "    from apex import amp\n",
        "    has_apex = True\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "has_native_amp = False\n",
        "try:\n",
        "    if getattr(torch.cuda.amp, 'autocast') is not None:\n",
        "        has_native_amp = True\n",
        "except AttributeError:\n",
        "    pass\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def validate(args):\n",
        "\n",
        "    args['pretrained'] = args['pretrained'] or not args['checkpoint']  # might as well try to validate something\n",
        "    args['prefetcher'] = not args['no_prefetcher']\n",
        "\n",
        "    # create model\n",
        "    with set_layer_config(scriptable=args['torchscript']):\n",
        "        extra_args = {}\n",
        "        if args['img_size'] is not None:\n",
        "            extra_args = dict(image_size=(args['img_size'] ,args['img_size']))\n",
        "        bench = create_model(\n",
        "            args['model'],\n",
        "            bench_task='predict',\n",
        "            num_classes=args['num_classes'],\n",
        "            pretrained=args['pretrained'],\n",
        "            redundant_bias=args['redundant_bias'],\n",
        "            soft_nms=args['soft_nms'],\n",
        "            checkpoint_path=args['checkpoint'],\n",
        "            checkpoint_ema=args['use_ema'],\n",
        "            **extra_args,\n",
        "        )\n",
        "    model_config = bench.config\n",
        "\n",
        "    param_count = sum([m.numel() for m in bench.parameters()])\n",
        "    print('Model %s created, param count: %d' % (args['model'], param_count))\n",
        "\n",
        "    bench = bench.cuda()\n",
        "\n",
        "    amp_autocast = suppress\n",
        "   \n",
        "\n",
        "    dataset = create_dataset(args['dataset'], args['root'], args['split'])\n",
        "    input_config = resolve_input_config(args, model_config)\n",
        "    loader = create_loader(\n",
        "        dataset,\n",
        "        input_size=input_config['input_size'],\n",
        "        batch_size=args['batch_size'],\n",
        "        use_prefetcher=args['prefetcher'],\n",
        "        interpolation=input_config['interpolation'],\n",
        "        fill_color=input_config['fill_color'],\n",
        "        mean=input_config['mean'],\n",
        "        std=input_config['std'],\n",
        "        num_workers=args['workers'],\n",
        "        pin_mem=args['pin_mem'])\n",
        "\n",
        "    evaluator = create_evaluator(args['dataset'], dataset, pred_yxyx=False)\n",
        "    bench.eval()\n",
        "    batch_time = AverageMeter()\n",
        "    end = time.time()\n",
        "    last_idx = len(loader) - 1\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(loader):\n",
        "\n",
        "            with amp_autocast():\n",
        "                #### HERE IS WHERE SAVING THE OUTPUTS CAN TAKE PLACE\n",
        "                output = bench(input, img_info=target)\n",
        "            evaluator.add_predictions(output, target)\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if i % args['log_freq'] == 0 or i == last_idx:\n",
        "                print(\n",
        "                    'Test: [{0:>4d}/{1}]  '\n",
        "                    'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
        "                    .format(\n",
        "                        i, len(loader), batch_time=batch_time,\n",
        "                        rate_avg=input.size(0) / batch_time.avg)\n",
        "                )\n",
        "\n",
        "    mean_ap = 0.\n",
        "    if dataset.parser.has_labels:\n",
        "        mean_ap = evaluator.evaluate()\n",
        "    else:\n",
        "        evaluator.save(args.results)\n",
        "\n",
        "    return mean_ap\n",
        "\n",
        "args={}\n",
        "\n",
        "args['dataset'] = 'coco2017'\n",
        "args['num_classes'] = 6\n",
        "args['pretrained'] = True\n",
        "args['checkpoint'] = '/content/efficientdet-pytorch/output/train/20210615-213943-tf_efficientdet_d0/model_best.pth.tar'\n",
        "args['redundant_bias'] = None\n",
        "args['model'] = 'tf_efficientdet_d0'\n",
        "args['workers'] = 4\n",
        "args['batch_size'] = 128\n",
        "args['soft_nms'] = None\n",
        "args['use_ema'] = True\n",
        "args['img_size'] = None\n",
        "args['mean'] = None\n",
        "args['std'] = None\n",
        "args['interpolation'] = 'bilinear'\n",
        "args['fill_color'] = None\n",
        "args['no_prefetcher'] = False\n",
        "args['pin_mem'] = False\n",
        "args['torchscript'] = True\n",
        "args['split'] ='val'\n",
        "args['root'] = '/content/coco/'\n",
        "args['log_freq'] = 10\n",
        "\n",
        "validate(args)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model tf_efficientdet_d0 created, param count: 3830927\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test: [   0/1]  Time: 2.562s (2.562s,   41.77/s)  \n",
            "Loading and preparing results...\n",
            "DONE (t=0.04s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.16s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.09s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.003\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.037\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.060\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.072\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.013\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.089\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0001205984967999586"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}